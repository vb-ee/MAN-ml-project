{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression, RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('df_exp1.pkl')\n",
    "X = data.drop(columns=[\"N AufgewAmplitudeNom [MPa]\"])\n",
    "y = data[\"N AufgewAmplitudeNom [MPa]\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, X_train, X_test):\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('encoder', OrdinalEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor.fit(X_train)\n",
    "\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_jobs=4, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(n_jobs=4, random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_jobs=4, random_state=0)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = preprocess(X, X_train, X_test)\n",
    "rf = RandomForestRegressor(random_state=0, n_jobs=4)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_i = list(zip(X.columns, rf.feature_importances_))\n",
    "f_i.sort(key = lambda x : x[1])\n",
    "f_i = f_i[-20:]\n",
    "plt.barh([x[0] for x in f_i],[x[1] for x in f_i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(rf.feature_importances_)[::-1]\n",
    "selected_features_rf = X.columns[indices[:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFECV(rf, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/MAN-ml-project/venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/MAN-ml-project/venv/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:777\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    774\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    775\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m--> 777\u001b[0m scores_features \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m scores, step_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores_features)\n\u001b[1;32m    783\u001b[0m step_n_features_rev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(step_n_features[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/projects/MAN-ml-project/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/MAN-ml-project/venv/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/MAN-ml-project/venv/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/MAN-ml-project/venv/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rfe = RFECV(rf, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=4)\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F DMS ', 'G Zeit [s]', 'H Drehzahl [1/min]', 'J Frequenz [Hz]',\n",
       "       'U Sensitivität []', 'W ex_ey []', 'AS Drossel Drucks.',\n",
       "       'AX Drehzahl  Motor', 'BA Umgebungsdruck Kontrolle', 'BM m V red',\n",
       "       'BS T-Luftfeuchte', 'BV TvV_2', 'CG TnV_4', 'CI TnV_6',\n",
       "       'CL p_stat_SEALING-COVER_01', 'DQ T_Axiallager 2'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_rfe = np.array(X.columns)[rfe.get_support()]\n",
    "selected_features_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = X[selected_features_rf]\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_selected, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1 = preprocess(X_selected, X_train_1, X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 37.66861912707881\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_1, y_train_1)\n",
    "y_pred_lr_model = lr_model.predict(X_test_1)\n",
    "mse_lr_model = mean_squared_error(y_test_1, y_pred_lr_model)\n",
    "print(\"Linear Regression MSE:\", mse_lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 3.719132922640474\n",
      "Hyperparameter Decision Tree MSE: 3.7581952816019446\n",
      "Best Decision Tree Params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeRegressor(random_state=0)\n",
    "dt_model.fit(X_train_1, y_train_1)\n",
    "y_pred_dt_model = dt_model.predict(X_test_1)\n",
    "mse_dt_model = mean_squared_error(y_test_1, y_pred_dt_model)\n",
    "\n",
    "param_grid_dt_model = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "grid_search_dt_model = GridSearchCV(dt_model, param_grid_dt_model, cv=5, scoring='neg_mean_squared_error', n_jobs=4)\n",
    "grid_search_dt_model.fit(X_train_1, y_train_1)\n",
    "\n",
    "best_dt_model = grid_search_dt_model.best_estimator_\n",
    "y_pred_dt_model_hyper = best_dt_model.predict(X_test_1)\n",
    "mse_dt_model_hyper = mean_squared_error(y_test_1, y_pred_dt_model_hyper)\n",
    "print(\"Decision Tree MSE:\", mse_dt_model)\n",
    "print(\"Hyperparameter Decision Tree MSE:\", mse_dt_model_hyper)\n",
    "print(\"Best Decision Tree Params:\", grid_search_dt_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, l1, l2,  output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, l1)\n",
    "        self.linear2 = nn.Linear(l1, l2)\n",
    "        self.linear3 = nn.Linear(l2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(15, 8, 4, 1)\n",
    "\n",
    "loss_module = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_1, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_1, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_1, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_1.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_module, train_loader, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_module(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, loss_module):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_module(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {average_loss}\")\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, loss_module, train_loader, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 41.47043433808189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41.47043433808189"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, loss_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "l1 = [4, 8, 16, 32]\n",
    "l2 = [4, 8, 16]\n",
    "\n",
    "hyperparameter_combinations = list(product(learning_rates, l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.001, hidden1=4, hidden2=4\n",
      "Epoch [1/50], Loss: 69.69173276932513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vb/projects/MAN-ml-project/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/vb/projects/MAN-ml-project/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([48])) that is different to the input size (torch.Size([48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 44.64548655025295\n",
      "Epoch [3/50], Loss: 44.63651178391253\n",
      "Epoch [4/50], Loss: 44.623649347023886\n",
      "Epoch [5/50], Loss: 44.629998322783926\n",
      "Epoch [6/50], Loss: 44.61096170769363\n",
      "Epoch [7/50], Loss: 44.63733670281582\n",
      "Epoch [8/50], Loss: 44.618929859849274\n",
      "Epoch [9/50], Loss: 44.61622520821994\n",
      "Epoch [10/50], Loss: 44.60828386838319\n",
      "Epoch [11/50], Loss: 44.60249194160836\n",
      "Epoch [12/50], Loss: 44.64033391983783\n",
      "Epoch [13/50], Loss: 44.619784333275966\n",
      "Epoch [14/50], Loss: 44.59896064508157\n",
      "Epoch [15/50], Loss: 44.62060247327461\n",
      "Epoch [16/50], Loss: 44.624217261642706\n",
      "Epoch [17/50], Loss: 44.62200493734391\n",
      "Epoch [18/50], Loss: 44.62235969168241\n",
      "Epoch [19/50], Loss: 44.627219403376344\n",
      "Epoch [20/50], Loss: 44.6144962185719\n",
      "Epoch [21/50], Loss: 44.614820374035446\n",
      "Epoch [22/50], Loss: 44.634568492701796\n",
      "Epoch [23/50], Loss: 44.6062069001745\n",
      "Epoch [24/50], Loss: 44.61867786157327\n",
      "Epoch [25/50], Loss: 44.61031502895668\n",
      "Epoch [26/50], Loss: 44.62045262446169\n",
      "Epoch [27/50], Loss: 44.63873460175561\n",
      "Epoch [28/50], Loss: 44.62173034167681\n",
      "Epoch [29/50], Loss: 44.61714504429551\n",
      "Epoch [30/50], Loss: 44.655943554737526\n",
      "Epoch [31/50], Loss: 44.611379961107595\n",
      "Epoch [32/50], Loss: 44.6219892251687\n",
      "Epoch [33/50], Loss: 44.61406077400583\n",
      "Epoch [34/50], Loss: 44.61049854716317\n",
      "Epoch [35/50], Loss: 44.63809634349385\n",
      "Epoch [36/50], Loss: 44.66223219574475\n",
      "Epoch [37/50], Loss: 44.62230065142522\n",
      "Epoch [38/50], Loss: 44.62070387543225\n",
      "Epoch [39/50], Loss: 44.649633679624465\n",
      "Epoch [40/50], Loss: 44.61723705041604\n",
      "Epoch [41/50], Loss: 44.62395175871302\n",
      "Epoch [42/50], Loss: 44.61811633188216\n",
      "Epoch [43/50], Loss: 44.61122101955726\n",
      "Epoch [44/50], Loss: 44.6132875848989\n",
      "Epoch [45/50], Loss: 44.62743943636535\n",
      "Epoch [46/50], Loss: 44.62949750306176\n",
      "Epoch [47/50], Loss: 44.66182971078842\n",
      "Epoch [48/50], Loss: 44.61769710915988\n",
      "Epoch [49/50], Loss: 44.61019340890353\n",
      "Epoch [50/50], Loss: 44.63816602738177\n",
      "Test Loss: 41.52025902726268\n",
      "Training with lr=0.001, hidden1=4, hidden2=8\n",
      "Epoch [1/50], Loss: 58.67164843199683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vb/projects/MAN-ml-project/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 44.79766516138296\n",
      "Epoch [3/50], Loss: 44.77346752667036\n",
      "Epoch [4/50], Loss: 44.701930311859634\n",
      "Epoch [5/50], Loss: 44.70416608403941\n",
      "Epoch [6/50], Loss: 44.74438298960201\n",
      "Epoch [7/50], Loss: 44.739250089301436\n",
      "Epoch [8/50], Loss: 44.75087171460761\n",
      "Epoch [9/50], Loss: 44.72646034741011\n",
      "Epoch [10/50], Loss: 44.72859086834016\n",
      "Epoch [11/50], Loss: 44.699225844711556\n",
      "Epoch [12/50], Loss: 44.71837844848633\n",
      "Epoch [13/50], Loss: 44.70812491119885\n",
      "Epoch [14/50], Loss: 44.67697618828445\n",
      "Epoch [15/50], Loss: 44.68841822264624\n",
      "Epoch [16/50], Loss: 44.69538296558818\n",
      "Epoch [17/50], Loss: 44.72817788045914\n",
      "Epoch [18/50], Loss: 44.683219390618994\n",
      "Epoch [19/50], Loss: 44.6995849734447\n",
      "Epoch [20/50], Loss: 44.66862213259838\n",
      "Epoch [21/50], Loss: 44.71946004648677\n",
      "Epoch [22/50], Loss: 44.71544822317655\n",
      "Epoch [23/50], Loss: 44.65639320748751\n",
      "Epoch [24/50], Loss: 44.70897717085041\n",
      "Epoch [25/50], Loss: 44.69381074749055\n",
      "Epoch [26/50], Loss: 44.735061160853654\n",
      "Epoch [27/50], Loss: 44.67920190779889\n",
      "Epoch [28/50], Loss: 44.686781736280096\n",
      "Epoch [29/50], Loss: 44.66695185176662\n",
      "Epoch [30/50], Loss: 44.74282144640313\n",
      "Epoch [31/50], Loss: 44.69672221199411\n",
      "Epoch [32/50], Loss: 44.66985665618396\n",
      "Epoch [33/50], Loss: 44.69657173782098\n",
      "Epoch [34/50], Loss: 44.65835941814986\n",
      "Epoch [35/50], Loss: 44.69757394008949\n",
      "Epoch [36/50], Loss: 44.70003577060387\n",
      "Epoch [37/50], Loss: 44.69498860718774\n",
      "Epoch [38/50], Loss: 44.65118733703113\n",
      "Epoch [39/50], Loss: 44.714345634960736\n",
      "Epoch [40/50], Loss: 44.674503782928966\n",
      "Epoch [41/50], Loss: 44.69696295065958\n",
      "Epoch [42/50], Loss: 44.67596342055524\n",
      "Epoch [43/50], Loss: 44.6859842081539\n",
      "Epoch [44/50], Loss: 44.69391873156438\n",
      "Epoch [45/50], Loss: 44.68884977121822\n",
      "Epoch [46/50], Loss: 44.68827256687352\n",
      "Epoch [47/50], Loss: 44.640597953171024\n",
      "Epoch [48/50], Loss: 44.675889299736646\n",
      "Epoch [49/50], Loss: 44.691650002901675\n",
      "Epoch [50/50], Loss: 44.660904512249054\n",
      "Test Loss: 41.51368414900685\n",
      "Training with lr=0.001, hidden1=4, hidden2=16\n",
      "Epoch [1/50], Loss: 60.02306758693007\n",
      "Epoch [2/50], Loss: 45.011655688676676\n",
      "Epoch [3/50], Loss: 44.7671504349005\n",
      "Epoch [4/50], Loss: 44.77239586877041\n",
      "Epoch [5/50], Loss: 44.703887908185116\n",
      "Epoch [6/50], Loss: 44.733966914943004\n",
      "Epoch [7/50], Loss: 44.73934465627201\n",
      "Epoch [8/50], Loss: 44.685990230372695\n",
      "Epoch [9/50], Loss: 44.70312034732006\n",
      "Epoch [10/50], Loss: 44.71760769828421\n",
      "Epoch [11/50], Loss: 44.69066047668457\n",
      "Epoch [12/50], Loss: 44.69402304477379\n",
      "Epoch [13/50], Loss: 44.70527955352283\n",
      "Epoch [14/50], Loss: 44.69977572394199\n",
      "Epoch [15/50], Loss: 44.70922044222472\n",
      "Epoch [16/50], Loss: 44.698881239969225\n",
      "Epoch [17/50], Loss: 44.69246676710785\n",
      "Epoch [18/50], Loss: 44.69052478602675\n",
      "Epoch [19/50], Loss: 44.668141743394195\n",
      "Epoch [20/50], Loss: 44.682535384131256\n",
      "Epoch [21/50], Loss: 44.66059939275022\n",
      "Epoch [22/50], Loss: 44.69048663280049\n",
      "Epoch [23/50], Loss: 44.66297664329654\n",
      "Epoch [24/50], Loss: 44.69689448622407\n",
      "Epoch [25/50], Loss: 44.66353941120085\n",
      "Epoch [26/50], Loss: 44.68897160076704\n",
      "Epoch [27/50], Loss: 44.66768102802214\n",
      "Epoch [28/50], Loss: 44.71475870101178\n",
      "Epoch [29/50], Loss: 44.72599821247038\n",
      "Epoch [30/50], Loss: 44.690925132251174\n",
      "Epoch [31/50], Loss: 44.6532901763916\n",
      "Epoch [32/50], Loss: 44.68155904676093\n",
      "Epoch [33/50], Loss: 44.642858486488215\n",
      "Epoch [34/50], Loss: 44.69679472251016\n",
      "Epoch [35/50], Loss: 44.684076834506676\n",
      "Epoch [36/50], Loss: 44.62248251868076\n",
      "Epoch [37/50], Loss: 44.68018742545706\n",
      "Epoch [38/50], Loss: 44.711640917668575\n",
      "Epoch [39/50], Loss: 44.69132622890785\n",
      "Epoch [40/50], Loss: 44.6669518017378\n",
      "Epoch [41/50], Loss: 44.68381208826284\n",
      "Epoch [42/50], Loss: 44.64630181984823\n",
      "Epoch [43/50], Loss: 44.65891977529057\n",
      "Epoch [44/50], Loss: 44.679243231601404\n",
      "Epoch [45/50], Loss: 44.620063544101406\n",
      "Epoch [46/50], Loss: 44.6854877909676\n",
      "Epoch [47/50], Loss: 44.640623117665776\n",
      "Epoch [48/50], Loss: 44.66232116574147\n",
      "Epoch [49/50], Loss: 44.66177550769243\n",
      "Epoch [50/50], Loss: 44.67619946589235\n",
      "Test Loss: 41.52815335397502\n",
      "Training with lr=0.001, hidden1=8, hidden2=4\n",
      "Epoch [1/50], Loss: 70.05658384854677\n",
      "Epoch [2/50], Loss: 45.08835458599153\n",
      "Epoch [3/50], Loss: 44.806726630789335\n",
      "Epoch [4/50], Loss: 44.73567505508173\n",
      "Epoch [5/50], Loss: 44.792408533565336\n",
      "Epoch [6/50], Loss: 44.70061507928567\n",
      "Epoch [7/50], Loss: 44.70095790800501\n",
      "Epoch [8/50], Loss: 44.702368811310315\n",
      "Epoch [9/50], Loss: 44.73889331817627\n",
      "Epoch [10/50], Loss: 44.72695005448138\n",
      "Epoch [11/50], Loss: 44.669016750523305\n",
      "Epoch [12/50], Loss: 44.67233844194256\n",
      "Epoch [13/50], Loss: 44.6888044951392\n",
      "Epoch [14/50], Loss: 44.69915650789855\n",
      "Epoch [15/50], Loss: 44.68227915529345\n",
      "Epoch [16/50], Loss: 44.68202437729132\n",
      "Epoch [17/50], Loss: 44.674900458288974\n",
      "Epoch [18/50], Loss: 44.68069677509245\n",
      "Epoch [19/50], Loss: 44.699264282476705\n",
      "Epoch [20/50], Loss: 44.70372759084233\n",
      "Epoch [21/50], Loss: 44.680769998519146\n",
      "Epoch [22/50], Loss: 44.72468484972344\n",
      "Epoch [23/50], Loss: 44.690868405826755\n",
      "Epoch [24/50], Loss: 44.651853736502225\n",
      "Epoch [25/50], Loss: 44.67536504151391\n",
      "Epoch [26/50], Loss: 44.670428366739245\n",
      "Epoch [27/50], Loss: 44.69957115923772\n",
      "Epoch [28/50], Loss: 44.69010997209393\n",
      "Epoch [29/50], Loss: 44.68740671501785\n",
      "Epoch [30/50], Loss: 44.75020432394059\n",
      "Epoch [31/50], Loss: 44.69607087432361\n",
      "Epoch [32/50], Loss: 44.6641655843766\n",
      "Epoch [33/50], Loss: 44.66833272840156\n",
      "Epoch [34/50], Loss: 44.6643449189233\n",
      "Epoch [35/50], Loss: 44.68414926685271\n",
      "Epoch [36/50], Loss: 44.66675091727835\n",
      "Epoch [37/50], Loss: 44.70868998042873\n",
      "Epoch [38/50], Loss: 44.669769771763534\n",
      "Epoch [39/50], Loss: 44.650655677670336\n",
      "Epoch [40/50], Loss: 44.67661679377321\n",
      "Epoch [41/50], Loss: 44.66874151386198\n",
      "Epoch [42/50], Loss: 44.679703037074354\n",
      "Epoch [43/50], Loss: 44.667885445766764\n",
      "Epoch [44/50], Loss: 44.666292534499874\n",
      "Epoch [45/50], Loss: 44.652402293095825\n",
      "Epoch [46/50], Loss: 44.68686921791952\n",
      "Epoch [47/50], Loss: 44.65438515710049\n",
      "Epoch [48/50], Loss: 44.669798988592426\n",
      "Epoch [49/50], Loss: 44.685569581829135\n",
      "Epoch [50/50], Loss: 44.65862431448014\n",
      "Test Loss: 41.53729009264298\n",
      "Training with lr=0.001, hidden1=8, hidden2=8\n",
      "Epoch [1/50], Loss: 61.50059953908451\n",
      "Epoch [2/50], Loss: 44.92370151144559\n",
      "Epoch [3/50], Loss: 44.853036417726614\n",
      "Epoch [4/50], Loss: 44.773442102651124\n",
      "Epoch [5/50], Loss: 44.795175983866706\n",
      "Epoch [6/50], Loss: 44.72415864037686\n",
      "Epoch [7/50], Loss: 44.78149326824751\n",
      "Epoch [8/50], Loss: 44.712831616010824\n",
      "Epoch [9/50], Loss: 44.68892228173428\n",
      "Epoch [10/50], Loss: 44.700555000930535\n",
      "Epoch [11/50], Loss: 44.71647846659676\n",
      "Epoch [12/50], Loss: 44.71195844431392\n",
      "Epoch [13/50], Loss: 44.68884988378306\n",
      "Epoch [14/50], Loss: 44.7127872529577\n",
      "Epoch [15/50], Loss: 44.64833235506151\n",
      "Epoch [16/50], Loss: 44.676642383512906\n",
      "Epoch [17/50], Loss: 44.69422556142338\n",
      "Epoch [18/50], Loss: 44.712521637463176\n",
      "Epoch [19/50], Loss: 44.69529703484207\n",
      "Epoch [20/50], Loss: 44.73512854654281\n",
      "Epoch [21/50], Loss: 44.69939184345183\n",
      "Epoch [22/50], Loss: 44.67318180584517\n",
      "Epoch [23/50], Loss: 44.70908451393002\n",
      "Epoch [24/50], Loss: 44.68076799111288\n",
      "Epoch [25/50], Loss: 44.68498134300357\n",
      "Epoch [26/50], Loss: 44.66868950890713\n",
      "Epoch [27/50], Loss: 44.72730550609651\n",
      "Epoch [28/50], Loss: 44.67909308261559\n",
      "Epoch [29/50], Loss: 44.68851709209505\n",
      "Epoch [30/50], Loss: 44.68058759032703\n",
      "Epoch [31/50], Loss: 44.71218037214435\n",
      "Epoch [32/50], Loss: 44.66937701741203\n",
      "Epoch [33/50], Loss: 44.65795632034052\n",
      "Epoch [34/50], Loss: 44.699182347782326\n",
      "Epoch [35/50], Loss: 44.67466399396052\n",
      "Epoch [36/50], Loss: 44.697759690831916\n",
      "Epoch [37/50], Loss: 44.6883798255295\n",
      "Epoch [38/50], Loss: 44.68108788787342\n",
      "Epoch [39/50], Loss: 44.71402838034708\n",
      "Epoch [40/50], Loss: 44.6889998076392\n",
      "Epoch [41/50], Loss: 44.66330441959569\n",
      "Epoch [42/50], Loss: 44.65058695058354\n",
      "Epoch [43/50], Loss: 44.712019116761255\n",
      "Epoch [44/50], Loss: 44.72472629547119\n",
      "Epoch [45/50], Loss: 44.73576084824859\n",
      "Epoch [46/50], Loss: 44.67418912043337\n",
      "Epoch [47/50], Loss: 44.683969322579806\n",
      "Epoch [48/50], Loss: 44.6792031335049\n",
      "Epoch [49/50], Loss: 44.70810165405273\n",
      "Epoch [50/50], Loss: 44.6592881937496\n",
      "Test Loss: 41.603657081836964\n",
      "Training with lr=0.001, hidden1=8, hidden2=16\n",
      "Epoch [1/50], Loss: 58.990940806904774\n",
      "Epoch [2/50], Loss: 45.11269125078545\n",
      "Epoch [3/50], Loss: 44.841013404971264\n",
      "Epoch [4/50], Loss: 44.751078302352155\n",
      "Epoch [5/50], Loss: 44.716787494596886\n",
      "Epoch [6/50], Loss: 44.71997608122278\n",
      "Epoch [7/50], Loss: 44.71732961310715\n",
      "Epoch [8/50], Loss: 44.71040123173448\n",
      "Epoch [9/50], Loss: 44.72060685704966\n",
      "Epoch [10/50], Loss: 44.72979132855525\n",
      "Epoch [11/50], Loss: 44.69760783461274\n",
      "Epoch [12/50], Loss: 44.70071084694784\n",
      "Epoch [13/50], Loss: 44.663646904366914\n",
      "Epoch [14/50], Loss: 44.74573697574803\n",
      "Epoch [15/50], Loss: 44.68452618395696\n",
      "Epoch [16/50], Loss: 44.668715936629496\n",
      "Epoch [17/50], Loss: 44.658057728751764\n",
      "Epoch [18/50], Loss: 44.660083870809586\n",
      "Epoch [19/50], Loss: 44.68751847626733\n",
      "Epoch [20/50], Loss: 44.690215770533825\n",
      "Epoch [21/50], Loss: 44.70203134505475\n",
      "Epoch [22/50], Loss: 44.66092119060579\n",
      "Epoch [23/50], Loss: 44.694154270359725\n",
      "Epoch [24/50], Loss: 44.73260066548332\n",
      "Epoch [25/50], Loss: 44.680596229678294\n",
      "Epoch [26/50], Loss: 44.659121866695216\n",
      "Epoch [27/50], Loss: 44.655063541599965\n",
      "Epoch [28/50], Loss: 44.67009114750096\n",
      "Epoch [29/50], Loss: 44.65395198884558\n",
      "Epoch [30/50], Loss: 44.67829538564213\n",
      "Epoch [31/50], Loss: 44.71796841855909\n",
      "Epoch [32/50], Loss: 44.709598084746816\n",
      "Epoch [33/50], Loss: 44.65845848771392\n",
      "Epoch [34/50], Loss: 44.66889585901479\n",
      "Epoch [35/50], Loss: 44.6934787375028\n",
      "Epoch [36/50], Loss: 44.65615640859135\n",
      "Epoch [37/50], Loss: 44.66405862276672\n",
      "Epoch [38/50], Loss: 44.65320257906054\n",
      "Epoch [39/50], Loss: 44.643990691763456\n",
      "Epoch [40/50], Loss: 44.683803914804926\n",
      "Epoch [41/50], Loss: 44.69842275400631\n",
      "Epoch [42/50], Loss: 44.653383348808916\n",
      "Epoch [43/50], Loss: 44.668569258392836\n",
      "Epoch [44/50], Loss: 44.66463808466177\n",
      "Epoch [45/50], Loss: 44.68583090735264\n",
      "Epoch [46/50], Loss: 44.7115966921947\n",
      "Epoch [47/50], Loss: 44.64705002268807\n",
      "Epoch [48/50], Loss: 44.642492572596815\n",
      "Epoch [49/50], Loss: 44.667493141674605\n",
      "Epoch [50/50], Loss: 44.66594679160196\n",
      "Test Loss: 41.674531514408024\n",
      "Training with lr=0.001, hidden1=16, hidden2=4\n",
      "Epoch [1/50], Loss: 68.29236946731318\n",
      "Epoch [2/50], Loss: 45.214995618726384\n",
      "Epoch [3/50], Loss: 44.87318275951948\n",
      "Epoch [4/50], Loss: 44.83156291774062\n",
      "Epoch [5/50], Loss: 44.742121868446226\n",
      "Epoch [6/50], Loss: 44.77660693809634\n",
      "Epoch [7/50], Loss: 44.74149827175453\n",
      "Epoch [8/50], Loss: 44.748758816328206\n",
      "Epoch [9/50], Loss: 44.72905215904361\n",
      "Epoch [10/50], Loss: 44.68747583295478\n",
      "Epoch [11/50], Loss: 44.70692255614234\n",
      "Epoch [12/50], Loss: 44.70752988096143\n",
      "Epoch [13/50], Loss: 44.751302031219986\n",
      "Epoch [14/50], Loss: 44.67880810596904\n",
      "Epoch [15/50], Loss: 44.71860992556713\n",
      "Epoch [16/50], Loss: 44.706964258287776\n",
      "Epoch [17/50], Loss: 44.685535706066695\n",
      "Epoch [18/50], Loss: 44.734727065289604\n",
      "Epoch [19/50], Loss: 44.69319345442975\n",
      "Epoch [20/50], Loss: 44.68386157301606\n",
      "Epoch [21/50], Loss: 44.6842040953089\n",
      "Epoch [22/50], Loss: 44.664645801606724\n",
      "Epoch [23/50], Loss: 44.70240500403232\n",
      "Epoch [24/50], Loss: 44.72248690870941\n",
      "Epoch [25/50], Loss: 44.69215476239314\n",
      "Epoch [26/50], Loss: 44.72099858893723\n",
      "Epoch [27/50], Loss: 44.68374733221336\n",
      "Epoch [28/50], Loss: 44.663915615394465\n",
      "Epoch [29/50], Loss: 44.67651556984323\n",
      "Epoch [30/50], Loss: 44.67317380436131\n",
      "Epoch [31/50], Loss: 44.667420234054816\n",
      "Epoch [32/50], Loss: 44.70080615184346\n",
      "Epoch [33/50], Loss: 44.66698483326396\n",
      "Epoch [34/50], Loss: 44.661914650338595\n",
      "Epoch [35/50], Loss: 44.68363790355745\n",
      "Epoch [36/50], Loss: 44.71387377566979\n",
      "Epoch [37/50], Loss: 44.694954625114065\n",
      "Epoch [38/50], Loss: 44.71937647960225\n",
      "Epoch [39/50], Loss: 44.67703568505459\n",
      "Epoch [40/50], Loss: 44.673485558931944\n",
      "Epoch [41/50], Loss: 44.68723916225746\n",
      "Epoch [42/50], Loss: 44.67930597399102\n",
      "Epoch [43/50], Loss: 44.686274400304576\n",
      "Epoch [44/50], Loss: 44.6911431203123\n",
      "Epoch [45/50], Loss: 44.66994575125272\n",
      "Epoch [46/50], Loss: 44.65023425368012\n",
      "Epoch [47/50], Loss: 44.6690486845423\n",
      "Epoch [48/50], Loss: 44.680115115056275\n",
      "Epoch [49/50], Loss: 44.63727105562804\n",
      "Epoch [50/50], Loss: 44.68104707999308\n",
      "Test Loss: 41.51003341092408\n",
      "Training with lr=0.001, hidden1=16, hidden2=8\n",
      "Epoch [1/50], Loss: 61.79637819509037\n",
      "Epoch [2/50], Loss: 44.944545333111876\n",
      "Epoch [3/50], Loss: 44.790339485543676\n",
      "Epoch [4/50], Loss: 44.77468757629394\n",
      "Epoch [5/50], Loss: 44.76194038391113\n",
      "Epoch [6/50], Loss: 44.73208086607886\n",
      "Epoch [7/50], Loss: 44.72925960978524\n",
      "Epoch [8/50], Loss: 44.70442355109043\n",
      "Epoch [9/50], Loss: 44.73271051625736\n",
      "Epoch [10/50], Loss: 44.73137601320861\n",
      "Epoch [11/50], Loss: 44.70469616749248\n",
      "Epoch [12/50], Loss: 44.67089936303311\n",
      "Epoch [13/50], Loss: 44.725346096226424\n",
      "Epoch [14/50], Loss: 44.68104851519475\n",
      "Epoch [15/50], Loss: 44.69237520811988\n",
      "Epoch [16/50], Loss: 44.70378676867876\n",
      "Epoch [17/50], Loss: 44.69343695093374\n",
      "Epoch [18/50], Loss: 44.7068061015645\n",
      "Epoch [19/50], Loss: 44.6769323255195\n",
      "Epoch [20/50], Loss: 44.74433124417164\n",
      "Epoch [21/50], Loss: 44.72220787298484\n",
      "Epoch [22/50], Loss: 44.71778844301818\n",
      "Epoch [23/50], Loss: 44.6954309494769\n",
      "Epoch [24/50], Loss: 44.721528312808175\n",
      "Epoch [25/50], Loss: 44.68105051009381\n",
      "Epoch [26/50], Loss: 44.68870204863001\n",
      "Epoch [27/50], Loss: 44.70869432980897\n",
      "Epoch [28/50], Loss: 44.67432630570208\n",
      "Epoch [29/50], Loss: 44.69539604812372\n",
      "Epoch [30/50], Loss: 44.67916023379467\n",
      "Epoch [31/50], Loss: 44.71708866807281\n",
      "Epoch [32/50], Loss: 44.682406253502016\n",
      "Epoch [33/50], Loss: 44.67342230062016\n",
      "Epoch [34/50], Loss: 44.66509857803094\n",
      "Epoch [35/50], Loss: 44.67063571116963\n",
      "Epoch [36/50], Loss: 44.67094894159035\n",
      "Epoch [37/50], Loss: 44.713217300665185\n",
      "Epoch [38/50], Loss: 44.728266744144626\n",
      "Epoch [39/50], Loss: 44.66182289748895\n",
      "Epoch [40/50], Loss: 44.67483786598581\n",
      "Epoch [41/50], Loss: 44.71476893815838\n",
      "Epoch [42/50], Loss: 44.64457035377377\n",
      "Epoch [43/50], Loss: 44.7204038150975\n",
      "Epoch [44/50], Loss: 44.66854427525254\n",
      "Epoch [45/50], Loss: 44.662805463446944\n",
      "Epoch [46/50], Loss: 44.67969724936563\n",
      "Epoch [47/50], Loss: 44.704885195122394\n",
      "Epoch [48/50], Loss: 44.67170624654801\n",
      "Epoch [49/50], Loss: 44.678459814728285\n",
      "Epoch [50/50], Loss: 44.70825119018555\n",
      "Test Loss: 41.50783485674676\n",
      "Training with lr=0.001, hidden1=16, hidden2=16\n",
      "Epoch [1/50], Loss: 64.66781923888159\n",
      "Epoch [2/50], Loss: 45.019764221691695\n",
      "Epoch [3/50], Loss: 44.847980561803595\n",
      "Epoch [4/50], Loss: 44.77419813187396\n",
      "Epoch [5/50], Loss: 44.748447049250366\n",
      "Epoch [6/50], Loss: 44.71910342857486\n",
      "Epoch [7/50], Loss: 44.731710205703486\n",
      "Epoch [8/50], Loss: 44.7281699071165\n",
      "Epoch [9/50], Loss: 44.71211078206046\n",
      "Epoch [10/50], Loss: 44.765372704677894\n",
      "Epoch [11/50], Loss: 44.73523652123623\n",
      "Epoch [12/50], Loss: 44.693624871676086\n",
      "Epoch [13/50], Loss: 44.70880796713907\n",
      "Epoch [14/50], Loss: 44.746467965548156\n",
      "Epoch [15/50], Loss: 44.68727986069976\n",
      "Epoch [16/50], Loss: 44.71571192506884\n",
      "Epoch [17/50], Loss: 44.72459635500048\n",
      "Epoch [18/50], Loss: 44.68971734594126\n",
      "Epoch [19/50], Loss: 44.66878460118028\n",
      "Epoch [20/50], Loss: 44.71128339923796\n",
      "Epoch [21/50], Loss: 44.65106229000404\n",
      "Epoch [22/50], Loss: 44.744849136227465\n",
      "Epoch [23/50], Loss: 44.74016343101126\n",
      "Epoch [24/50], Loss: 44.71456137485192\n",
      "Epoch [25/50], Loss: 44.711162973622805\n",
      "Epoch [26/50], Loss: 44.7389280663162\n",
      "Epoch [27/50], Loss: 44.712539053744955\n",
      "Epoch [28/50], Loss: 44.73209447704378\n",
      "Epoch [29/50], Loss: 44.71841337798072\n",
      "Epoch [30/50], Loss: 44.662378301776826\n",
      "Epoch [31/50], Loss: 44.705849312954264\n",
      "Epoch [32/50], Loss: 44.70345150056433\n",
      "Epoch [33/50], Loss: 44.689464318947714\n",
      "Epoch [34/50], Loss: 44.71949971308474\n",
      "Epoch [35/50], Loss: 44.67314739539975\n",
      "Epoch [36/50], Loss: 44.70557645578853\n",
      "Epoch [37/50], Loss: 44.69136366922347\n",
      "Epoch [38/50], Loss: 44.67093292611544\n",
      "Epoch [39/50], Loss: 44.74402954851995\n",
      "Epoch [40/50], Loss: 44.69932865080286\n",
      "Epoch [41/50], Loss: 44.69383192218718\n",
      "Epoch [42/50], Loss: 44.719901632090085\n",
      "Epoch [43/50], Loss: 44.67395376611928\n",
      "Epoch [44/50], Loss: 44.720447333914336\n",
      "Epoch [45/50], Loss: 44.681644596037316\n",
      "Epoch [46/50], Loss: 44.70860441864514\n",
      "Epoch [47/50], Loss: 44.67219529073746\n",
      "Epoch [48/50], Loss: 44.715210179813575\n",
      "Epoch [49/50], Loss: 44.69483995281282\n",
      "Epoch [50/50], Loss: 44.694965249983994\n",
      "Test Loss: 41.51526124968783\n",
      "Training with lr=0.001, hidden1=32, hidden2=4\n",
      "Epoch [1/50], Loss: 63.90309177461218\n",
      "Epoch [2/50], Loss: 45.343355378948274\n",
      "Epoch [3/50], Loss: 44.92934227615106\n",
      "Epoch [4/50], Loss: 44.777061462402344\n",
      "Epoch [5/50], Loss: 44.78253607828109\n",
      "Epoch [6/50], Loss: 44.77379057211954\n",
      "Epoch [7/50], Loss: 44.77045844343842\n",
      "Epoch [8/50], Loss: 44.71164953513224\n",
      "Epoch [9/50], Loss: 44.72910960463227\n",
      "Epoch [10/50], Loss: 44.727774579407736\n",
      "Epoch [11/50], Loss: 44.73853756013464\n",
      "Epoch [12/50], Loss: 44.70574681328945\n",
      "Epoch [13/50], Loss: 44.74721624030442\n",
      "Epoch [14/50], Loss: 44.68083079603852\n",
      "Epoch [15/50], Loss: 44.710885663892405\n",
      "Epoch [16/50], Loss: 44.711722314553185\n",
      "Epoch [17/50], Loss: 44.68434879740731\n",
      "Epoch [18/50], Loss: 44.65844473916976\n",
      "Epoch [19/50], Loss: 44.67602514048092\n",
      "Epoch [20/50], Loss: 44.695130220006725\n",
      "Epoch [21/50], Loss: 44.69828413978952\n",
      "Epoch [22/50], Loss: 44.6745023821221\n",
      "Epoch [23/50], Loss: 44.69968149779273\n",
      "Epoch [24/50], Loss: 44.706807408567336\n",
      "Epoch [25/50], Loss: 44.70790400270556\n",
      "Epoch [26/50], Loss: 44.691570844806606\n",
      "Epoch [27/50], Loss: 44.66896600566927\n",
      "Epoch [28/50], Loss: 44.658173595491\n",
      "Epoch [29/50], Loss: 44.690822363681484\n",
      "Epoch [30/50], Loss: 44.668118805181784\n",
      "Epoch [31/50], Loss: 44.668357104942444\n",
      "Epoch [32/50], Loss: 44.68126133778056\n",
      "Epoch [33/50], Loss: 44.70865466008421\n",
      "Epoch [34/50], Loss: 44.70500209683278\n",
      "Epoch [35/50], Loss: 44.69338700341397\n",
      "Epoch [36/50], Loss: 44.69800640794097\n",
      "Epoch [37/50], Loss: 44.68992612870013\n",
      "Epoch [38/50], Loss: 44.662335599055055\n",
      "Epoch [39/50], Loss: 44.67052246468966\n",
      "Epoch [40/50], Loss: 44.704478373293014\n",
      "Epoch [41/50], Loss: 44.64124956287321\n",
      "Epoch [42/50], Loss: 44.70603725245741\n",
      "Epoch [43/50], Loss: 44.6557321704802\n",
      "Epoch [44/50], Loss: 44.66830468099625\n",
      "Epoch [45/50], Loss: 44.682833643428616\n",
      "Epoch [46/50], Loss: 44.66405599625384\n",
      "Epoch [47/50], Loss: 44.71375769318127\n",
      "Epoch [48/50], Loss: 44.64875786890749\n",
      "Epoch [49/50], Loss: 44.65363742640761\n",
      "Epoch [50/50], Loss: 44.69471715708248\n",
      "Test Loss: 41.575622442114444\n",
      "Training with lr=0.001, hidden1=32, hidden2=8\n",
      "Epoch [1/50], Loss: 61.44156672993644\n",
      "Epoch [2/50], Loss: 45.1120394472216\n",
      "Epoch [3/50], Loss: 44.8913610302034\n",
      "Epoch [4/50], Loss: 44.818785226540484\n",
      "Epoch [5/50], Loss: 44.81582239494949\n",
      "Epoch [6/50], Loss: 44.770617500680395\n",
      "Epoch [7/50], Loss: 44.75864688060323\n",
      "Epoch [8/50], Loss: 44.74053562351915\n",
      "Epoch [9/50], Loss: 44.73277489709072\n",
      "Epoch [10/50], Loss: 44.74742375670886\n",
      "Epoch [11/50], Loss: 44.71574939665247\n",
      "Epoch [12/50], Loss: 44.7257779230837\n",
      "Epoch [13/50], Loss: 44.73780195517618\n",
      "Epoch [14/50], Loss: 44.678486986629295\n",
      "Epoch [15/50], Loss: 44.74392650166496\n",
      "Epoch [16/50], Loss: 44.71163561774082\n",
      "Epoch [17/50], Loss: 44.68421880690778\n",
      "Epoch [18/50], Loss: 44.698388890751076\n",
      "Epoch [19/50], Loss: 44.689951365111305\n",
      "Epoch [20/50], Loss: 44.69353193689565\n",
      "Epoch [21/50], Loss: 44.71811776395704\n",
      "Epoch [22/50], Loss: 44.7221100979164\n",
      "Epoch [23/50], Loss: 44.68564860234495\n",
      "Epoch [24/50], Loss: 44.662546170344115\n",
      "Epoch [25/50], Loss: 44.70184735157451\n",
      "Epoch [26/50], Loss: 44.69506849070064\n",
      "Epoch [27/50], Loss: 44.735771676360585\n",
      "Epoch [28/50], Loss: 44.71251529631068\n",
      "Epoch [29/50], Loss: 44.675509968742\n",
      "Epoch [30/50], Loss: 44.70289011470607\n",
      "Epoch [31/50], Loss: 44.69983947316154\n",
      "Epoch [32/50], Loss: 44.705915898182354\n",
      "Epoch [33/50], Loss: 44.72196039918993\n",
      "Epoch [34/50], Loss: 44.653467960044985\n",
      "Epoch [35/50], Loss: 44.68377637706819\n",
      "Epoch [36/50], Loss: 44.71739339984831\n",
      "Epoch [37/50], Loss: 44.70940236576268\n",
      "Epoch [38/50], Loss: 44.67971888057521\n",
      "Epoch [39/50], Loss: 44.65108897099729\n",
      "Epoch [40/50], Loss: 44.68765618996542\n",
      "Epoch [41/50], Loss: 44.68050368887479\n",
      "Epoch [42/50], Loss: 44.67332362190622\n",
      "Epoch [43/50], Loss: 44.639755111444195\n",
      "Epoch [44/50], Loss: 44.74256524883333\n",
      "Epoch [45/50], Loss: 44.696521734018795\n",
      "Epoch [46/50], Loss: 44.66438045501709\n",
      "Epoch [47/50], Loss: 44.68650468294738\n",
      "Epoch [48/50], Loss: 44.70778031271012\n",
      "Epoch [49/50], Loss: 44.677920532226565\n",
      "Epoch [50/50], Loss: 44.69014855369193\n",
      "Test Loss: 41.52597324720776\n",
      "Training with lr=0.001, hidden1=32, hidden2=16\n",
      "Epoch [1/50], Loss: 61.407313925320985\n",
      "Epoch [2/50], Loss: 45.2540147625032\n",
      "Epoch [3/50], Loss: 44.94979687675101\n",
      "Epoch [4/50], Loss: 44.88442385313941\n",
      "Epoch [5/50], Loss: 44.79965260927794\n",
      "Epoch [6/50], Loss: 44.77065445321505\n",
      "Epoch [7/50], Loss: 44.72577501828553\n",
      "Epoch [8/50], Loss: 44.73126488982654\n",
      "Epoch [9/50], Loss: 44.74155477304927\n",
      "Epoch [10/50], Loss: 44.75147880491663\n",
      "Epoch [11/50], Loss: 44.69565650439653\n",
      "Epoch [12/50], Loss: 44.715692657720844\n",
      "Epoch [13/50], Loss: 44.70708770439273\n",
      "Epoch [14/50], Loss: 44.72559239121734\n",
      "Epoch [15/50], Loss: 44.71562480613834\n",
      "Epoch [16/50], Loss: 44.69065221567623\n",
      "Epoch [17/50], Loss: 44.6735716397645\n",
      "Epoch [18/50], Loss: 44.71779086003538\n",
      "Epoch [19/50], Loss: 44.703637232545944\n",
      "Epoch [20/50], Loss: 44.67109428468298\n",
      "Epoch [21/50], Loss: 44.72368818189277\n",
      "Epoch [22/50], Loss: 44.68615233624568\n",
      "Epoch [23/50], Loss: 44.695442912617665\n",
      "Epoch [24/50], Loss: 44.717099436775584\n",
      "Epoch [25/50], Loss: 44.64835891723633\n",
      "Epoch [26/50], Loss: 44.69814367450652\n",
      "Epoch [27/50], Loss: 44.664783906154945\n",
      "Epoch [28/50], Loss: 44.741389452824826\n",
      "Epoch [29/50], Loss: 44.68787818033187\n",
      "Epoch [30/50], Loss: 44.680092933529714\n",
      "Epoch [31/50], Loss: 44.69713397416912\n",
      "Epoch [32/50], Loss: 44.68962379518103\n",
      "Epoch [33/50], Loss: 44.67255895020532\n",
      "Epoch [34/50], Loss: 44.70176773071289\n",
      "Epoch [35/50], Loss: 44.65021716258565\n",
      "Epoch [36/50], Loss: 44.697472222124944\n",
      "Epoch [37/50], Loss: 44.71257826070317\n",
      "Epoch [38/50], Loss: 44.64214141720631\n",
      "Epoch [39/50], Loss: 44.68413320447578\n",
      "Epoch [40/50], Loss: 44.66350263689385\n",
      "Epoch [41/50], Loss: 44.69137450046227\n",
      "Epoch [42/50], Loss: 44.651439807454096\n",
      "Epoch [43/50], Loss: 44.67276830204197\n",
      "Epoch [44/50], Loss: 44.72531955906602\n",
      "Epoch [45/50], Loss: 44.69844131469726\n",
      "Epoch [46/50], Loss: 44.70869088720103\n",
      "Epoch [47/50], Loss: 44.655640774085875\n",
      "Epoch [48/50], Loss: 44.665675704205626\n",
      "Epoch [49/50], Loss: 44.670888788192\n",
      "Epoch [50/50], Loss: 44.717901439354065\n",
      "Test Loss: 41.67911797443419\n",
      "Training with lr=0.005, hidden1=4, hidden2=4\n",
      "Epoch [1/50], Loss: 48.93775943068207\n",
      "Epoch [2/50], Loss: 44.995066139346264\n",
      "Epoch [3/50], Loss: 44.93333712405846\n",
      "Epoch [4/50], Loss: 44.85952890114706\n",
      "Epoch [5/50], Loss: 44.80702740716153\n",
      "Epoch [6/50], Loss: 44.82204993201084\n",
      "Epoch [7/50], Loss: 44.77915546854989\n",
      "Epoch [8/50], Loss: 44.74823206291824\n",
      "Epoch [9/50], Loss: 44.719069815463705\n",
      "Epoch [10/50], Loss: 44.71773283911533\n",
      "Epoch [11/50], Loss: 44.71623720262871\n",
      "Epoch [12/50], Loss: 44.73210331025671\n",
      "Epoch [13/50], Loss: 44.723792445073364\n",
      "Epoch [14/50], Loss: 44.701385354214025\n",
      "Epoch [15/50], Loss: 44.71195637437164\n",
      "Epoch [16/50], Loss: 44.67835210581295\n",
      "Epoch [17/50], Loss: 44.69052390114206\n",
      "Epoch [18/50], Loss: 44.66027025941943\n",
      "Epoch [19/50], Loss: 44.71198229555224\n",
      "Epoch [20/50], Loss: 44.693046041394844\n",
      "Epoch [21/50], Loss: 44.68716614832643\n",
      "Epoch [22/50], Loss: 44.64857530437532\n",
      "Epoch [23/50], Loss: 44.671711480812945\n",
      "Epoch [24/50], Loss: 44.68820034089636\n",
      "Epoch [25/50], Loss: 44.64931688777736\n",
      "Epoch [26/50], Loss: 44.693496835427204\n",
      "Epoch [27/50], Loss: 44.67604201895292\n",
      "Epoch [28/50], Loss: 44.666459136712746\n",
      "Epoch [29/50], Loss: 44.68548924805688\n",
      "Epoch [30/50], Loss: 44.66207023995822\n",
      "Epoch [31/50], Loss: 44.656546261271494\n",
      "Epoch [32/50], Loss: 44.67989405334973\n",
      "Epoch [33/50], Loss: 44.671592374707835\n",
      "Epoch [34/50], Loss: 44.650832626467846\n",
      "Epoch [35/50], Loss: 44.659837507029046\n",
      "Epoch [36/50], Loss: 44.66838611540247\n",
      "Epoch [37/50], Loss: 44.695764841798876\n",
      "Epoch [38/50], Loss: 44.67342375770944\n",
      "Epoch [39/50], Loss: 44.66805290472312\n",
      "Epoch [40/50], Loss: 44.733283230515774\n",
      "Epoch [41/50], Loss: 44.724615597334065\n",
      "Epoch [42/50], Loss: 44.68624478324515\n",
      "Epoch [43/50], Loss: 44.700932337026124\n",
      "Epoch [44/50], Loss: 44.669877440030454\n",
      "Epoch [45/50], Loss: 44.69383177210073\n",
      "Epoch [46/50], Loss: 44.70034225338795\n",
      "Epoch [47/50], Loss: 44.68798318769111\n",
      "Epoch [48/50], Loss: 44.673024981139136\n",
      "Epoch [49/50], Loss: 44.698059913760325\n",
      "Epoch [50/50], Loss: 44.675182367543705\n",
      "Test Loss: 41.533668430706925\n",
      "Training with lr=0.005, hidden1=4, hidden2=8\n",
      "Epoch [1/50], Loss: 53.889125004752735\n",
      "Epoch [2/50], Loss: 44.684850630213006\n",
      "Epoch [3/50], Loss: 44.69585252980717\n",
      "Epoch [4/50], Loss: 44.676253240616596\n",
      "Epoch [5/50], Loss: 44.63500777072594\n",
      "Epoch [6/50], Loss: 44.66803446597741\n",
      "Epoch [7/50], Loss: 44.692914481241196\n",
      "Epoch [8/50], Loss: 44.67587841221544\n",
      "Epoch [9/50], Loss: 44.65964533071049\n",
      "Epoch [10/50], Loss: 44.6845184576316\n",
      "Epoch [11/50], Loss: 44.64819542619048\n",
      "Epoch [12/50], Loss: 44.66554799627085\n",
      "Epoch [13/50], Loss: 44.666392879798764\n",
      "Epoch [14/50], Loss: 44.69835325772645\n",
      "Epoch [15/50], Loss: 44.66977103499116\n",
      "Epoch [16/50], Loss: 44.69974454660885\n",
      "Epoch [17/50], Loss: 44.696635724677414\n",
      "Epoch [18/50], Loss: 44.66056158659888\n",
      "Epoch [19/50], Loss: 44.66492677282115\n",
      "Epoch [20/50], Loss: 44.71580605428727\n",
      "Epoch [21/50], Loss: 44.680603677718366\n",
      "Epoch [22/50], Loss: 44.680066074308805\n",
      "Epoch [23/50], Loss: 44.66941241279977\n",
      "Epoch [24/50], Loss: 44.71850025614754\n",
      "Epoch [25/50], Loss: 44.68924530216905\n",
      "Epoch [26/50], Loss: 44.668343578401156\n",
      "Epoch [27/50], Loss: 44.62634214807729\n",
      "Epoch [28/50], Loss: 44.65857455144163\n",
      "Epoch [29/50], Loss: 44.659655173880154\n",
      "Epoch [30/50], Loss: 44.703170523096304\n",
      "Epoch [31/50], Loss: 44.66234678674917\n",
      "Epoch [32/50], Loss: 44.683243954767946\n",
      "Epoch [33/50], Loss: 44.66038897780121\n",
      "Epoch [34/50], Loss: 44.67875353703733\n",
      "Epoch [35/50], Loss: 44.63673706054688\n",
      "Epoch [36/50], Loss: 44.68565458704214\n",
      "Epoch [37/50], Loss: 44.68354444034764\n",
      "Epoch [38/50], Loss: 44.66732151156566\n",
      "Epoch [39/50], Loss: 44.6551230352433\n",
      "Epoch [40/50], Loss: 44.67612511681729\n",
      "Epoch [41/50], Loss: 44.70204599724441\n",
      "Epoch [42/50], Loss: 44.63115363824563\n",
      "Epoch [43/50], Loss: 44.67730438982854\n",
      "Epoch [44/50], Loss: 44.66086081520456\n",
      "Epoch [45/50], Loss: 44.64889799024238\n",
      "Epoch [46/50], Loss: 44.70213461703941\n",
      "Epoch [47/50], Loss: 44.68813496261347\n",
      "Epoch [48/50], Loss: 44.68305377022165\n",
      "Epoch [49/50], Loss: 44.68528155342477\n",
      "Epoch [50/50], Loss: 44.64603818049196\n",
      "Test Loss: 41.647507470982674\n",
      "Training with lr=0.005, hidden1=4, hidden2=16\n",
      "Epoch [1/50], Loss: 49.680522862418755\n",
      "Epoch [2/50], Loss: 45.095852554821576\n",
      "Epoch [3/50], Loss: 45.063605149065864\n",
      "Epoch [4/50], Loss: 45.04514445633185\n",
      "Epoch [5/50], Loss: 44.95546642366003\n",
      "Epoch [6/50], Loss: 44.8580581602503\n",
      "Epoch [7/50], Loss: 44.850059787562635\n",
      "Epoch [8/50], Loss: 44.80574983065246\n",
      "Epoch [9/50], Loss: 44.80430537989882\n",
      "Epoch [10/50], Loss: 44.823765420132\n",
      "Epoch [11/50], Loss: 44.75991861624796\n",
      "Epoch [12/50], Loss: 44.798173141479495\n",
      "Epoch [13/50], Loss: 44.7010850687496\n",
      "Epoch [14/50], Loss: 44.73198548301322\n",
      "Epoch [15/50], Loss: 44.74053568605517\n",
      "Epoch [16/50], Loss: 44.6969934463501\n",
      "Epoch [17/50], Loss: 44.73273395163114\n",
      "Epoch [18/50], Loss: 44.693040319348945\n",
      "Epoch [19/50], Loss: 44.68542718730989\n",
      "Epoch [20/50], Loss: 44.675113384059216\n",
      "Epoch [21/50], Loss: 44.68411160453421\n",
      "Epoch [22/50], Loss: 44.67626988145172\n",
      "Epoch [23/50], Loss: 44.679244144627305\n",
      "Epoch [24/50], Loss: 44.685492246659074\n",
      "Epoch [25/50], Loss: 44.650914226594516\n",
      "Epoch [26/50], Loss: 44.684983828810395\n",
      "Epoch [27/50], Loss: 44.66125791580951\n",
      "Epoch [28/50], Loss: 44.68970581429904\n",
      "Epoch [29/50], Loss: 44.69503506519755\n",
      "Epoch [30/50], Loss: 44.67748935261711\n",
      "Epoch [31/50], Loss: 44.69339557084881\n",
      "Epoch [32/50], Loss: 44.69391990411477\n",
      "Epoch [33/50], Loss: 44.69488964080811\n",
      "Epoch [34/50], Loss: 44.70589014897581\n",
      "Epoch [35/50], Loss: 44.64237240025255\n",
      "Epoch [36/50], Loss: 44.73731718845055\n",
      "Epoch [37/50], Loss: 44.69791826341973\n",
      "Epoch [38/50], Loss: 44.679444497530575\n",
      "Epoch [39/50], Loss: 44.680898216122486\n",
      "Epoch [40/50], Loss: 44.6527955727499\n",
      "Epoch [41/50], Loss: 44.67258398962802\n",
      "Epoch [42/50], Loss: 44.67796212493396\n",
      "Epoch [43/50], Loss: 44.720572324659\n",
      "Epoch [44/50], Loss: 44.686250893014375\n",
      "Epoch [45/50], Loss: 44.67626215825315\n",
      "Epoch [46/50], Loss: 44.68793119211666\n",
      "Epoch [47/50], Loss: 44.69393398097304\n",
      "Epoch [48/50], Loss: 44.66553514824539\n",
      "Epoch [49/50], Loss: 44.7085072407957\n",
      "Epoch [50/50], Loss: 44.69779216578749\n",
      "Test Loss: 41.50522992083135\n",
      "Training with lr=0.005, hidden1=8, hidden2=4\n",
      "Epoch [1/50], Loss: 52.656786221363504\n",
      "Epoch [2/50], Loss: 44.990915117107456\n",
      "Epoch [3/50], Loss: 44.93454988198202\n",
      "Epoch [4/50], Loss: 44.8842360699763\n",
      "Epoch [5/50], Loss: 44.85583544871846\n",
      "Epoch [6/50], Loss: 44.796200217575326\n",
      "Epoch [7/50], Loss: 44.78683639901583\n",
      "Epoch [8/50], Loss: 44.78896440599785\n",
      "Epoch [9/50], Loss: 44.74507642652168\n",
      "Epoch [10/50], Loss: 44.736382662663694\n",
      "Epoch [11/50], Loss: 44.693754177406184\n",
      "Epoch [12/50], Loss: 44.70216127614506\n",
      "Epoch [13/50], Loss: 44.72984854276063\n",
      "Epoch [14/50], Loss: 44.690291501654954\n",
      "Epoch [15/50], Loss: 44.70165023491031\n",
      "Epoch [16/50], Loss: 44.71066607490915\n",
      "Epoch [17/50], Loss: 44.67156924888736\n",
      "Epoch [18/50], Loss: 44.66558136236472\n",
      "Epoch [19/50], Loss: 44.65293219206763\n",
      "Epoch [20/50], Loss: 44.65423885407995\n",
      "Epoch [21/50], Loss: 44.66923469793601\n",
      "Epoch [22/50], Loss: 44.650196238033104\n",
      "Epoch [23/50], Loss: 44.665295116236955\n",
      "Epoch [24/50], Loss: 44.67838007817503\n",
      "Epoch [25/50], Loss: 44.67377088890701\n",
      "Epoch [26/50], Loss: 44.68564899319508\n",
      "Epoch [27/50], Loss: 44.65846419412582\n",
      "Epoch [28/50], Loss: 44.66388047640441\n",
      "Epoch [29/50], Loss: 44.68695489539475\n",
      "Epoch [30/50], Loss: 44.708859246676084\n",
      "Epoch [31/50], Loss: 44.63740396343294\n",
      "Epoch [32/50], Loss: 44.64709040532347\n",
      "Epoch [33/50], Loss: 44.66978662834793\n",
      "Epoch [34/50], Loss: 44.64834288925421\n",
      "Epoch [35/50], Loss: 44.660497337091165\n",
      "Epoch [36/50], Loss: 44.69318256690854\n",
      "Epoch [37/50], Loss: 44.64223970507012\n",
      "Epoch [38/50], Loss: 44.66768877623511\n",
      "Epoch [39/50], Loss: 44.66578975427346\n",
      "Epoch [40/50], Loss: 44.70688792994765\n",
      "Epoch [41/50], Loss: 44.71422444953293\n",
      "Epoch [42/50], Loss: 44.6762484159626\n",
      "Epoch [43/50], Loss: 44.665254330244224\n",
      "Epoch [44/50], Loss: 44.65454367653268\n",
      "Epoch [45/50], Loss: 44.64537169659724\n",
      "Epoch [46/50], Loss: 44.675742021154186\n",
      "Epoch [47/50], Loss: 44.692741147025686\n",
      "Epoch [48/50], Loss: 44.6413616461832\n",
      "Epoch [49/50], Loss: 44.68892093095623\n",
      "Epoch [50/50], Loss: 44.666394777766996\n",
      "Test Loss: 41.530380270863304\n",
      "Training with lr=0.005, hidden1=8, hidden2=8\n",
      "Epoch [1/50], Loss: 52.049888160580494\n",
      "Epoch [2/50], Loss: 45.2027850197964\n",
      "Epoch [3/50], Loss: 45.06960616189925\n",
      "Epoch [4/50], Loss: 44.99854549970783\n",
      "Epoch [5/50], Loss: 44.95171565696841\n",
      "Epoch [6/50], Loss: 44.83261308826384\n",
      "Epoch [7/50], Loss: 44.84260356465324\n",
      "Epoch [8/50], Loss: 44.83696131471728\n",
      "Epoch [9/50], Loss: 44.86016810682953\n",
      "Epoch [10/50], Loss: 44.767168032536745\n",
      "Epoch [11/50], Loss: 44.77819418359975\n",
      "Epoch [12/50], Loss: 44.77123432472104\n",
      "Epoch [13/50], Loss: 44.75463740864738\n",
      "Epoch [14/50], Loss: 44.73921977809218\n",
      "Epoch [15/50], Loss: 44.705571577978915\n",
      "Epoch [16/50], Loss: 44.71342782817903\n",
      "Epoch [17/50], Loss: 44.74224010217385\n",
      "Epoch [18/50], Loss: 44.67731092171591\n",
      "Epoch [19/50], Loss: 44.69425648548564\n",
      "Epoch [20/50], Loss: 44.65890702106913\n",
      "Epoch [21/50], Loss: 44.727946121966255\n",
      "Epoch [22/50], Loss: 44.70073849724942\n",
      "Epoch [23/50], Loss: 44.69106337125184\n",
      "Epoch [24/50], Loss: 44.668203510221886\n",
      "Epoch [25/50], Loss: 44.69400586300209\n",
      "Epoch [26/50], Loss: 44.67568382513328\n",
      "Epoch [27/50], Loss: 44.662912956613006\n",
      "Epoch [28/50], Loss: 44.6683156623215\n",
      "Epoch [29/50], Loss: 44.68076300386522\n",
      "Epoch [30/50], Loss: 44.66950980327169\n",
      "Epoch [31/50], Loss: 44.70298841507709\n",
      "Epoch [32/50], Loss: 44.687562023225375\n",
      "Epoch [33/50], Loss: 44.686302041225744\n",
      "Epoch [34/50], Loss: 44.65966200281362\n",
      "Epoch [35/50], Loss: 44.658334269289114\n",
      "Epoch [36/50], Loss: 44.70786176587715\n",
      "Epoch [37/50], Loss: 44.68780685174661\n",
      "Epoch [38/50], Loss: 44.69994327670238\n",
      "Epoch [39/50], Loss: 44.64514714225394\n",
      "Epoch [40/50], Loss: 44.6607599852515\n",
      "Epoch [41/50], Loss: 44.68689606463323\n",
      "Epoch [42/50], Loss: 44.68367619748975\n",
      "Epoch [43/50], Loss: 44.64545969728564\n",
      "Epoch [44/50], Loss: 44.7002185383781\n",
      "Epoch [45/50], Loss: 44.693824311553456\n",
      "Epoch [46/50], Loss: 44.6835657151019\n",
      "Epoch [47/50], Loss: 44.64961333665691\n",
      "Epoch [48/50], Loss: 44.659381015965195\n",
      "Epoch [49/50], Loss: 44.656320240458506\n",
      "Epoch [50/50], Loss: 44.658314908137086\n",
      "Test Loss: 41.50635820490713\n",
      "Training with lr=0.005, hidden1=8, hidden2=16\n",
      "Epoch [1/50], Loss: 47.68754395031538\n",
      "Epoch [2/50], Loss: 45.00867459656762\n",
      "Epoch [3/50], Loss: 45.127404169176444\n",
      "Epoch [4/50], Loss: 45.03539605062516\n",
      "Epoch [5/50], Loss: 44.897750132201146\n",
      "Epoch [6/50], Loss: 44.93152901383697\n",
      "Epoch [7/50], Loss: 44.870059548049674\n",
      "Epoch [8/50], Loss: 44.80421348946994\n",
      "Epoch [9/50], Loss: 44.79307114648037\n",
      "Epoch [10/50], Loss: 44.82591226609027\n",
      "Epoch [11/50], Loss: 44.78754931590596\n",
      "Epoch [12/50], Loss: 44.694315325627564\n",
      "Epoch [13/50], Loss: 44.68848483601555\n",
      "Epoch [14/50], Loss: 44.68961249179527\n",
      "Epoch [15/50], Loss: 44.70913887649286\n",
      "Epoch [16/50], Loss: 44.68920729590244\n",
      "Epoch [17/50], Loss: 44.73889864311844\n",
      "Epoch [18/50], Loss: 44.68575321885406\n",
      "Epoch [19/50], Loss: 44.69162081734079\n",
      "Epoch [20/50], Loss: 44.71041324803087\n",
      "Epoch [21/50], Loss: 44.72593762444668\n",
      "Epoch [22/50], Loss: 44.68208712280774\n",
      "Epoch [23/50], Loss: 44.72565923284312\n",
      "Epoch [24/50], Loss: 44.67777624286589\n",
      "Epoch [25/50], Loss: 44.67582471253442\n",
      "Epoch [26/50], Loss: 44.69889665822514\n",
      "Epoch [27/50], Loss: 44.711403493412206\n",
      "Epoch [28/50], Loss: 44.72047400552719\n",
      "Epoch [29/50], Loss: 44.717639028830604\n",
      "Epoch [30/50], Loss: 44.689648306174355\n",
      "Epoch [31/50], Loss: 44.674186628372944\n",
      "Epoch [32/50], Loss: 44.67716317098649\n",
      "Epoch [33/50], Loss: 44.68094512439165\n",
      "Epoch [34/50], Loss: 44.740219432017845\n",
      "Epoch [35/50], Loss: 44.69139351766618\n",
      "Epoch [36/50], Loss: 44.6957766611068\n",
      "Epoch [37/50], Loss: 44.670093630180986\n",
      "Epoch [38/50], Loss: 44.67963169910869\n",
      "Epoch [39/50], Loss: 44.67671521296267\n",
      "Epoch [40/50], Loss: 44.64983768463135\n",
      "Epoch [41/50], Loss: 44.659101786378955\n",
      "Epoch [42/50], Loss: 44.70120927154041\n",
      "Epoch [43/50], Loss: 44.68627108589548\n",
      "Epoch [44/50], Loss: 44.672067495252264\n",
      "Epoch [45/50], Loss: 44.705586255182986\n",
      "Epoch [46/50], Loss: 44.72478408813477\n",
      "Epoch [47/50], Loss: 44.70495102053783\n",
      "Epoch [48/50], Loss: 44.68418943061203\n",
      "Epoch [49/50], Loss: 44.67158361966493\n",
      "Epoch [50/50], Loss: 44.69052866013324\n",
      "Test Loss: 41.584644448666175\n",
      "Training with lr=0.005, hidden1=16, hidden2=4\n",
      "Epoch [1/50], Loss: 65.2976074250018\n",
      "Epoch [2/50], Loss: 44.652241409802045\n",
      "Epoch [3/50], Loss: 44.60520105830959\n",
      "Epoch [4/50], Loss: 44.60314638106549\n",
      "Epoch [5/50], Loss: 44.62460066685911\n",
      "Epoch [6/50], Loss: 44.60005293048796\n",
      "Epoch [7/50], Loss: 44.59689517412029\n",
      "Epoch [8/50], Loss: 44.60880469650519\n",
      "Epoch [9/50], Loss: 44.609082534664964\n",
      "Epoch [10/50], Loss: 44.60095555039703\n",
      "Epoch [11/50], Loss: 44.60022030814749\n",
      "Epoch [12/50], Loss: 44.62176601222304\n",
      "Epoch [13/50], Loss: 44.650158591348614\n",
      "Epoch [14/50], Loss: 44.61297503924761\n",
      "Epoch [15/50], Loss: 44.631196853762766\n",
      "Epoch [16/50], Loss: 44.60816568468438\n",
      "Epoch [17/50], Loss: 44.597920864918194\n",
      "Epoch [18/50], Loss: 44.60358778531434\n",
      "Epoch [19/50], Loss: 44.61818149754259\n",
      "Epoch [20/50], Loss: 44.66325737687408\n",
      "Epoch [21/50], Loss: 44.62626977201368\n",
      "Epoch [22/50], Loss: 44.61117196630259\n",
      "Epoch [23/50], Loss: 44.667180039452724\n",
      "Epoch [24/50], Loss: 44.61280945324507\n",
      "Epoch [25/50], Loss: 44.698836586123605\n",
      "Epoch [26/50], Loss: 44.595251815045465\n",
      "Epoch [27/50], Loss: 44.60298803360736\n",
      "Epoch [28/50], Loss: 44.60271754655682\n",
      "Epoch [29/50], Loss: 44.6264111878442\n",
      "Epoch [30/50], Loss: 44.60508000108062\n",
      "Epoch [31/50], Loss: 44.59526671737921\n",
      "Epoch [32/50], Loss: 44.61257608757644\n",
      "Epoch [33/50], Loss: 44.63106780755715\n",
      "Epoch [34/50], Loss: 44.608356801017386\n",
      "Epoch [35/50], Loss: 44.63582223986016\n",
      "Epoch [36/50], Loss: 44.60036784312764\n",
      "Epoch [37/50], Loss: 44.60489994111608\n",
      "Epoch [38/50], Loss: 44.61650528829606\n",
      "Epoch [39/50], Loss: 44.622909114399896\n",
      "Epoch [40/50], Loss: 44.62146240609591\n",
      "Epoch [41/50], Loss: 44.597424213221814\n",
      "Epoch [42/50], Loss: 44.609415423283814\n",
      "Epoch [43/50], Loss: 44.60945011826812\n",
      "Epoch [44/50], Loss: 44.598559395211645\n",
      "Epoch [45/50], Loss: 44.61157810023573\n",
      "Epoch [46/50], Loss: 44.65257179698006\n",
      "Epoch [47/50], Loss: 44.6239631590296\n",
      "Epoch [48/50], Loss: 44.61223604796363\n",
      "Epoch [49/50], Loss: 44.61279059863482\n",
      "Epoch [50/50], Loss: 44.62101462942655\n",
      "Test Loss: 41.50471035032782\n",
      "Training with lr=0.005, hidden1=16, hidden2=8\n",
      "Epoch [1/50], Loss: 49.96877454851494\n",
      "Epoch [2/50], Loss: 45.25470308397637\n",
      "Epoch [3/50], Loss: 45.12747694859739\n",
      "Epoch [4/50], Loss: 45.032120176221504\n",
      "Epoch [5/50], Loss: 44.939154027719965\n",
      "Epoch [6/50], Loss: 44.91696994343742\n",
      "Epoch [7/50], Loss: 44.918766184322166\n",
      "Epoch [8/50], Loss: 44.86903462644483\n",
      "Epoch [9/50], Loss: 44.87311840370053\n",
      "Epoch [10/50], Loss: 44.84674515646012\n",
      "Epoch [11/50], Loss: 44.80203477202869\n",
      "Epoch [12/50], Loss: 44.736778978441585\n",
      "Epoch [13/50], Loss: 44.738688956714064\n",
      "Epoch [14/50], Loss: 44.67905589557085\n",
      "Epoch [15/50], Loss: 44.70274725116667\n",
      "Epoch [16/50], Loss: 44.68194325869201\n",
      "Epoch [17/50], Loss: 44.711016573671436\n",
      "Epoch [18/50], Loss: 44.74450777397781\n",
      "Epoch [19/50], Loss: 44.667344102703154\n",
      "Epoch [20/50], Loss: 44.709309133936145\n",
      "Epoch [21/50], Loss: 44.727717609092835\n",
      "Epoch [22/50], Loss: 44.72146179949651\n",
      "Epoch [23/50], Loss: 44.68771957647605\n",
      "Epoch [24/50], Loss: 44.68471742848881\n",
      "Epoch [25/50], Loss: 44.70560116377033\n",
      "Epoch [26/50], Loss: 44.669170185777006\n",
      "Epoch [27/50], Loss: 44.68803299450484\n",
      "Epoch [28/50], Loss: 44.67128225232734\n",
      "Epoch [29/50], Loss: 44.673810799395454\n",
      "Epoch [30/50], Loss: 44.67734197397701\n",
      "Epoch [31/50], Loss: 44.69272010365471\n",
      "Epoch [32/50], Loss: 44.70275009030201\n",
      "Epoch [33/50], Loss: 44.70068824955675\n",
      "Epoch [34/50], Loss: 44.662759249327614\n",
      "Epoch [35/50], Loss: 44.68792364401895\n",
      "Epoch [36/50], Loss: 44.648615880872384\n",
      "Epoch [37/50], Loss: 44.6979871968754\n",
      "Epoch [38/50], Loss: 44.649349956825134\n",
      "Epoch [39/50], Loss: 44.68553027168649\n",
      "Epoch [40/50], Loss: 44.675709358590545\n",
      "Epoch [41/50], Loss: 44.68779544517642\n",
      "Epoch [42/50], Loss: 44.65727557510626\n",
      "Epoch [43/50], Loss: 44.69093277102611\n",
      "Epoch [44/50], Loss: 44.66504911829214\n",
      "Epoch [45/50], Loss: 44.663952955652455\n",
      "Epoch [46/50], Loss: 44.67122570725738\n",
      "Epoch [47/50], Loss: 44.641757427278115\n",
      "Epoch [48/50], Loss: 44.73697730517778\n",
      "Epoch [49/50], Loss: 44.6785080706487\n",
      "Epoch [50/50], Loss: 44.674765765080686\n",
      "Test Loss: 41.784502582695644\n",
      "Training with lr=0.005, hidden1=16, hidden2=16\n",
      "Epoch [1/50], Loss: 49.58264060098617\n",
      "Epoch [2/50], Loss: 45.20672082744661\n",
      "Epoch [3/50], Loss: 45.07933760158351\n",
      "Epoch [4/50], Loss: 45.049751200441456\n",
      "Epoch [5/50], Loss: 44.99605630968438\n",
      "Epoch [6/50], Loss: 44.93354069444\n",
      "Epoch [7/50], Loss: 44.82431758505399\n",
      "Epoch [8/50], Loss: 44.87341053446785\n",
      "Epoch [9/50], Loss: 44.8519805720595\n",
      "Epoch [10/50], Loss: 44.84945037716725\n",
      "Epoch [11/50], Loss: 44.82564972736797\n",
      "Epoch [12/50], Loss: 44.784212250006\n",
      "Epoch [13/50], Loss: 44.84877915304215\n",
      "Epoch [14/50], Loss: 44.76351236437188\n",
      "Epoch [15/50], Loss: 44.73150858019219\n",
      "Epoch [16/50], Loss: 44.728681251650954\n",
      "Epoch [17/50], Loss: 44.756701522577\n",
      "Epoch [18/50], Loss: 44.698332239369876\n",
      "Epoch [19/50], Loss: 44.72659823933586\n",
      "Epoch [20/50], Loss: 44.68189837346311\n",
      "Epoch [21/50], Loss: 44.72744987675401\n",
      "Epoch [22/50], Loss: 44.69197236358142\n",
      "Epoch [23/50], Loss: 44.68478927299625\n",
      "Epoch [24/50], Loss: 44.68372753330919\n",
      "Epoch [25/50], Loss: 44.69272527225682\n",
      "Epoch [26/50], Loss: 44.6773763062524\n",
      "Epoch [27/50], Loss: 44.690010783711415\n",
      "Epoch [28/50], Loss: 44.65376825801662\n",
      "Epoch [29/50], Loss: 44.68024049352427\n",
      "Epoch [30/50], Loss: 44.6666048769091\n",
      "Epoch [31/50], Loss: 44.65480663737313\n",
      "Epoch [32/50], Loss: 44.65937039109527\n",
      "Epoch [33/50], Loss: 44.76057547272229\n",
      "Epoch [34/50], Loss: 44.6809937961766\n",
      "Epoch [35/50], Loss: 44.665239371628054\n",
      "Epoch [36/50], Loss: 44.680553342475264\n",
      "Epoch [37/50], Loss: 44.667837024125895\n",
      "Epoch [38/50], Loss: 44.65243161936275\n",
      "Epoch [39/50], Loss: 44.722726971985864\n",
      "Epoch [40/50], Loss: 44.697577923634014\n",
      "Epoch [41/50], Loss: 44.676846644917475\n",
      "Epoch [42/50], Loss: 44.69950636566662\n",
      "Epoch [43/50], Loss: 44.69729650841385\n",
      "Epoch [44/50], Loss: 44.67558752591493\n",
      "Epoch [45/50], Loss: 44.64783155097336\n",
      "Epoch [46/50], Loss: 44.68007558603756\n",
      "Epoch [47/50], Loss: 44.68338791894131\n",
      "Epoch [48/50], Loss: 44.67169972404105\n",
      "Epoch [49/50], Loss: 44.655961846523596\n",
      "Epoch [50/50], Loss: 44.67817272436423\n",
      "Test Loss: 41.54383970944936\n",
      "Training with lr=0.005, hidden1=32, hidden2=4\n",
      "Epoch [1/50], Loss: 49.19122405443035\n",
      "Epoch [2/50], Loss: 45.29282916334809\n",
      "Epoch [3/50], Loss: 45.1131356536365\n",
      "Epoch [4/50], Loss: 44.93135347835353\n",
      "Epoch [5/50], Loss: 44.97490018313049\n",
      "Epoch [6/50], Loss: 44.909737152349756\n",
      "Epoch [7/50], Loss: 44.887344798103705\n",
      "Epoch [8/50], Loss: 44.895819098050474\n",
      "Epoch [9/50], Loss: 44.796250190109504\n",
      "Epoch [10/50], Loss: 44.84292087242252\n",
      "Epoch [11/50], Loss: 44.768467037013316\n",
      "Epoch [12/50], Loss: 44.79312305450439\n",
      "Epoch [13/50], Loss: 44.74685956611008\n",
      "Epoch [14/50], Loss: 44.73930553373743\n",
      "Epoch [15/50], Loss: 44.73318247560595\n",
      "Epoch [16/50], Loss: 44.66754404912229\n",
      "Epoch [17/50], Loss: 44.73296358077253\n",
      "Epoch [18/50], Loss: 44.716161709144465\n",
      "Epoch [19/50], Loss: 44.68121170919449\n",
      "Epoch [20/50], Loss: 44.74940466333608\n",
      "Epoch [21/50], Loss: 44.69945042719606\n",
      "Epoch [22/50], Loss: 44.7843560140641\n",
      "Epoch [23/50], Loss: 44.73530954454766\n",
      "Epoch [24/50], Loss: 44.69317106028072\n",
      "Epoch [25/50], Loss: 44.68390256537766\n",
      "Epoch [26/50], Loss: 44.70792881699859\n",
      "Epoch [27/50], Loss: 44.70234593876073\n",
      "Epoch [28/50], Loss: 44.69457077901871\n",
      "Epoch [29/50], Loss: 44.67116084489666\n",
      "Epoch [30/50], Loss: 44.70556789460729\n",
      "Epoch [31/50], Loss: 44.63047815854432\n",
      "Epoch [32/50], Loss: 44.67258175709208\n",
      "Epoch [33/50], Loss: 44.6736994321229\n",
      "Epoch [34/50], Loss: 44.638334536943276\n",
      "Epoch [35/50], Loss: 44.6665982418373\n",
      "Epoch [36/50], Loss: 44.661184339054294\n",
      "Epoch [37/50], Loss: 44.68311042472964\n",
      "Epoch [38/50], Loss: 44.68115605213603\n",
      "Epoch [39/50], Loss: 44.65358212267766\n",
      "Epoch [40/50], Loss: 44.70535373375064\n",
      "Epoch [41/50], Loss: 44.73970220597064\n",
      "Epoch [42/50], Loss: 44.67650468232202\n",
      "Epoch [43/50], Loss: 44.659357464899784\n",
      "Epoch [44/50], Loss: 44.68526272070213\n",
      "Epoch [45/50], Loss: 44.70357791713027\n",
      "Epoch [46/50], Loss: 44.706428596621656\n",
      "Epoch [47/50], Loss: 44.66497533829486\n",
      "Epoch [48/50], Loss: 44.6666748109411\n",
      "Epoch [49/50], Loss: 44.671450849439275\n",
      "Epoch [50/50], Loss: 44.68632202773798\n",
      "Test Loss: 41.74835151206446\n",
      "Training with lr=0.005, hidden1=32, hidden2=8\n",
      "Epoch [1/50], Loss: 49.15282393908892\n",
      "Epoch [2/50], Loss: 45.18374688820761\n",
      "Epoch [3/50], Loss: 45.1687056932293\n",
      "Epoch [4/50], Loss: 45.0957676309054\n",
      "Epoch [5/50], Loss: 44.999337090038864\n",
      "Epoch [6/50], Loss: 44.81254898446505\n",
      "Epoch [7/50], Loss: 44.90425898129823\n",
      "Epoch [8/50], Loss: 44.85324085423204\n",
      "Epoch [9/50], Loss: 44.85882679986172\n",
      "Epoch [10/50], Loss: 44.88156419972904\n",
      "Epoch [11/50], Loss: 44.7425027628414\n",
      "Epoch [12/50], Loss: 44.7492732876637\n",
      "Epoch [13/50], Loss: 44.742483595551036\n",
      "Epoch [14/50], Loss: 44.725697001472845\n",
      "Epoch [15/50], Loss: 44.74725848025963\n",
      "Epoch [16/50], Loss: 44.715483749889934\n",
      "Epoch [17/50], Loss: 44.72418146602443\n",
      "Epoch [18/50], Loss: 44.70467339812732\n",
      "Epoch [19/50], Loss: 44.66424664356669\n",
      "Epoch [20/50], Loss: 44.72241139021076\n",
      "Epoch [21/50], Loss: 44.73453759365395\n",
      "Epoch [22/50], Loss: 44.70098624307601\n",
      "Epoch [23/50], Loss: 44.75492566530822\n",
      "Epoch [24/50], Loss: 44.70558308460674\n",
      "Epoch [25/50], Loss: 44.70184687942755\n",
      "Epoch [26/50], Loss: 44.694284295254064\n",
      "Epoch [27/50], Loss: 44.69240244255691\n",
      "Epoch [28/50], Loss: 44.68420163138968\n",
      "Epoch [29/50], Loss: 44.694803882036055\n",
      "Epoch [30/50], Loss: 44.679638121558014\n",
      "Epoch [31/50], Loss: 44.68736722977435\n",
      "Epoch [32/50], Loss: 44.70960027350754\n",
      "Epoch [33/50], Loss: 44.669156096411534\n",
      "Epoch [34/50], Loss: 44.68616499353627\n",
      "Epoch [35/50], Loss: 44.69026832893247\n",
      "Epoch [36/50], Loss: 44.673749736097996\n",
      "Epoch [37/50], Loss: 44.68212244627906\n",
      "Epoch [38/50], Loss: 44.72092835473232\n",
      "Epoch [39/50], Loss: 44.69239475062636\n",
      "Epoch [40/50], Loss: 44.657083889695464\n",
      "Epoch [41/50], Loss: 44.69150581359863\n",
      "Epoch [42/50], Loss: 44.69090314145948\n",
      "Epoch [43/50], Loss: 44.72652099796983\n",
      "Epoch [44/50], Loss: 44.6961251055608\n",
      "Epoch [45/50], Loss: 44.69444688265441\n",
      "Epoch [46/50], Loss: 44.67394941048544\n",
      "Epoch [47/50], Loss: 44.676692818813635\n",
      "Epoch [48/50], Loss: 44.69810402041576\n",
      "Epoch [49/50], Loss: 44.654556837238246\n",
      "Epoch [50/50], Loss: 44.67863322398701\n",
      "Test Loss: 41.62825634643322\n",
      "Training with lr=0.005, hidden1=32, hidden2=16\n",
      "Epoch [1/50], Loss: 49.487500469020155\n",
      "Epoch [2/50], Loss: 45.097873009228316\n",
      "Epoch [3/50], Loss: 45.08332100539911\n",
      "Epoch [4/50], Loss: 45.007169210715375\n",
      "Epoch [5/50], Loss: 44.97971845220347\n",
      "Epoch [6/50], Loss: 44.90427308629771\n",
      "Epoch [7/50], Loss: 44.82456218531874\n",
      "Epoch [8/50], Loss: 44.882506204823976\n",
      "Epoch [9/50], Loss: 44.73944313799748\n",
      "Epoch [10/50], Loss: 44.73931961997611\n",
      "Epoch [11/50], Loss: 44.80135012767354\n",
      "Epoch [12/50], Loss: 44.73464672839055\n",
      "Epoch [13/50], Loss: 44.72169887667797\n",
      "Epoch [14/50], Loss: 44.72506102890265\n",
      "Epoch [15/50], Loss: 44.73522605583316\n",
      "Epoch [16/50], Loss: 44.71653211937576\n",
      "Epoch [17/50], Loss: 44.66831321090948\n",
      "Epoch [18/50], Loss: 44.68716075146784\n",
      "Epoch [19/50], Loss: 44.69360817455855\n",
      "Epoch [20/50], Loss: 44.679217892005795\n",
      "Epoch [21/50], Loss: 44.68326516073258\n",
      "Epoch [22/50], Loss: 44.69478175366511\n",
      "Epoch [23/50], Loss: 44.68462067275751\n",
      "Epoch [24/50], Loss: 44.68424729519203\n",
      "Epoch [25/50], Loss: 44.69209807349033\n",
      "Epoch [26/50], Loss: 44.70035815629803\n",
      "Epoch [27/50], Loss: 44.720598058231545\n",
      "Epoch [28/50], Loss: 44.68425481514853\n",
      "Epoch [29/50], Loss: 44.718449545688316\n",
      "Epoch [30/50], Loss: 44.695076370239256\n",
      "Epoch [31/50], Loss: 44.67328333620165\n",
      "Epoch [32/50], Loss: 44.660604708311986\n",
      "Epoch [33/50], Loss: 44.682586660541475\n",
      "Epoch [34/50], Loss: 44.677067275125474\n",
      "Epoch [35/50], Loss: 44.71537116003818\n",
      "Epoch [36/50], Loss: 44.69772880429127\n",
      "Epoch [37/50], Loss: 44.68056732865631\n",
      "Epoch [38/50], Loss: 44.67486633550925\n",
      "Epoch [39/50], Loss: 44.68231622664655\n",
      "Epoch [40/50], Loss: 44.70356424050253\n",
      "Epoch [41/50], Loss: 44.67922577154441\n",
      "Epoch [42/50], Loss: 44.6773877565978\n",
      "Epoch [43/50], Loss: 44.69024089125336\n",
      "Epoch [44/50], Loss: 44.66577973912974\n",
      "Epoch [45/50], Loss: 44.68079921534804\n",
      "Epoch [46/50], Loss: 44.668573898565576\n",
      "Epoch [47/50], Loss: 44.709909745513414\n",
      "Epoch [48/50], Loss: 44.66893014126136\n",
      "Epoch [49/50], Loss: 44.67266721881804\n",
      "Epoch [50/50], Loss: 44.69024355528784\n",
      "Test Loss: 41.534769626064154\n",
      "Training with lr=0.01, hidden1=4, hidden2=4\n",
      "Epoch [1/50], Loss: 48.21374734034304\n",
      "Epoch [2/50], Loss: 44.8821780220407\n",
      "Epoch [3/50], Loss: 44.79935581332347\n",
      "Epoch [4/50], Loss: 44.75401860221488\n",
      "Epoch [5/50], Loss: 44.775618484371996\n",
      "Epoch [6/50], Loss: 44.797064021376315\n",
      "Epoch [7/50], Loss: 44.76060375151087\n",
      "Epoch [8/50], Loss: 44.71883145316703\n",
      "Epoch [9/50], Loss: 44.74935169845331\n",
      "Epoch [10/50], Loss: 44.72848725240738\n",
      "Epoch [11/50], Loss: 44.76808216532723\n",
      "Epoch [12/50], Loss: 44.760543863890604\n",
      "Epoch [13/50], Loss: 44.76264357019643\n",
      "Epoch [14/50], Loss: 44.817290950212325\n",
      "Epoch [15/50], Loss: 44.74693125740426\n",
      "Epoch [16/50], Loss: 44.75358580292249\n",
      "Epoch [17/50], Loss: 44.78237250281162\n",
      "Epoch [18/50], Loss: 44.763509997383494\n",
      "Epoch [19/50], Loss: 44.723094246035714\n",
      "Epoch [20/50], Loss: 44.722191891904735\n",
      "Epoch [21/50], Loss: 44.747171908519306\n",
      "Epoch [22/50], Loss: 44.68919953518226\n",
      "Epoch [23/50], Loss: 44.77639681706663\n",
      "Epoch [24/50], Loss: 44.74615100172699\n",
      "Epoch [25/50], Loss: 44.76306051660757\n",
      "Epoch [26/50], Loss: 44.72358583544121\n",
      "Epoch [27/50], Loss: 44.713148942540904\n",
      "Epoch [28/50], Loss: 44.771747939313045\n",
      "Epoch [29/50], Loss: 44.78459619615899\n",
      "Epoch [30/50], Loss: 44.72542573897565\n",
      "Epoch [31/50], Loss: 44.76565486094991\n",
      "Epoch [32/50], Loss: 44.73325521125168\n",
      "Epoch [33/50], Loss: 44.7300503965284\n",
      "Epoch [34/50], Loss: 44.760203733600555\n",
      "Epoch [35/50], Loss: 44.71832586820008\n",
      "Epoch [36/50], Loss: 44.71493007472304\n",
      "Epoch [37/50], Loss: 44.754046243136045\n",
      "Epoch [38/50], Loss: 44.707223885958314\n",
      "Epoch [39/50], Loss: 44.72335549651599\n",
      "Epoch [40/50], Loss: 44.73193809634349\n",
      "Epoch [41/50], Loss: 44.700864066452276\n",
      "Epoch [42/50], Loss: 44.72378126988645\n",
      "Epoch [43/50], Loss: 44.708742864014674\n",
      "Epoch [44/50], Loss: 44.73841742218518\n",
      "Epoch [45/50], Loss: 44.703604357359836\n",
      "Epoch [46/50], Loss: 44.74946554215228\n",
      "Epoch [47/50], Loss: 44.75913646885606\n",
      "Epoch [48/50], Loss: 44.72109628583564\n",
      "Epoch [49/50], Loss: 44.68926664258613\n",
      "Epoch [50/50], Loss: 44.68222669382564\n",
      "Test Loss: 41.75788598388206\n",
      "Training with lr=0.01, hidden1=4, hidden2=8\n",
      "Epoch [1/50], Loss: 47.42736716035937\n",
      "Epoch [2/50], Loss: 44.81720641089267\n",
      "Epoch [3/50], Loss: 44.75564377581487\n",
      "Epoch [4/50], Loss: 44.769998143930906\n",
      "Epoch [5/50], Loss: 44.77108713368901\n",
      "Epoch [6/50], Loss: 44.79076927685347\n",
      "Epoch [7/50], Loss: 44.7877136918365\n",
      "Epoch [8/50], Loss: 44.74812655839764\n",
      "Epoch [9/50], Loss: 44.72276674489506\n",
      "Epoch [10/50], Loss: 44.78569400662281\n",
      "Epoch [11/50], Loss: 44.74686939051894\n",
      "Epoch [12/50], Loss: 44.68980452740779\n",
      "Epoch [13/50], Loss: 44.76240016749648\n",
      "Epoch [14/50], Loss: 44.83640390614994\n",
      "Epoch [15/50], Loss: 44.81257470553039\n",
      "Epoch [16/50], Loss: 44.74574880130955\n",
      "Epoch [17/50], Loss: 44.747255731801516\n",
      "Epoch [18/50], Loss: 44.75799399516622\n",
      "Epoch [19/50], Loss: 44.71359884230817\n",
      "Epoch [20/50], Loss: 44.79021119289711\n",
      "Epoch [21/50], Loss: 44.76816748322034\n",
      "Epoch [22/50], Loss: 44.74379798076192\n",
      "Epoch [23/50], Loss: 44.74681882701936\n",
      "Epoch [24/50], Loss: 44.73783072487252\n",
      "Epoch [25/50], Loss: 44.85129193790623\n",
      "Epoch [26/50], Loss: 44.72262179890617\n",
      "Epoch [27/50], Loss: 44.71808548286313\n",
      "Epoch [28/50], Loss: 44.78189269519243\n",
      "Epoch [29/50], Loss: 44.828474788978454\n",
      "Epoch [30/50], Loss: 44.74506481983622\n",
      "Epoch [31/50], Loss: 44.780273215497125\n",
      "Epoch [32/50], Loss: 44.77116242705799\n",
      "Epoch [33/50], Loss: 44.7444592147577\n",
      "Epoch [34/50], Loss: 44.75460142229424\n",
      "Epoch [35/50], Loss: 44.747224251168674\n",
      "Epoch [36/50], Loss: 44.7143575824675\n",
      "Epoch [37/50], Loss: 44.72750042149278\n",
      "Epoch [38/50], Loss: 44.73967054085654\n",
      "Epoch [39/50], Loss: 44.70857691530321\n",
      "Epoch [40/50], Loss: 44.75478415880047\n",
      "Epoch [41/50], Loss: 44.775328164022476\n",
      "Epoch [42/50], Loss: 44.72990743293137\n",
      "Epoch [43/50], Loss: 44.75543452090904\n",
      "Epoch [44/50], Loss: 44.71840732136711\n",
      "Epoch [45/50], Loss: 44.711837190096496\n",
      "Epoch [46/50], Loss: 44.73663743441222\n",
      "Epoch [47/50], Loss: 44.756731327244495\n",
      "Epoch [48/50], Loss: 44.7238956451416\n",
      "Epoch [49/50], Loss: 44.756347768814834\n",
      "Epoch [50/50], Loss: 44.71319815838923\n",
      "Test Loss: 41.509957335377464\n",
      "Training with lr=0.01, hidden1=4, hidden2=16\n",
      "Epoch [1/50], Loss: 47.487534938874795\n",
      "Epoch [2/50], Loss: 44.8883171144079\n",
      "Epoch [3/50], Loss: 44.755746519370156\n",
      "Epoch [4/50], Loss: 44.823995096175395\n",
      "Epoch [5/50], Loss: 44.79393559127558\n",
      "Epoch [6/50], Loss: 44.73782565007444\n",
      "Epoch [7/50], Loss: 44.800785699437874\n",
      "Epoch [8/50], Loss: 44.77337999187532\n",
      "Epoch [9/50], Loss: 44.78260689407099\n",
      "Epoch [10/50], Loss: 44.73754578262079\n",
      "Epoch [11/50], Loss: 44.80204421496782\n",
      "Epoch [12/50], Loss: 44.742168707925764\n",
      "Epoch [13/50], Loss: 44.7602466833396\n",
      "Epoch [14/50], Loss: 44.817904290996616\n",
      "Epoch [15/50], Loss: 44.77281264946109\n",
      "Epoch [16/50], Loss: 44.810489592005\n",
      "Epoch [17/50], Loss: 44.7885567336786\n",
      "Epoch [18/50], Loss: 44.73388027753986\n",
      "Epoch [19/50], Loss: 44.77239857345331\n",
      "Epoch [20/50], Loss: 44.806087168709176\n",
      "Epoch [21/50], Loss: 44.70956815188048\n",
      "Epoch [22/50], Loss: 44.71626334268539\n",
      "Epoch [23/50], Loss: 44.777988358794666\n",
      "Epoch [24/50], Loss: 44.73144797653448\n",
      "Epoch [25/50], Loss: 44.74325674713635\n",
      "Epoch [26/50], Loss: 44.71079604352107\n",
      "Epoch [27/50], Loss: 44.750136559908505\n",
      "Epoch [28/50], Loss: 44.73482474655402\n",
      "Epoch [29/50], Loss: 44.81138697764913\n",
      "Epoch [30/50], Loss: 44.81467609092837\n",
      "Epoch [31/50], Loss: 44.753471865419485\n",
      "Epoch [32/50], Loss: 44.73483597176974\n",
      "Epoch [33/50], Loss: 44.75591385325448\n",
      "Epoch [34/50], Loss: 44.68503082150318\n",
      "Epoch [35/50], Loss: 44.77980554924637\n",
      "Epoch [36/50], Loss: 44.712982928166625\n",
      "Epoch [37/50], Loss: 44.717699435499846\n",
      "Epoch [38/50], Loss: 44.76926205119149\n",
      "Epoch [39/50], Loss: 44.68957819078789\n",
      "Epoch [40/50], Loss: 44.72365591955967\n",
      "Epoch [41/50], Loss: 44.740015061175235\n",
      "Epoch [42/50], Loss: 44.71789146173196\n",
      "Epoch [43/50], Loss: 44.748514982129706\n",
      "Epoch [44/50], Loss: 44.777726395403754\n",
      "Epoch [45/50], Loss: 44.78322197648345\n",
      "Epoch [46/50], Loss: 44.72998797307249\n",
      "Epoch [47/50], Loss: 44.74763836157126\n",
      "Epoch [48/50], Loss: 44.75762013294658\n",
      "Epoch [49/50], Loss: 44.772042934230114\n",
      "Epoch [50/50], Loss: 44.73912501100634\n",
      "Test Loss: 41.5041191304913\n",
      "Training with lr=0.01, hidden1=8, hidden2=4\n",
      "Epoch [1/50], Loss: 49.18097222437624\n",
      "Epoch [2/50], Loss: 44.926028307930366\n",
      "Epoch [3/50], Loss: 44.820807313137365\n",
      "Epoch [4/50], Loss: 44.76499840783291\n",
      "Epoch [5/50], Loss: 44.725478056610605\n",
      "Epoch [6/50], Loss: 44.78892861037958\n",
      "Epoch [7/50], Loss: 44.73018464260414\n",
      "Epoch [8/50], Loss: 44.688738031856346\n",
      "Epoch [9/50], Loss: 44.78795697571801\n",
      "Epoch [10/50], Loss: 44.7423797826298\n",
      "Epoch [11/50], Loss: 44.740119440047465\n",
      "Epoch [12/50], Loss: 44.717369392269944\n",
      "Epoch [13/50], Loss: 44.71092225059134\n",
      "Epoch [14/50], Loss: 44.73057072123543\n",
      "Epoch [15/50], Loss: 44.770722314177966\n",
      "Epoch [16/50], Loss: 44.72840977653128\n",
      "Epoch [17/50], Loss: 44.71669127042176\n",
      "Epoch [18/50], Loss: 44.729119457182335\n",
      "Epoch [19/50], Loss: 44.743153312558036\n",
      "Epoch [20/50], Loss: 44.72999034944128\n",
      "Epoch [21/50], Loss: 44.720554723895965\n",
      "Epoch [22/50], Loss: 44.70000229507196\n",
      "Epoch [23/50], Loss: 44.71599968456831\n",
      "Epoch [24/50], Loss: 44.719757499069466\n",
      "Epoch [25/50], Loss: 44.79102310430808\n",
      "Epoch [26/50], Loss: 44.643076055558\n",
      "Epoch [27/50], Loss: 44.73232187990283\n",
      "Epoch [28/50], Loss: 44.698622869272704\n",
      "Epoch [29/50], Loss: 44.77733509188793\n",
      "Epoch [30/50], Loss: 44.70124402280714\n",
      "Epoch [31/50], Loss: 44.76231777003554\n",
      "Epoch [32/50], Loss: 44.71831120975682\n",
      "Epoch [33/50], Loss: 44.711371412433564\n",
      "Epoch [34/50], Loss: 44.706594829872\n",
      "Epoch [35/50], Loss: 44.73577015048168\n",
      "Epoch [36/50], Loss: 44.70980976917705\n",
      "Epoch [37/50], Loss: 44.71264691587354\n",
      "Epoch [38/50], Loss: 44.69093387166008\n",
      "Epoch [39/50], Loss: 44.73345348795907\n",
      "Epoch [40/50], Loss: 44.6918465973901\n",
      "Epoch [41/50], Loss: 44.70829188862785\n",
      "Epoch [42/50], Loss: 44.7202039937504\n",
      "Epoch [43/50], Loss: 44.721926883009615\n",
      "Epoch [44/50], Loss: 44.715839426634744\n",
      "Epoch [45/50], Loss: 44.67052508494893\n",
      "Epoch [46/50], Loss: 44.667254063340486\n",
      "Epoch [47/50], Loss: 44.67376899719238\n",
      "Epoch [48/50], Loss: 44.70501889088115\n",
      "Epoch [49/50], Loss: 44.67836464428511\n",
      "Epoch [50/50], Loss: 44.72212761112901\n",
      "Test Loss: 41.56943736185554\n",
      "Training with lr=0.01, hidden1=8, hidden2=8\n",
      "Epoch [1/50], Loss: 47.948578024692225\n",
      "Epoch [2/50], Loss: 44.82034786099293\n",
      "Epoch [3/50], Loss: 44.81631256791412\n",
      "Epoch [4/50], Loss: 44.697061907658814\n",
      "Epoch [5/50], Loss: 44.7696108208328\n",
      "Epoch [6/50], Loss: 44.712250206118725\n",
      "Epoch [7/50], Loss: 44.782542156782306\n",
      "Epoch [8/50], Loss: 44.79828868303142\n",
      "Epoch [9/50], Loss: 44.75069222059406\n",
      "Epoch [10/50], Loss: 44.73972491279977\n",
      "Epoch [11/50], Loss: 44.78776398955799\n",
      "Epoch [12/50], Loss: 44.75566122961826\n",
      "Epoch [13/50], Loss: 44.75290998239986\n",
      "Epoch [14/50], Loss: 44.760243337662494\n",
      "Epoch [15/50], Loss: 44.81351237062548\n",
      "Epoch [16/50], Loss: 44.755713247080315\n",
      "Epoch [17/50], Loss: 44.77128342175093\n",
      "Epoch [18/50], Loss: 44.72776445795278\n",
      "Epoch [19/50], Loss: 44.72176275096956\n",
      "Epoch [20/50], Loss: 44.78256940998015\n",
      "Epoch [21/50], Loss: 44.738155527583885\n",
      "Epoch [22/50], Loss: 44.69885808600754\n",
      "Epoch [23/50], Loss: 44.77165494199659\n",
      "Epoch [24/50], Loss: 44.77518097299044\n",
      "Epoch [25/50], Loss: 44.742617679033124\n",
      "Epoch [26/50], Loss: 44.77111891136795\n",
      "Epoch [27/50], Loss: 44.72495276028992\n",
      "Epoch [28/50], Loss: 44.749971405404516\n",
      "Epoch [29/50], Loss: 44.73012195024334\n",
      "Epoch [30/50], Loss: 44.738921543809234\n",
      "Epoch [31/50], Loss: 44.69700486230069\n",
      "Epoch [32/50], Loss: 44.77989625149086\n",
      "Epoch [33/50], Loss: 44.73000353828805\n",
      "Epoch [34/50], Loss: 44.73301048278809\n",
      "Epoch [35/50], Loss: 44.78983221210417\n",
      "Epoch [36/50], Loss: 44.71802311256284\n",
      "Epoch [37/50], Loss: 44.74842160959713\n",
      "Epoch [38/50], Loss: 44.736284718747996\n",
      "Epoch [39/50], Loss: 44.71769712166708\n",
      "Epoch [40/50], Loss: 44.72902524354028\n",
      "Epoch [41/50], Loss: 44.743930291347816\n",
      "Epoch [42/50], Loss: 44.755318094472415\n",
      "Epoch [43/50], Loss: 44.76690451199891\n",
      "Epoch [44/50], Loss: 44.71474569351947\n",
      "Epoch [45/50], Loss: 44.74631941748447\n",
      "Epoch [46/50], Loss: 44.76294803306705\n",
      "Epoch [47/50], Loss: 44.723894106755495\n",
      "Epoch [48/50], Loss: 44.72999167833172\n",
      "Epoch [49/50], Loss: 44.682109814002864\n",
      "Epoch [50/50], Loss: 44.69934377201268\n",
      "Test Loss: 41.62579326775238\n",
      "Training with lr=0.01, hidden1=8, hidden2=16\n",
      "Epoch [1/50], Loss: 47.8861629611156\n",
      "Epoch [2/50], Loss: 44.82271088772133\n",
      "Epoch [3/50], Loss: 44.77439783440261\n",
      "Epoch [4/50], Loss: 44.73783537755247\n",
      "Epoch [5/50], Loss: 44.78736162342009\n",
      "Epoch [6/50], Loss: 44.730176519175046\n",
      "Epoch [7/50], Loss: 44.790556541818084\n",
      "Epoch [8/50], Loss: 44.76937855579814\n",
      "Epoch [9/50], Loss: 44.80684394523745\n",
      "Epoch [10/50], Loss: 44.84900108087258\n",
      "Epoch [11/50], Loss: 44.80969997781222\n",
      "Epoch [12/50], Loss: 44.71569845168317\n",
      "Epoch [13/50], Loss: 44.79201620133197\n",
      "Epoch [14/50], Loss: 44.772748912748746\n",
      "Epoch [15/50], Loss: 44.75961253682121\n",
      "Epoch [16/50], Loss: 44.78402135254907\n",
      "Epoch [17/50], Loss: 44.815124730594825\n",
      "Epoch [18/50], Loss: 44.74185243200083\n",
      "Epoch [19/50], Loss: 44.8017074647497\n",
      "Epoch [20/50], Loss: 44.71388368450227\n",
      "Epoch [21/50], Loss: 44.74692753651103\n",
      "Epoch [22/50], Loss: 44.708086495321304\n",
      "Epoch [23/50], Loss: 44.752195242584726\n",
      "Epoch [24/50], Loss: 44.730543943311346\n",
      "Epoch [25/50], Loss: 44.7428277000052\n",
      "Epoch [26/50], Loss: 44.722963007942575\n",
      "Epoch [27/50], Loss: 44.70838283476282\n",
      "Epoch [28/50], Loss: 44.7251600421843\n",
      "Epoch [29/50], Loss: 44.702419027734976\n",
      "Epoch [30/50], Loss: 44.71757381001457\n",
      "Epoch [31/50], Loss: 44.69348309000985\n",
      "Epoch [32/50], Loss: 44.73954025956451\n",
      "Epoch [33/50], Loss: 44.784433674421464\n",
      "Epoch [34/50], Loss: 44.74453966422159\n",
      "Epoch [35/50], Loss: 44.74075424944768\n",
      "Epoch [36/50], Loss: 44.71014248269503\n",
      "Epoch [37/50], Loss: 44.72212121056729\n",
      "Epoch [38/50], Loss: 44.73361479962458\n",
      "Epoch [39/50], Loss: 44.697795205038105\n",
      "Epoch [40/50], Loss: 44.766622668407\n",
      "Epoch [41/50], Loss: 44.72091395268675\n",
      "Epoch [42/50], Loss: 44.74570237144095\n",
      "Epoch [43/50], Loss: 44.71920667554512\n",
      "Epoch [44/50], Loss: 44.6864248932385\n",
      "Epoch [45/50], Loss: 44.726531231989625\n",
      "Epoch [46/50], Loss: 44.74081087894127\n",
      "Epoch [47/50], Loss: 44.71770658024022\n",
      "Epoch [48/50], Loss: 44.75579798651523\n",
      "Epoch [49/50], Loss: 44.742845460235095\n",
      "Epoch [50/50], Loss: 44.764283089559584\n",
      "Test Loss: 41.736271253978934\n",
      "Training with lr=0.01, hidden1=16, hidden2=4\n",
      "Epoch [1/50], Loss: 49.557092244507835\n",
      "Epoch [2/50], Loss: 44.900010083933346\n",
      "Epoch [3/50], Loss: 44.83664833444064\n",
      "Epoch [4/50], Loss: 44.77129333371022\n",
      "Epoch [5/50], Loss: 44.75629143949415\n",
      "Epoch [6/50], Loss: 44.74513037634678\n",
      "Epoch [7/50], Loss: 44.697509527988124\n",
      "Epoch [8/50], Loss: 44.74118303705434\n",
      "Epoch [9/50], Loss: 44.74366959118452\n",
      "Epoch [10/50], Loss: 44.757772983488486\n",
      "Epoch [11/50], Loss: 44.765385724677415\n",
      "Epoch [12/50], Loss: 44.736022905443534\n",
      "Epoch [13/50], Loss: 44.715491216690815\n",
      "Epoch [14/50], Loss: 44.78133866982382\n",
      "Epoch [15/50], Loss: 44.76486736672823\n",
      "Epoch [16/50], Loss: 44.7044261838569\n",
      "Epoch [17/50], Loss: 44.769049309902506\n",
      "Epoch [18/50], Loss: 44.694022119240685\n",
      "Epoch [19/50], Loss: 44.68188501264228\n",
      "Epoch [20/50], Loss: 44.73662459264036\n",
      "Epoch [21/50], Loss: 44.71690201055808\n",
      "Epoch [22/50], Loss: 44.73946997533079\n",
      "Epoch [23/50], Loss: 44.75339924233859\n",
      "Epoch [24/50], Loss: 44.696545841654796\n",
      "Epoch [25/50], Loss: 44.75948965979404\n",
      "Epoch [26/50], Loss: 44.707021431844744\n",
      "Epoch [27/50], Loss: 44.72968229700307\n",
      "Epoch [28/50], Loss: 44.71527858796667\n",
      "Epoch [29/50], Loss: 44.71824820784272\n",
      "Epoch [30/50], Loss: 44.730488936627495\n",
      "Epoch [31/50], Loss: 44.74170639788518\n",
      "Epoch [32/50], Loss: 44.726849446531205\n",
      "Epoch [33/50], Loss: 44.74909015092693\n",
      "Epoch [34/50], Loss: 44.69403676517674\n",
      "Epoch [35/50], Loss: 44.71771451606125\n",
      "Epoch [36/50], Loss: 44.80644796090048\n",
      "Epoch [37/50], Loss: 44.73680389904585\n",
      "Epoch [38/50], Loss: 44.74274534006588\n",
      "Epoch [39/50], Loss: 44.7314703269083\n",
      "Epoch [40/50], Loss: 44.75979017038814\n",
      "Epoch [41/50], Loss: 44.73947767351495\n",
      "Epoch [42/50], Loss: 44.70925447432721\n",
      "Epoch [43/50], Loss: 44.693311472408105\n",
      "Epoch [44/50], Loss: 44.71871338515985\n",
      "Epoch [45/50], Loss: 44.68491924786177\n",
      "Epoch [46/50], Loss: 44.713368906740286\n",
      "Epoch [47/50], Loss: 44.72324739049693\n",
      "Epoch [48/50], Loss: 44.75372617127466\n",
      "Epoch [49/50], Loss: 44.696515430387905\n",
      "Epoch [50/50], Loss: 44.687034438086336\n",
      "Test Loss: 41.74556301204303\n",
      "Training with lr=0.01, hidden1=16, hidden2=8\n",
      "Epoch [1/50], Loss: 48.44703183095963\n",
      "Epoch [2/50], Loss: 45.027597583708214\n",
      "Epoch [3/50], Loss: 44.83478595233355\n",
      "Epoch [4/50], Loss: 44.79243003970287\n",
      "Epoch [5/50], Loss: 44.80716007420274\n",
      "Epoch [6/50], Loss: 44.782907761120406\n",
      "Epoch [7/50], Loss: 44.785341062702116\n",
      "Epoch [8/50], Loss: 44.748657639300234\n",
      "Epoch [9/50], Loss: 44.71433107657511\n",
      "Epoch [10/50], Loss: 44.7635727303927\n",
      "Epoch [11/50], Loss: 44.754402379520606\n",
      "Epoch [12/50], Loss: 44.743488668222895\n",
      "Epoch [13/50], Loss: 44.74650211646909\n",
      "Epoch [14/50], Loss: 44.8241232012139\n",
      "Epoch [15/50], Loss: 44.74019467713403\n",
      "Epoch [16/50], Loss: 44.74335673597992\n",
      "Epoch [17/50], Loss: 44.76317328156018\n",
      "Epoch [18/50], Loss: 44.78649383920138\n",
      "Epoch [19/50], Loss: 44.74556555200795\n",
      "Epoch [20/50], Loss: 44.73714930424925\n",
      "Epoch [21/50], Loss: 44.78437164494249\n",
      "Epoch [22/50], Loss: 44.726751039848956\n",
      "Epoch [23/50], Loss: 44.743551035396386\n",
      "Epoch [24/50], Loss: 44.72043308820881\n",
      "Epoch [25/50], Loss: 44.75542730112545\n",
      "Epoch [26/50], Loss: 44.786450067113655\n",
      "Epoch [27/50], Loss: 44.76761114089216\n",
      "Epoch [28/50], Loss: 44.82695635185867\n",
      "Epoch [29/50], Loss: 44.748655938320475\n",
      "Epoch [30/50], Loss: 44.728029226084224\n",
      "Epoch [31/50], Loss: 44.77854085515757\n",
      "Epoch [32/50], Loss: 44.72954752249796\n",
      "Epoch [33/50], Loss: 44.75023200551017\n",
      "Epoch [34/50], Loss: 44.763894640813106\n",
      "Epoch [35/50], Loss: 44.74719092885002\n",
      "Epoch [36/50], Loss: 44.7376045977483\n",
      "Epoch [37/50], Loss: 44.74436102069792\n",
      "Epoch [38/50], Loss: 44.762876560648934\n",
      "Epoch [39/50], Loss: 44.74884241448074\n",
      "Epoch [40/50], Loss: 44.76825682843318\n",
      "Epoch [41/50], Loss: 44.79685100805564\n",
      "Epoch [42/50], Loss: 44.73191685911085\n",
      "Epoch [43/50], Loss: 44.7606999944468\n",
      "Epoch [44/50], Loss: 44.68119464936804\n",
      "Epoch [45/50], Loss: 44.72291313546603\n",
      "Epoch [46/50], Loss: 44.716478879334495\n",
      "Epoch [47/50], Loss: 44.75911692009598\n",
      "Epoch [48/50], Loss: 44.70552526067515\n",
      "Epoch [49/50], Loss: 44.681121375912525\n",
      "Epoch [50/50], Loss: 44.705996147530975\n",
      "Test Loss: 41.582907683976735\n",
      "Training with lr=0.01, hidden1=16, hidden2=16\n",
      "Epoch [1/50], Loss: 47.964336939327055\n",
      "Epoch [2/50], Loss: 44.934245738045114\n",
      "Epoch [3/50], Loss: 44.87559719711054\n",
      "Epoch [4/50], Loss: 44.85836052816422\n",
      "Epoch [5/50], Loss: 44.78973356465824\n",
      "Epoch [6/50], Loss: 44.79551377218278\n",
      "Epoch [7/50], Loss: 44.77833802270108\n",
      "Epoch [8/50], Loss: 44.78276555420923\n",
      "Epoch [9/50], Loss: 44.72330129029321\n",
      "Epoch [10/50], Loss: 44.82349545213043\n",
      "Epoch [11/50], Loss: 44.79696982649506\n",
      "Epoch [12/50], Loss: 44.77272238184194\n",
      "Epoch [13/50], Loss: 44.82392152254699\n",
      "Epoch [14/50], Loss: 44.75392873169946\n",
      "Epoch [15/50], Loss: 44.75476679254751\n",
      "Epoch [16/50], Loss: 44.708221732592975\n",
      "Epoch [17/50], Loss: 44.80300578133004\n",
      "Epoch [18/50], Loss: 44.72455880837362\n",
      "Epoch [19/50], Loss: 44.79555873870849\n",
      "Epoch [20/50], Loss: 44.75083080979644\n",
      "Epoch [21/50], Loss: 44.72635641254362\n",
      "Epoch [22/50], Loss: 44.77146230603828\n",
      "Epoch [23/50], Loss: 44.813412744490826\n",
      "Epoch [24/50], Loss: 44.77185328123999\n",
      "Epoch [25/50], Loss: 44.75262840771284\n",
      "Epoch [26/50], Loss: 44.73542530184886\n",
      "Epoch [27/50], Loss: 44.76011473858943\n",
      "Epoch [28/50], Loss: 44.77784623943391\n",
      "Epoch [29/50], Loss: 44.71792356772501\n",
      "Epoch [30/50], Loss: 44.729879291722035\n",
      "Epoch [31/50], Loss: 44.78917890454902\n",
      "Epoch [32/50], Loss: 44.74839446583732\n",
      "Epoch [33/50], Loss: 44.728285070325505\n",
      "Epoch [34/50], Loss: 44.80015874768867\n",
      "Epoch [35/50], Loss: 44.79036262387135\n",
      "Epoch [36/50], Loss: 44.74141938881796\n",
      "Epoch [37/50], Loss: 44.77184814077909\n",
      "Epoch [38/50], Loss: 44.710578533860506\n",
      "Epoch [39/50], Loss: 44.79181222759309\n",
      "Epoch [40/50], Loss: 44.77674117166488\n",
      "Epoch [41/50], Loss: 44.71964861760374\n",
      "Epoch [42/50], Loss: 44.73375998325035\n",
      "Epoch [43/50], Loss: 44.758247756958006\n",
      "Epoch [44/50], Loss: 44.69450099194636\n",
      "Epoch [45/50], Loss: 44.752352999077466\n",
      "Epoch [46/50], Loss: 44.6990029506996\n",
      "Epoch [47/50], Loss: 44.72400412012319\n",
      "Epoch [48/50], Loss: 44.73165680932217\n",
      "Epoch [49/50], Loss: 44.718557676721794\n",
      "Epoch [50/50], Loss: 44.70544798491431\n",
      "Test Loss: 41.50530059465015\n",
      "Training with lr=0.01, hidden1=32, hidden2=4\n",
      "Epoch [1/50], Loss: 49.16889530869781\n",
      "Epoch [2/50], Loss: 45.1111427494737\n",
      "Epoch [3/50], Loss: 44.95780508322794\n",
      "Epoch [4/50], Loss: 44.80778815160032\n",
      "Epoch [5/50], Loss: 44.725103012460174\n",
      "Epoch [6/50], Loss: 44.830067418833245\n",
      "Epoch [7/50], Loss: 44.777508938898805\n",
      "Epoch [8/50], Loss: 44.796586533843495\n",
      "Epoch [9/50], Loss: 44.779978079874006\n",
      "Epoch [10/50], Loss: 44.79390570218446\n",
      "Epoch [11/50], Loss: 44.78065400358106\n",
      "Epoch [12/50], Loss: 44.80010594852635\n",
      "Epoch [13/50], Loss: 44.78026292832171\n",
      "Epoch [14/50], Loss: 44.71598222451132\n",
      "Epoch [15/50], Loss: 44.71487272919202\n",
      "Epoch [16/50], Loss: 44.735008392959344\n",
      "Epoch [17/50], Loss: 44.80507452917881\n",
      "Epoch [18/50], Loss: 44.745183838390915\n",
      "Epoch [19/50], Loss: 44.697247852262905\n",
      "Epoch [20/50], Loss: 44.73571360228492\n",
      "Epoch [21/50], Loss: 44.750546974432275\n",
      "Epoch [22/50], Loss: 44.75336196774342\n",
      "Epoch [23/50], Loss: 44.6944818152756\n",
      "Epoch [24/50], Loss: 44.7166280840264\n",
      "Epoch [25/50], Loss: 44.73610188531094\n",
      "Epoch [26/50], Loss: 44.77622803860023\n",
      "Epoch [27/50], Loss: 44.73271147618528\n",
      "Epoch [28/50], Loss: 44.77112945181425\n",
      "Epoch [29/50], Loss: 44.71463531118925\n",
      "Epoch [30/50], Loss: 44.72769545883429\n",
      "Epoch [31/50], Loss: 44.74134051213499\n",
      "Epoch [32/50], Loss: 44.75356092609343\n",
      "Epoch [33/50], Loss: 44.71626242340588\n",
      "Epoch [34/50], Loss: 44.73568663049917\n",
      "Epoch [35/50], Loss: 44.708770139100125\n",
      "Epoch [36/50], Loss: 44.70460494619901\n",
      "Epoch [37/50], Loss: 44.6752052588541\n",
      "Epoch [38/50], Loss: 44.716038588226816\n",
      "Epoch [39/50], Loss: 44.6758970041744\n",
      "Epoch [40/50], Loss: 44.73717286782187\n",
      "Epoch [41/50], Loss: 44.72937625822474\n",
      "Epoch [42/50], Loss: 44.70040929200219\n",
      "Epoch [43/50], Loss: 44.698483326396\n",
      "Epoch [44/50], Loss: 44.7297405430528\n",
      "Epoch [45/50], Loss: 44.684975883608956\n",
      "Epoch [46/50], Loss: 44.71544569046771\n",
      "Epoch [47/50], Loss: 44.70659201887787\n",
      "Epoch [48/50], Loss: 44.72579737803975\n",
      "Epoch [49/50], Loss: 44.790431597975434\n",
      "Epoch [50/50], Loss: 44.71209294991415\n",
      "Test Loss: 41.591161363907446\n",
      "Training with lr=0.01, hidden1=32, hidden2=8\n",
      "Epoch [1/50], Loss: 48.069399608549524\n",
      "Epoch [2/50], Loss: 45.141815929725524\n",
      "Epoch [3/50], Loss: 44.955455536138814\n",
      "Epoch [4/50], Loss: 44.83292607479408\n",
      "Epoch [5/50], Loss: 44.78599726254823\n",
      "Epoch [6/50], Loss: 44.777796423239785\n",
      "Epoch [7/50], Loss: 44.826017167138275\n",
      "Epoch [8/50], Loss: 44.83192053622887\n",
      "Epoch [9/50], Loss: 44.7750729388878\n",
      "Epoch [10/50], Loss: 44.80583507350234\n",
      "Epoch [11/50], Loss: 44.752972643492654\n",
      "Epoch [12/50], Loss: 44.73578199167721\n",
      "Epoch [13/50], Loss: 44.73780241168913\n",
      "Epoch [14/50], Loss: 44.75977051531682\n",
      "Epoch [15/50], Loss: 44.73142221794754\n",
      "Epoch [16/50], Loss: 44.747106383276765\n",
      "Epoch [17/50], Loss: 44.755772005925415\n",
      "Epoch [18/50], Loss: 44.75486370774566\n",
      "Epoch [19/50], Loss: 44.746224431522556\n",
      "Epoch [20/50], Loss: 44.70381651706383\n",
      "Epoch [21/50], Loss: 44.756452103911855\n",
      "Epoch [22/50], Loss: 44.72551603473601\n",
      "Epoch [23/50], Loss: 44.76078869553863\n",
      "Epoch [24/50], Loss: 44.7592499091977\n",
      "Epoch [25/50], Loss: 44.73453905699683\n",
      "Epoch [26/50], Loss: 44.73294683987977\n",
      "Epoch [27/50], Loss: 44.69315719291812\n",
      "Epoch [28/50], Loss: 44.718647772366886\n",
      "Epoch [29/50], Loss: 44.725669976531485\n",
      "Epoch [30/50], Loss: 44.7270589171863\n",
      "Epoch [31/50], Loss: 44.695219615248384\n",
      "Epoch [32/50], Loss: 44.72172646444352\n",
      "Epoch [33/50], Loss: 44.74949704467273\n",
      "Epoch [34/50], Loss: 44.72478614556985\n",
      "Epoch [35/50], Loss: 44.7221685753494\n",
      "Epoch [36/50], Loss: 44.74624009679575\n",
      "Epoch [37/50], Loss: 44.739019559641356\n",
      "Epoch [38/50], Loss: 44.74280440533747\n",
      "Epoch [39/50], Loss: 44.726592576699176\n",
      "Epoch [40/50], Loss: 44.74406306157346\n",
      "Epoch [41/50], Loss: 44.727178923810115\n",
      "Epoch [42/50], Loss: 44.75412121131772\n",
      "Epoch [43/50], Loss: 44.741597747802736\n",
      "Epoch [44/50], Loss: 44.72308434033003\n",
      "Epoch [45/50], Loss: 44.72204183046935\n",
      "Epoch [46/50], Loss: 44.740894896085145\n",
      "Epoch [47/50], Loss: 44.71607282044457\n",
      "Epoch [48/50], Loss: 44.702598990768685\n",
      "Epoch [49/50], Loss: 44.65152569129819\n",
      "Epoch [50/50], Loss: 44.72745806584593\n",
      "Test Loss: 41.696807308051426\n",
      "Training with lr=0.01, hidden1=32, hidden2=16\n",
      "Epoch [1/50], Loss: 48.333968853559654\n",
      "Epoch [2/50], Loss: 44.95114737338707\n",
      "Epoch [3/50], Loss: 44.84551000126073\n",
      "Epoch [4/50], Loss: 44.79531283769451\n",
      "Epoch [5/50], Loss: 44.84288916040639\n",
      "Epoch [6/50], Loss: 44.817654787907834\n",
      "Epoch [7/50], Loss: 44.7671944227375\n",
      "Epoch [8/50], Loss: 44.786490868740394\n",
      "Epoch [9/50], Loss: 44.76964207633597\n",
      "Epoch [10/50], Loss: 44.82548696173996\n",
      "Epoch [11/50], Loss: 44.76625379968862\n",
      "Epoch [12/50], Loss: 44.74959433196021\n",
      "Epoch [13/50], Loss: 44.760209724551345\n",
      "Epoch [14/50], Loss: 44.71936376602923\n",
      "Epoch [15/50], Loss: 44.774817538652265\n",
      "Epoch [16/50], Loss: 44.73264226757112\n",
      "Epoch [17/50], Loss: 44.73295546985064\n",
      "Epoch [18/50], Loss: 44.71117786970295\n",
      "Epoch [19/50], Loss: 44.79271484124856\n",
      "Epoch [20/50], Loss: 44.726222729292076\n",
      "Epoch [21/50], Loss: 44.765669025358605\n",
      "Epoch [22/50], Loss: 44.74106562254859\n",
      "Epoch [23/50], Loss: 44.744361577268506\n",
      "Epoch [24/50], Loss: 44.73550368137047\n",
      "Epoch [25/50], Loss: 44.75039161306913\n",
      "Epoch [26/50], Loss: 44.76607489664047\n",
      "Epoch [27/50], Loss: 44.71721508463875\n",
      "Epoch [28/50], Loss: 44.73793426263528\n",
      "Epoch [29/50], Loss: 44.76684211355741\n",
      "Epoch [30/50], Loss: 44.745638068777616\n",
      "Epoch [31/50], Loss: 44.709805347880376\n",
      "Epoch [32/50], Loss: 44.72949553317711\n",
      "Epoch [33/50], Loss: 44.74402631540767\n",
      "Epoch [34/50], Loss: 44.772669051123444\n",
      "Epoch [35/50], Loss: 44.718556910655536\n",
      "Epoch [36/50], Loss: 44.77751003327917\n",
      "Epoch [37/50], Loss: 44.770182900350605\n",
      "Epoch [38/50], Loss: 44.72843680459945\n",
      "Epoch [39/50], Loss: 44.75375929660485\n",
      "Epoch [40/50], Loss: 44.773436511930875\n",
      "Epoch [41/50], Loss: 44.74713580960133\n",
      "Epoch [42/50], Loss: 44.733293239405896\n",
      "Epoch [43/50], Loss: 44.73658170231053\n",
      "Epoch [44/50], Loss: 44.745053238165184\n",
      "Epoch [45/50], Loss: 44.776954944798206\n",
      "Epoch [46/50], Loss: 44.68679261754771\n",
      "Epoch [47/50], Loss: 44.73333508225738\n",
      "Epoch [48/50], Loss: 44.70193731902076\n",
      "Epoch [49/50], Loss: 44.73829879760742\n",
      "Epoch [50/50], Loss: 44.74130453516225\n",
      "Test Loss: 41.510035303712804\n",
      "Training with lr=0.05, hidden1=4, hidden2=4\n",
      "Epoch [1/50], Loss: 48.15860688569116\n",
      "Epoch [2/50], Loss: 44.63861139016073\n",
      "Epoch [3/50], Loss: 44.63330599675413\n",
      "Epoch [4/50], Loss: 44.65410054316286\n",
      "Epoch [5/50], Loss: 44.64108881090508\n",
      "Epoch [6/50], Loss: 44.652610459874886\n",
      "Epoch [7/50], Loss: 44.64732632246174\n",
      "Epoch [8/50], Loss: 44.64872151984543\n",
      "Epoch [9/50], Loss: 44.6373098029465\n",
      "Epoch [10/50], Loss: 44.647625676139455\n",
      "Epoch [11/50], Loss: 44.63231508849097\n",
      "Epoch [12/50], Loss: 44.644762705193195\n",
      "Epoch [13/50], Loss: 44.632948440801904\n",
      "Epoch [14/50], Loss: 44.636078218553884\n",
      "Epoch [15/50], Loss: 44.627468768885876\n",
      "Epoch [16/50], Loss: 44.66854121098753\n",
      "Epoch [17/50], Loss: 44.639519538254035\n",
      "Epoch [18/50], Loss: 44.63163088814157\n",
      "Epoch [19/50], Loss: 44.62922370785572\n",
      "Epoch [20/50], Loss: 44.64847527175653\n",
      "Epoch [21/50], Loss: 44.65570579278665\n",
      "Epoch [22/50], Loss: 44.655465623198964\n",
      "Epoch [23/50], Loss: 44.65506741883325\n",
      "Epoch [24/50], Loss: 44.666602481779506\n",
      "Epoch [25/50], Loss: 44.62740777750484\n",
      "Epoch [26/50], Loss: 44.64467367578725\n",
      "Epoch [27/50], Loss: 44.63337424856717\n",
      "Epoch [28/50], Loss: 44.645476238063125\n",
      "Epoch [29/50], Loss: 44.679106471577626\n",
      "Epoch [30/50], Loss: 44.64729239667048\n",
      "Epoch [31/50], Loss: 44.63820096000296\n",
      "Epoch [32/50], Loss: 44.662276371189805\n",
      "Epoch [33/50], Loss: 44.6418854885414\n",
      "Epoch [34/50], Loss: 44.63658017017802\n",
      "Epoch [35/50], Loss: 44.66335287875817\n",
      "Epoch [36/50], Loss: 44.63252879908828\n",
      "Epoch [37/50], Loss: 44.64110017370005\n",
      "Epoch [38/50], Loss: 44.65887635027776\n",
      "Epoch [39/50], Loss: 44.63917436443391\n",
      "Epoch [40/50], Loss: 44.638472363206205\n",
      "Epoch [41/50], Loss: 44.63225309028\n",
      "Epoch [42/50], Loss: 44.652242604240044\n",
      "Epoch [43/50], Loss: 44.64750532869433\n",
      "Epoch [44/50], Loss: 44.629452636593676\n",
      "Epoch [45/50], Loss: 44.648621702975916\n",
      "Epoch [46/50], Loss: 44.64686363095143\n",
      "Epoch [47/50], Loss: 44.671188839146346\n",
      "Epoch [48/50], Loss: 44.64423731819528\n",
      "Epoch [49/50], Loss: 44.64598681965812\n",
      "Epoch [50/50], Loss: 44.64764168536077\n",
      "Test Loss: 41.52435779571533\n",
      "Training with lr=0.05, hidden1=4, hidden2=8\n",
      "Epoch [1/50], Loss: 48.70676053156618\n",
      "Epoch [2/50], Loss: 45.29360499772869\n",
      "Epoch [3/50], Loss: 44.87132293200884\n",
      "Epoch [4/50], Loss: 44.76643826218902\n",
      "Epoch [5/50], Loss: 44.679459468653945\n",
      "Epoch [6/50], Loss: 44.64567950201816\n",
      "Epoch [7/50], Loss: 44.6552218890581\n",
      "Epoch [8/50], Loss: 44.64499303473801\n",
      "Epoch [9/50], Loss: 44.65272107358839\n",
      "Epoch [10/50], Loss: 44.626280453166025\n",
      "Epoch [11/50], Loss: 44.683957034251726\n",
      "Epoch [12/50], Loss: 44.647580087380334\n",
      "Epoch [13/50], Loss: 44.63702705258229\n",
      "Epoch [14/50], Loss: 44.651017248435096\n",
      "Epoch [15/50], Loss: 44.62066619435295\n",
      "Epoch [16/50], Loss: 44.65169041743044\n",
      "Epoch [17/50], Loss: 44.681367292560516\n",
      "Epoch [18/50], Loss: 44.61942220594062\n",
      "Epoch [19/50], Loss: 44.637823148633615\n",
      "Epoch [20/50], Loss: 44.64879771998671\n",
      "Epoch [21/50], Loss: 44.66561254595147\n",
      "Epoch [22/50], Loss: 44.64709033028024\n",
      "Epoch [23/50], Loss: 44.63243628329918\n",
      "Epoch [24/50], Loss: 44.63502495249764\n",
      "Epoch [25/50], Loss: 44.68035885545074\n",
      "Epoch [26/50], Loss: 44.66216595446477\n",
      "Epoch [27/50], Loss: 44.64646722449631\n",
      "Epoch [28/50], Loss: 44.65925967732414\n",
      "Epoch [29/50], Loss: 44.68171710655337\n",
      "Epoch [30/50], Loss: 44.65513511720251\n",
      "Epoch [31/50], Loss: 44.64520072937012\n",
      "Epoch [32/50], Loss: 44.641231686951684\n",
      "Epoch [33/50], Loss: 44.669733591548734\n",
      "Epoch [34/50], Loss: 44.67713795646292\n",
      "Epoch [35/50], Loss: 44.67320850872603\n",
      "Epoch [36/50], Loss: 44.67437505878386\n",
      "Epoch [37/50], Loss: 44.67322988666472\n",
      "Epoch [38/50], Loss: 44.645311305562004\n",
      "Epoch [39/50], Loss: 44.64546435309238\n",
      "Epoch [40/50], Loss: 44.66331436907659\n",
      "Epoch [41/50], Loss: 44.64260998084897\n",
      "Epoch [42/50], Loss: 44.66489963531494\n",
      "Epoch [43/50], Loss: 44.644921674884735\n",
      "Epoch [44/50], Loss: 44.643929071895414\n",
      "Epoch [45/50], Loss: 44.643336258559934\n",
      "Epoch [46/50], Loss: 44.63311395801482\n",
      "Epoch [47/50], Loss: 44.64180880061916\n",
      "Epoch [48/50], Loss: 44.64898476522477\n",
      "Epoch [49/50], Loss: 44.66443368880475\n",
      "Epoch [50/50], Loss: 44.64498823197162\n",
      "Test Loss: 41.51233092155165\n",
      "Training with lr=0.05, hidden1=4, hidden2=16\n",
      "Epoch [1/50], Loss: 48.52188179141185\n",
      "Epoch [2/50], Loss: 45.178477990822714\n",
      "Epoch [3/50], Loss: 44.846288643508665\n",
      "Epoch [4/50], Loss: 44.77483967640361\n",
      "Epoch [5/50], Loss: 44.675447351424424\n",
      "Epoch [6/50], Loss: 44.645496036967295\n",
      "Epoch [7/50], Loss: 44.64246415310219\n",
      "Epoch [8/50], Loss: 44.63542169039367\n",
      "Epoch [9/50], Loss: 44.65527958479084\n",
      "Epoch [10/50], Loss: 44.64767443547483\n",
      "Epoch [11/50], Loss: 44.621304981044084\n",
      "Epoch [12/50], Loss: 44.65730345366431\n",
      "Epoch [13/50], Loss: 44.70312441216141\n",
      "Epoch [14/50], Loss: 44.692284321394126\n",
      "Epoch [15/50], Loss: 44.6580539203081\n",
      "Epoch [16/50], Loss: 44.65161953910452\n",
      "Epoch [17/50], Loss: 44.648514525616754\n",
      "Epoch [18/50], Loss: 44.63441140221768\n",
      "Epoch [19/50], Loss: 44.64725042249336\n",
      "Epoch [20/50], Loss: 44.62405367366603\n",
      "Epoch [21/50], Loss: 44.622547693721586\n",
      "Epoch [22/50], Loss: 44.65667664574795\n",
      "Epoch [23/50], Loss: 44.660867431515555\n",
      "Epoch [24/50], Loss: 44.6740406223985\n",
      "Epoch [25/50], Loss: 44.64763276459741\n",
      "Epoch [26/50], Loss: 44.62359931820729\n",
      "Epoch [27/50], Loss: 44.66112577094406\n",
      "Epoch [28/50], Loss: 44.65062852765693\n",
      "Epoch [29/50], Loss: 44.64091327229484\n",
      "Epoch [30/50], Loss: 44.637401293144855\n",
      "Epoch [31/50], Loss: 44.639693056950804\n",
      "Epoch [32/50], Loss: 44.61981175844787\n",
      "Epoch [33/50], Loss: 44.61078516850706\n",
      "Epoch [34/50], Loss: 44.656467694141824\n",
      "Epoch [35/50], Loss: 44.67028655380499\n",
      "Epoch [36/50], Loss: 44.64038299498011\n",
      "Epoch [37/50], Loss: 44.65596015805104\n",
      "Epoch [38/50], Loss: 44.633517718705974\n",
      "Epoch [39/50], Loss: 44.658894692092645\n",
      "Epoch [40/50], Loss: 44.642577293270925\n",
      "Epoch [41/50], Loss: 44.680786057769275\n",
      "Epoch [42/50], Loss: 44.638289833068846\n",
      "Epoch [43/50], Loss: 44.64851398155337\n",
      "Epoch [44/50], Loss: 44.638138696013904\n",
      "Epoch [45/50], Loss: 44.71694956920186\n",
      "Epoch [46/50], Loss: 44.631561310564884\n",
      "Epoch [47/50], Loss: 44.626011570164415\n",
      "Epoch [48/50], Loss: 44.641368168690164\n",
      "Epoch [49/50], Loss: 44.631745510414\n",
      "Epoch [50/50], Loss: 44.66446244286709\n",
      "Test Loss: 41.60536859235691\n",
      "Training with lr=0.05, hidden1=8, hidden2=4\n",
      "Epoch [1/50], Loss: 47.02593505734303\n",
      "Epoch [2/50], Loss: 44.63859146618452\n",
      "Epoch [3/50], Loss: 44.635044197958024\n",
      "Epoch [4/50], Loss: 44.66396602568079\n",
      "Epoch [5/50], Loss: 44.62949758435859\n",
      "Epoch [6/50], Loss: 44.663759663065925\n",
      "Epoch [7/50], Loss: 44.680574892388016\n",
      "Epoch [8/50], Loss: 44.68561335704366\n",
      "Epoch [9/50], Loss: 44.63565596283459\n",
      "Epoch [10/50], Loss: 44.69133191030534\n",
      "Epoch [11/50], Loss: 44.63739034308762\n",
      "Epoch [12/50], Loss: 44.65676569078789\n",
      "Epoch [13/50], Loss: 44.655831390130714\n",
      "Epoch [14/50], Loss: 44.61969036665119\n",
      "Epoch [15/50], Loss: 44.63104941571345\n",
      "Epoch [16/50], Loss: 44.627106013063525\n",
      "Epoch [17/50], Loss: 44.63104787732734\n",
      "Epoch [18/50], Loss: 44.64396995169218\n",
      "Epoch [19/50], Loss: 44.651500723791905\n",
      "Epoch [20/50], Loss: 44.66541903448886\n",
      "Epoch [21/50], Loss: 44.68215738515385\n",
      "Epoch [22/50], Loss: 44.634706594123216\n",
      "Epoch [23/50], Loss: 44.6488875029517\n",
      "Epoch [24/50], Loss: 44.68204865690137\n",
      "Epoch [25/50], Loss: 44.67678303327717\n",
      "Epoch [26/50], Loss: 44.64111047338267\n",
      "Epoch [27/50], Loss: 44.65870672757508\n",
      "Epoch [28/50], Loss: 44.61982485661741\n",
      "Epoch [29/50], Loss: 44.63778636494621\n",
      "Epoch [30/50], Loss: 44.65849275119969\n",
      "Epoch [31/50], Loss: 44.65192041240755\n",
      "Epoch [32/50], Loss: 44.63857046033515\n",
      "Epoch [33/50], Loss: 44.63843117385614\n",
      "Epoch [34/50], Loss: 44.67698842345691\n",
      "Epoch [35/50], Loss: 44.6541112055544\n",
      "Epoch [36/50], Loss: 44.65616120510414\n",
      "Epoch [37/50], Loss: 44.64853825803663\n",
      "Epoch [38/50], Loss: 44.63597690394667\n",
      "Epoch [39/50], Loss: 44.64857337826588\n",
      "Epoch [40/50], Loss: 44.6335120623229\n",
      "Epoch [41/50], Loss: 44.63505199619981\n",
      "Epoch [42/50], Loss: 44.63549831577989\n",
      "Epoch [43/50], Loss: 44.64878974351726\n",
      "Epoch [44/50], Loss: 44.638535052440204\n",
      "Epoch [45/50], Loss: 44.633114305089734\n",
      "Epoch [46/50], Loss: 44.619946601742605\n",
      "Epoch [47/50], Loss: 44.63217221244437\n",
      "Epoch [48/50], Loss: 44.62005096185403\n",
      "Epoch [49/50], Loss: 44.65202836208656\n",
      "Epoch [50/50], Loss: 44.65444778067167\n",
      "Test Loss: 41.504989136266346\n",
      "Training with lr=0.05, hidden1=8, hidden2=8\n",
      "Epoch [1/50], Loss: 49.72119156259005\n",
      "Epoch [2/50], Loss: 44.843645095825195\n",
      "Epoch [3/50], Loss: 44.75039731635422\n",
      "Epoch [4/50], Loss: 44.677551181980824\n",
      "Epoch [5/50], Loss: 44.684156030123354\n",
      "Epoch [6/50], Loss: 44.64883652984119\n",
      "Epoch [7/50], Loss: 44.653127695302494\n",
      "Epoch [8/50], Loss: 44.65978267231925\n",
      "Epoch [9/50], Loss: 44.63660761723753\n",
      "Epoch [10/50], Loss: 44.63786758047635\n",
      "Epoch [11/50], Loss: 44.65446793603115\n",
      "Epoch [12/50], Loss: 44.67757276316158\n",
      "Epoch [13/50], Loss: 44.666003286643104\n",
      "Epoch [14/50], Loss: 44.65668487548828\n",
      "Epoch [15/50], Loss: 44.64641645150107\n",
      "Epoch [16/50], Loss: 44.651602704407736\n",
      "Epoch [17/50], Loss: 44.65681292737116\n",
      "Epoch [18/50], Loss: 44.72171208741235\n",
      "Epoch [19/50], Loss: 44.67321864268819\n",
      "Epoch [20/50], Loss: 44.66417437694112\n",
      "Epoch [21/50], Loss: 44.6593752939193\n",
      "Epoch [22/50], Loss: 44.637134095489\n",
      "Epoch [23/50], Loss: 44.62555833410044\n",
      "Epoch [24/50], Loss: 44.66488402319736\n",
      "Epoch [25/50], Loss: 44.654943019053974\n",
      "Epoch [26/50], Loss: 44.637257022544986\n",
      "Epoch [27/50], Loss: 44.6283887394139\n",
      "Epoch [28/50], Loss: 44.651564745043146\n",
      "Epoch [29/50], Loss: 44.62760888396716\n",
      "Epoch [30/50], Loss: 44.65124284712995\n",
      "Epoch [31/50], Loss: 44.68253770421763\n",
      "Epoch [32/50], Loss: 44.68029637571241\n",
      "Epoch [33/50], Loss: 44.65128391453477\n",
      "Epoch [34/50], Loss: 44.62904806293425\n",
      "Epoch [35/50], Loss: 44.6363838789893\n",
      "Epoch [36/50], Loss: 44.70844437020724\n",
      "Epoch [37/50], Loss: 44.63982403239266\n",
      "Epoch [38/50], Loss: 44.6443537102371\n",
      "Epoch [39/50], Loss: 44.69137834642754\n",
      "Epoch [40/50], Loss: 44.674332853223454\n",
      "Epoch [41/50], Loss: 44.64377598684342\n",
      "Epoch [42/50], Loss: 44.66253362561836\n",
      "Epoch [43/50], Loss: 44.676022485826834\n",
      "Epoch [44/50], Loss: 44.642627547217195\n",
      "Epoch [45/50], Loss: 44.65905055374396\n",
      "Epoch [46/50], Loss: 44.65298570726738\n",
      "Epoch [47/50], Loss: 44.62387440165536\n",
      "Epoch [48/50], Loss: 44.690679937894224\n",
      "Epoch [49/50], Loss: 44.66073447993544\n",
      "Epoch [50/50], Loss: 44.636668846255446\n",
      "Test Loss: 41.55775373036625\n",
      "Training with lr=0.05, hidden1=8, hidden2=16\n",
      "Epoch [1/50], Loss: 53.61513809141565\n",
      "Epoch [2/50], Loss: 44.724358286623094\n",
      "Epoch [3/50], Loss: 44.63531962535421\n",
      "Epoch [4/50], Loss: 44.67229400384622\n",
      "Epoch [5/50], Loss: 44.66351227682145\n",
      "Epoch [6/50], Loss: 44.627536242125466\n",
      "Epoch [7/50], Loss: 44.63863906234992\n",
      "Epoch [8/50], Loss: 44.66151908499295\n",
      "Epoch [9/50], Loss: 44.642039521014105\n",
      "Epoch [10/50], Loss: 44.632861328125\n",
      "Epoch [11/50], Loss: 44.62544238919117\n",
      "Epoch [12/50], Loss: 44.65221492579726\n",
      "Epoch [13/50], Loss: 44.629613238475365\n",
      "Epoch [14/50], Loss: 44.63955166300789\n",
      "Epoch [15/50], Loss: 44.630013631601805\n",
      "Epoch [16/50], Loss: 44.694060378778175\n",
      "Epoch [17/50], Loss: 44.67097836791492\n",
      "Epoch [18/50], Loss: 44.65916425986368\n",
      "Epoch [19/50], Loss: 44.72433744024058\n",
      "Epoch [20/50], Loss: 44.67200172924605\n",
      "Epoch [21/50], Loss: 44.648971745225246\n",
      "Epoch [22/50], Loss: 44.640772334864884\n",
      "Epoch [23/50], Loss: 44.671579042028206\n",
      "Epoch [24/50], Loss: 44.66992310383281\n",
      "Epoch [25/50], Loss: 44.69592057525134\n",
      "Epoch [26/50], Loss: 44.64749859043809\n",
      "Epoch [27/50], Loss: 44.67893189289531\n",
      "Epoch [28/50], Loss: 44.64268925776247\n",
      "Epoch [29/50], Loss: 44.67809941651391\n",
      "Epoch [30/50], Loss: 44.621485813328476\n",
      "Epoch [31/50], Loss: 44.676800809141064\n",
      "Epoch [32/50], Loss: 44.67125082172331\n",
      "Epoch [33/50], Loss: 44.64105362813981\n",
      "Epoch [34/50], Loss: 44.6450798097204\n",
      "Epoch [35/50], Loss: 44.62084194871246\n",
      "Epoch [36/50], Loss: 44.638146928881035\n",
      "Epoch [37/50], Loss: 44.66547381291624\n",
      "Epoch [38/50], Loss: 44.64234279570032\n",
      "Epoch [39/50], Loss: 44.64647822145556\n",
      "Epoch [40/50], Loss: 44.659986583522105\n",
      "Epoch [41/50], Loss: 44.654290987233644\n",
      "Epoch [42/50], Loss: 44.64119863041112\n",
      "Epoch [43/50], Loss: 44.64524516746646\n",
      "Epoch [44/50], Loss: 44.639777899570156\n",
      "Epoch [45/50], Loss: 44.64686635126833\n",
      "Epoch [46/50], Loss: 44.63179219355349\n",
      "Epoch [47/50], Loss: 44.65378756288622\n",
      "Epoch [48/50], Loss: 44.64355271761535\n",
      "Epoch [49/50], Loss: 44.62880186487417\n",
      "Epoch [50/50], Loss: 44.64881793788222\n",
      "Test Loss: 41.508748294742965\n",
      "Training with lr=0.05, hidden1=16, hidden2=4\n",
      "Epoch [1/50], Loss: 54.00193126866075\n",
      "Epoch [2/50], Loss: 44.721548980963036\n",
      "Epoch [3/50], Loss: 44.63010230767922\n",
      "Epoch [4/50], Loss: 44.65231228187436\n",
      "Epoch [5/50], Loss: 44.67281379074347\n",
      "Epoch [6/50], Loss: 44.63261289752898\n",
      "Epoch [7/50], Loss: 44.659332094036166\n",
      "Epoch [8/50], Loss: 44.636013568815635\n",
      "Epoch [9/50], Loss: 44.64036497835253\n",
      "Epoch [10/50], Loss: 44.64923812991283\n",
      "Epoch [11/50], Loss: 44.64840016599561\n",
      "Epoch [12/50], Loss: 44.642749751982144\n",
      "Epoch [13/50], Loss: 44.642485071401126\n",
      "Epoch [14/50], Loss: 44.63952170200035\n",
      "Epoch [15/50], Loss: 44.64282311298808\n",
      "Epoch [16/50], Loss: 44.631855667614545\n",
      "Epoch [17/50], Loss: 44.63907276841461\n",
      "Epoch [18/50], Loss: 44.63891900484679\n",
      "Epoch [19/50], Loss: 44.66740535048188\n",
      "Epoch [20/50], Loss: 44.639115417980754\n",
      "Epoch [21/50], Loss: 44.629932231590395\n",
      "Epoch [22/50], Loss: 44.666094870645495\n",
      "Epoch [23/50], Loss: 44.64834128833208\n",
      "Epoch [24/50], Loss: 44.670358632822506\n",
      "Epoch [25/50], Loss: 44.6646629177156\n",
      "Epoch [26/50], Loss: 44.77647006550773\n",
      "Epoch [27/50], Loss: 44.67475449608975\n",
      "Epoch [28/50], Loss: 44.64819980683874\n",
      "Epoch [29/50], Loss: 44.64733882966589\n",
      "Epoch [30/50], Loss: 44.63318471439549\n",
      "Epoch [31/50], Loss: 44.67276478751761\n",
      "Epoch [32/50], Loss: 44.71821959448643\n",
      "Epoch [33/50], Loss: 44.62863124159516\n",
      "Epoch [34/50], Loss: 44.6698652111116\n",
      "Epoch [35/50], Loss: 44.664640257788484\n",
      "Epoch [36/50], Loss: 44.681742696293064\n",
      "Epoch [37/50], Loss: 44.659138376204695\n",
      "Epoch [38/50], Loss: 44.62796488902608\n",
      "Epoch [39/50], Loss: 44.68715074883133\n",
      "Epoch [40/50], Loss: 44.644239338108754\n",
      "Epoch [41/50], Loss: 44.66637137366123\n",
      "Epoch [42/50], Loss: 44.640090304515404\n",
      "Epoch [43/50], Loss: 44.63935007189141\n",
      "Epoch [44/50], Loss: 44.65074436000136\n",
      "Epoch [45/50], Loss: 44.65497182001833\n",
      "Epoch [46/50], Loss: 44.656010121204815\n",
      "Epoch [47/50], Loss: 44.639169524145906\n",
      "Epoch [48/50], Loss: 44.669155042679584\n",
      "Epoch [49/50], Loss: 44.66270790412778\n",
      "Epoch [50/50], Loss: 44.66534781221483\n",
      "Test Loss: 41.51049533145118\n",
      "Training with lr=0.05, hidden1=16, hidden2=8\n",
      "Epoch [1/50], Loss: 50.63799503514024\n",
      "Epoch [2/50], Loss: 44.64847415236176\n",
      "Epoch [3/50], Loss: 44.63320133646981\n",
      "Epoch [4/50], Loss: 44.636927401433226\n",
      "Epoch [5/50], Loss: 44.63713051217501\n",
      "Epoch [6/50], Loss: 44.779545424414465\n",
      "Epoch [7/50], Loss: 44.62916473951496\n",
      "Epoch [8/50], Loss: 44.6603090630203\n",
      "Epoch [9/50], Loss: 44.64195124516721\n",
      "Epoch [10/50], Loss: 44.66007272689069\n",
      "Epoch [11/50], Loss: 44.65619426789831\n",
      "Epoch [12/50], Loss: 44.64272317104652\n",
      "Epoch [13/50], Loss: 44.66383067897109\n",
      "Epoch [14/50], Loss: 44.64860832964788\n",
      "Epoch [15/50], Loss: 44.63937010217885\n",
      "Epoch [16/50], Loss: 44.69164050368012\n",
      "Epoch [17/50], Loss: 44.63522073089099\n",
      "Epoch [18/50], Loss: 44.656768198482325\n",
      "Epoch [19/50], Loss: 44.65438684869985\n",
      "Epoch [20/50], Loss: 44.6277482454894\n",
      "Epoch [21/50], Loss: 44.69405330595423\n",
      "Epoch [22/50], Loss: 44.669225517648165\n",
      "Epoch [23/50], Loss: 44.64103632129606\n",
      "Epoch [24/50], Loss: 44.638729064190976\n",
      "Epoch [25/50], Loss: 44.62726747168869\n",
      "Epoch [26/50], Loss: 44.64056680085229\n",
      "Epoch [27/50], Loss: 44.67530468800029\n",
      "Epoch [28/50], Loss: 44.629137558233545\n",
      "Epoch [29/50], Loss: 44.669181142087844\n",
      "Epoch [30/50], Loss: 44.68013385710169\n",
      "Epoch [31/50], Loss: 44.64665875669385\n",
      "Epoch [32/50], Loss: 44.641827323788505\n",
      "Epoch [33/50], Loss: 44.66737486417176\n",
      "Epoch [34/50], Loss: 44.644798710307136\n",
      "Epoch [35/50], Loss: 44.70444886879843\n",
      "Epoch [36/50], Loss: 44.67957848095503\n",
      "Epoch [37/50], Loss: 44.66386240974801\n",
      "Epoch [38/50], Loss: 44.660803688549606\n",
      "Epoch [39/50], Loss: 44.644593854810374\n",
      "Epoch [40/50], Loss: 44.71120694269899\n",
      "Epoch [41/50], Loss: 44.658277893066405\n",
      "Epoch [42/50], Loss: 44.64112585099017\n",
      "Epoch [43/50], Loss: 44.63746269101002\n",
      "Epoch [44/50], Loss: 44.65481549872727\n",
      "Epoch [45/50], Loss: 44.64527103236464\n",
      "Epoch [46/50], Loss: 44.64783633497895\n",
      "Epoch [47/50], Loss: 44.62764509857678\n",
      "Epoch [48/50], Loss: 44.66269992140473\n",
      "Epoch [49/50], Loss: 44.63226577883861\n",
      "Epoch [50/50], Loss: 44.62054863914115\n",
      "Test Loss: 41.513596731287834\n",
      "Training with lr=0.05, hidden1=16, hidden2=16\n",
      "Epoch [1/50], Loss: 49.2251494986112\n",
      "Epoch [2/50], Loss: 44.602142477817225\n",
      "Epoch [3/50], Loss: 44.67137010918289\n",
      "Epoch [4/50], Loss: 44.685440616920346\n",
      "Epoch [5/50], Loss: 44.697241592407224\n",
      "Epoch [6/50], Loss: 44.652107157472706\n",
      "Epoch [7/50], Loss: 44.6497712307289\n",
      "Epoch [8/50], Loss: 44.61124218174669\n",
      "Epoch [9/50], Loss: 44.65077720391946\n",
      "Epoch [10/50], Loss: 44.63421168718182\n",
      "Epoch [11/50], Loss: 44.651793720683116\n",
      "Epoch [12/50], Loss: 44.63971880303055\n",
      "Epoch [13/50], Loss: 44.663471565871944\n",
      "Epoch [14/50], Loss: 44.63444304544418\n",
      "Epoch [15/50], Loss: 44.6424916126689\n",
      "Epoch [16/50], Loss: 44.70498369560867\n",
      "Epoch [17/50], Loss: 44.71870963299861\n",
      "Epoch [18/50], Loss: 44.64442586429784\n",
      "Epoch [19/50], Loss: 44.691445303744956\n",
      "Epoch [20/50], Loss: 44.63931667765633\n",
      "Epoch [21/50], Loss: 44.6437466730837\n",
      "Epoch [22/50], Loss: 44.667958193919695\n",
      "Epoch [23/50], Loss: 44.632396429093156\n",
      "Epoch [24/50], Loss: 44.651860490392465\n",
      "Epoch [25/50], Loss: 44.670939286028755\n",
      "Epoch [26/50], Loss: 44.64111403480905\n",
      "Epoch [27/50], Loss: 44.639905285444414\n",
      "Epoch [28/50], Loss: 44.616179525656776\n",
      "Epoch [29/50], Loss: 44.713830703985494\n",
      "Epoch [30/50], Loss: 44.628519583530114\n",
      "Epoch [31/50], Loss: 44.644064506155544\n",
      "Epoch [32/50], Loss: 44.636589969572476\n",
      "Epoch [33/50], Loss: 44.643209382354236\n",
      "Epoch [34/50], Loss: 44.671971946466165\n",
      "Epoch [35/50], Loss: 44.71667383850598\n",
      "Epoch [36/50], Loss: 44.63650679666488\n",
      "Epoch [37/50], Loss: 44.64319562755647\n",
      "Epoch [38/50], Loss: 44.65662263338683\n",
      "Epoch [39/50], Loss: 44.64296431932293\n",
      "Epoch [40/50], Loss: 44.64602952863349\n",
      "Epoch [41/50], Loss: 44.62970606694456\n",
      "Epoch [42/50], Loss: 44.624363502127224\n",
      "Epoch [43/50], Loss: 44.63285115351442\n",
      "Epoch [44/50], Loss: 44.636229480680875\n",
      "Epoch [45/50], Loss: 44.63360704515801\n",
      "Epoch [46/50], Loss: 44.627436359593126\n",
      "Epoch [47/50], Loss: 44.63646955021092\n",
      "Epoch [48/50], Loss: 44.62074174099281\n",
      "Epoch [49/50], Loss: 44.663000866624174\n",
      "Epoch [50/50], Loss: 44.643307601428425\n",
      "Test Loss: 41.51548643330581\n",
      "Training with lr=0.05, hidden1=32, hidden2=4\n",
      "Epoch [1/50], Loss: 47.60200412312492\n",
      "Epoch [2/50], Loss: 44.90136255983447\n",
      "Epoch [3/50], Loss: 44.86988290255187\n",
      "Epoch [4/50], Loss: 44.75121770764961\n",
      "Epoch [5/50], Loss: 44.66533356338251\n",
      "Epoch [6/50], Loss: 44.667833847296045\n",
      "Epoch [7/50], Loss: 44.63939097982938\n",
      "Epoch [8/50], Loss: 44.631424237861005\n",
      "Epoch [9/50], Loss: 44.64606297602419\n",
      "Epoch [10/50], Loss: 44.66044156434106\n",
      "Epoch [11/50], Loss: 44.67582503459493\n",
      "Epoch [12/50], Loss: 44.6531659986152\n",
      "Epoch [13/50], Loss: 44.63157578765369\n",
      "Epoch [14/50], Loss: 44.657136135414\n",
      "Epoch [15/50], Loss: 44.639505755315064\n",
      "Epoch [16/50], Loss: 44.65584627995725\n",
      "Epoch [17/50], Loss: 44.64587685631924\n",
      "Epoch [18/50], Loss: 44.6502896574677\n",
      "Epoch [19/50], Loss: 44.64997509190294\n",
      "Epoch [20/50], Loss: 44.63277486582271\n",
      "Epoch [21/50], Loss: 44.60012483440462\n",
      "Epoch [22/50], Loss: 44.66384120378338\n",
      "Epoch [23/50], Loss: 44.633966730461744\n",
      "Epoch [24/50], Loss: 44.64763311792593\n",
      "Epoch [25/50], Loss: 44.6377016473989\n",
      "Epoch [26/50], Loss: 44.66028746307873\n",
      "Epoch [27/50], Loss: 44.66721187028728\n",
      "Epoch [28/50], Loss: 44.66255356835537\n",
      "Epoch [29/50], Loss: 44.652006877836634\n",
      "Epoch [30/50], Loss: 44.65032519356149\n",
      "Epoch [31/50], Loss: 44.62836096091348\n",
      "Epoch [32/50], Loss: 44.674740706897175\n",
      "Epoch [33/50], Loss: 44.62687075880707\n",
      "Epoch [34/50], Loss: 44.6534429769047\n",
      "Epoch [35/50], Loss: 44.65107688278449\n",
      "Epoch [36/50], Loss: 44.65293458094362\n",
      "Epoch [37/50], Loss: 44.63798814367075\n",
      "Epoch [38/50], Loss: 44.62192918746198\n",
      "Epoch [39/50], Loss: 44.66488983904729\n",
      "Epoch [40/50], Loss: 44.64074936538446\n",
      "Epoch [41/50], Loss: 44.68360932459597\n",
      "Epoch [42/50], Loss: 44.68471812889224\n",
      "Epoch [43/50], Loss: 44.64449062660092\n",
      "Epoch [44/50], Loss: 44.64868339163358\n",
      "Epoch [45/50], Loss: 44.68723934048512\n",
      "Epoch [46/50], Loss: 44.651835313390514\n",
      "Epoch [47/50], Loss: 44.62841017050821\n",
      "Epoch [48/50], Loss: 44.643828895443775\n",
      "Epoch [49/50], Loss: 44.64895086132112\n",
      "Epoch [50/50], Loss: 44.64511751268731\n",
      "Test Loss: 41.51843595868758\n",
      "Training with lr=0.05, hidden1=32, hidden2=8\n",
      "Epoch [1/50], Loss: 47.110457980046505\n",
      "Epoch [2/50], Loss: 44.75631838313869\n",
      "Epoch [3/50], Loss: 44.68147621467465\n",
      "Epoch [4/50], Loss: 44.68950020837002\n",
      "Epoch [5/50], Loss: 44.63931322566798\n",
      "Epoch [6/50], Loss: 44.62299633026123\n",
      "Epoch [7/50], Loss: 44.674016689863365\n",
      "Epoch [8/50], Loss: 44.64538248406082\n",
      "Epoch [9/50], Loss: 44.64960723001449\n",
      "Epoch [10/50], Loss: 44.64032331372871\n",
      "Epoch [11/50], Loss: 44.64461089900283\n",
      "Epoch [12/50], Loss: 44.635354733076255\n",
      "Epoch [13/50], Loss: 44.64379489460929\n",
      "Epoch [14/50], Loss: 44.64384664942006\n",
      "Epoch [15/50], Loss: 44.660758265510935\n",
      "Epoch [16/50], Loss: 44.649479956705065\n",
      "Epoch [17/50], Loss: 44.64052351654553\n",
      "Epoch [18/50], Loss: 44.62546082168329\n",
      "Epoch [19/50], Loss: 44.635016288131965\n",
      "Epoch [20/50], Loss: 44.64325684094038\n",
      "Epoch [21/50], Loss: 44.633133941400246\n",
      "Epoch [22/50], Loss: 44.674804493638334\n",
      "Epoch [23/50], Loss: 44.62427466345615\n",
      "Epoch [24/50], Loss: 44.69494430979744\n",
      "Epoch [25/50], Loss: 44.642902761990904\n",
      "Epoch [26/50], Loss: 44.6819200140531\n",
      "Epoch [27/50], Loss: 44.65163651763416\n",
      "Epoch [28/50], Loss: 44.656777366262965\n",
      "Epoch [29/50], Loss: 44.66402497838755\n",
      "Epoch [30/50], Loss: 44.63582187715124\n",
      "Epoch [31/50], Loss: 44.63854355108543\n",
      "Epoch [32/50], Loss: 44.67011740012247\n",
      "Epoch [33/50], Loss: 44.65603251535384\n",
      "Epoch [34/50], Loss: 44.6734679269009\n",
      "Epoch [35/50], Loss: 44.67436453709837\n",
      "Epoch [36/50], Loss: 44.65511775720315\n",
      "Epoch [37/50], Loss: 44.6525794545158\n",
      "Epoch [38/50], Loss: 44.64839624498711\n",
      "Epoch [39/50], Loss: 44.63998333039831\n",
      "Epoch [40/50], Loss: 44.6426757749964\n",
      "Epoch [41/50], Loss: 44.635226997000274\n",
      "Epoch [42/50], Loss: 44.638834231017064\n",
      "Epoch [43/50], Loss: 44.648831389380284\n",
      "Epoch [44/50], Loss: 44.68229159370797\n",
      "Epoch [45/50], Loss: 44.65681613234223\n",
      "Epoch [46/50], Loss: 44.646276292644565\n",
      "Epoch [47/50], Loss: 44.64592946787349\n",
      "Epoch [48/50], Loss: 44.65262893926902\n",
      "Epoch [49/50], Loss: 44.64334505425125\n",
      "Epoch [50/50], Loss: 44.65494352872254\n",
      "Test Loss: 41.524670746490244\n",
      "Training with lr=0.05, hidden1=32, hidden2=16\n",
      "Epoch [1/50], Loss: 57.3760269165039\n",
      "Epoch [2/50], Loss: 44.63449387472184\n",
      "Epoch [3/50], Loss: 44.69603424072265\n",
      "Epoch [4/50], Loss: 44.63806109819256\n",
      "Epoch [5/50], Loss: 44.64156988800549\n",
      "Epoch [6/50], Loss: 44.6431254746484\n",
      "Epoch [7/50], Loss: 44.623198699951175\n",
      "Epoch [8/50], Loss: 44.65443349431773\n",
      "Epoch [9/50], Loss: 44.622056889143145\n",
      "Epoch [10/50], Loss: 44.646999262200026\n",
      "Epoch [11/50], Loss: 44.63336600319284\n",
      "Epoch [12/50], Loss: 44.63023681953305\n",
      "Epoch [13/50], Loss: 44.654171140076684\n",
      "Epoch [14/50], Loss: 44.645078859172884\n",
      "Epoch [15/50], Loss: 44.6875626986144\n",
      "Epoch [16/50], Loss: 44.66089533196121\n",
      "Epoch [17/50], Loss: 44.656931367467664\n",
      "Epoch [18/50], Loss: 44.671353043102826\n",
      "Epoch [19/50], Loss: 44.622823064835345\n",
      "Epoch [20/50], Loss: 44.63014733361416\n",
      "Epoch [21/50], Loss: 44.66266859711194\n",
      "Epoch [22/50], Loss: 44.62166706147741\n",
      "Epoch [23/50], Loss: 44.6447132923564\n",
      "Epoch [24/50], Loss: 44.635519277854044\n",
      "Epoch [25/50], Loss: 44.66202718078113\n",
      "Epoch [26/50], Loss: 44.62221907943976\n",
      "Epoch [27/50], Loss: 44.62577730397709\n",
      "Epoch [28/50], Loss: 44.62858095012727\n",
      "Epoch [29/50], Loss: 44.624070595913246\n",
      "Epoch [30/50], Loss: 44.6441683597252\n",
      "Epoch [31/50], Loss: 44.66487229769347\n",
      "Epoch [32/50], Loss: 44.64481732728051\n",
      "Epoch [33/50], Loss: 44.65148965804303\n",
      "Epoch [34/50], Loss: 44.67641607816102\n",
      "Epoch [35/50], Loss: 44.67216657419674\n",
      "Epoch [36/50], Loss: 44.663168775839885\n",
      "Epoch [37/50], Loss: 44.647068270698924\n",
      "Epoch [38/50], Loss: 44.65065814784316\n",
      "Epoch [39/50], Loss: 44.64209834239522\n",
      "Epoch [40/50], Loss: 44.6807662213435\n",
      "Epoch [41/50], Loss: 44.63942789796923\n",
      "Epoch [42/50], Loss: 44.63177329829482\n",
      "Epoch [43/50], Loss: 44.640266487246656\n",
      "Epoch [44/50], Loss: 44.635493781918385\n",
      "Epoch [45/50], Loss: 44.68694579327693\n",
      "Epoch [46/50], Loss: 44.65134725727019\n",
      "Epoch [47/50], Loss: 44.638167912842796\n",
      "Epoch [48/50], Loss: 44.6517674493008\n",
      "Epoch [49/50], Loss: 44.64290246807161\n",
      "Epoch [50/50], Loss: 44.65163531068896\n",
      "Test Loss: 41.5500942332144\n",
      "Training with lr=0.1, hidden1=4, hidden2=4\n",
      "Epoch [1/50], Loss: 46.26305252450411\n",
      "Epoch [2/50], Loss: 44.68486151773421\n",
      "Epoch [3/50], Loss: 44.71402514723481\n",
      "Epoch [4/50], Loss: 44.68481698270704\n",
      "Epoch [5/50], Loss: 44.72122468792024\n",
      "Epoch [6/50], Loss: 44.68769175420042\n",
      "Epoch [7/50], Loss: 44.69370648743676\n",
      "Epoch [8/50], Loss: 44.682255854372116\n",
      "Epoch [9/50], Loss: 44.71227083675197\n",
      "Epoch [10/50], Loss: 44.729119460309136\n",
      "Epoch [11/50], Loss: 44.6885154348905\n",
      "Epoch [12/50], Loss: 44.67771787174412\n",
      "Epoch [13/50], Loss: 44.68598894838427\n",
      "Epoch [14/50], Loss: 44.68469626317259\n",
      "Epoch [15/50], Loss: 44.67308165128114\n",
      "Epoch [16/50], Loss: 44.700132545095975\n",
      "Epoch [17/50], Loss: 44.65576041174717\n",
      "Epoch [18/50], Loss: 44.70766135669145\n",
      "Epoch [19/50], Loss: 44.6647114362873\n",
      "Epoch [20/50], Loss: 44.65185169157434\n",
      "Epoch [21/50], Loss: 44.659238987281675\n",
      "Epoch [22/50], Loss: 44.694742890655014\n",
      "Epoch [23/50], Loss: 44.668720295390145\n",
      "Epoch [24/50], Loss: 44.73792009822658\n",
      "Epoch [25/50], Loss: 44.69522292653068\n",
      "Epoch [26/50], Loss: 44.675963367399625\n",
      "Epoch [27/50], Loss: 44.66935199675013\n",
      "Epoch [28/50], Loss: 44.69651684370197\n",
      "Epoch [29/50], Loss: 44.7084129052084\n",
      "Epoch [30/50], Loss: 44.69179858848697\n",
      "Epoch [31/50], Loss: 44.719691260916285\n",
      "Epoch [32/50], Loss: 44.718593372282434\n",
      "Epoch [33/50], Loss: 44.6872061244777\n",
      "Epoch [34/50], Loss: 44.693622188880795\n",
      "Epoch [35/50], Loss: 44.620898997197386\n",
      "Epoch [36/50], Loss: 44.67954594971704\n",
      "Epoch [37/50], Loss: 44.71273663630251\n",
      "Epoch [38/50], Loss: 44.67452699004627\n",
      "Epoch [39/50], Loss: 44.67970551662758\n",
      "Epoch [40/50], Loss: 44.699728837560436\n",
      "Epoch [41/50], Loss: 44.665102871128774\n",
      "Epoch [42/50], Loss: 44.65833408793465\n",
      "Epoch [43/50], Loss: 44.68150792043717\n",
      "Epoch [44/50], Loss: 44.68333445064357\n",
      "Epoch [45/50], Loss: 44.69702715951888\n",
      "Epoch [46/50], Loss: 44.65434967416232\n",
      "Epoch [47/50], Loss: 44.701096691069054\n",
      "Epoch [48/50], Loss: 44.694766541778066\n",
      "Epoch [49/50], Loss: 44.68523066161109\n",
      "Epoch [50/50], Loss: 44.66428361173536\n",
      "Test Loss: 41.504081456715824\n",
      "Training with lr=0.1, hidden1=4, hidden2=8\n",
      "Epoch [1/50], Loss: 52307.25704120573\n",
      "Epoch [2/50], Loss: 44.702894110757796\n",
      "Epoch [3/50], Loss: 44.711976048203766\n",
      "Epoch [4/50], Loss: 44.66366467710401\n",
      "Epoch [5/50], Loss: 44.652187631951\n",
      "Epoch [6/50], Loss: 44.682444037765755\n",
      "Epoch [7/50], Loss: 44.68271501259726\n",
      "Epoch [8/50], Loss: 44.66708788011895\n",
      "Epoch [9/50], Loss: 44.69145046922027\n",
      "Epoch [10/50], Loss: 44.64309610460625\n",
      "Epoch [11/50], Loss: 44.70525026165071\n",
      "Epoch [12/50], Loss: 44.676700742127466\n",
      "Epoch [13/50], Loss: 44.650461359493065\n",
      "Epoch [14/50], Loss: 44.67560754994877\n",
      "Epoch [15/50], Loss: 44.70191566279677\n",
      "Epoch [16/50], Loss: 44.7390281364566\n",
      "Epoch [17/50], Loss: 44.7001709046911\n",
      "Epoch [18/50], Loss: 44.73252724506816\n",
      "Epoch [19/50], Loss: 44.708604018414604\n",
      "Epoch [20/50], Loss: 44.72380739430912\n",
      "Epoch [21/50], Loss: 44.67726693075211\n",
      "Epoch [22/50], Loss: 44.67315315496726\n",
      "Epoch [23/50], Loss: 44.688655840764284\n",
      "Epoch [24/50], Loss: 44.67870628482006\n",
      "Epoch [25/50], Loss: 44.69501666084665\n",
      "Epoch [26/50], Loss: 44.675674375940545\n",
      "Epoch [27/50], Loss: 44.70956339601611\n",
      "Epoch [28/50], Loss: 44.69882098025963\n",
      "Epoch [29/50], Loss: 44.64716292209312\n",
      "Epoch [30/50], Loss: 44.685948700201315\n",
      "Epoch [31/50], Loss: 44.672094176245515\n",
      "Epoch [32/50], Loss: 44.66359659413823\n",
      "Epoch [33/50], Loss: 44.64179516151303\n",
      "Epoch [34/50], Loss: 44.6994752477427\n",
      "Epoch [35/50], Loss: 44.69794632958584\n",
      "Epoch [36/50], Loss: 44.717992000892515\n",
      "Epoch [37/50], Loss: 44.69387864910188\n",
      "Epoch [38/50], Loss: 44.69095441474289\n",
      "Epoch [39/50], Loss: 44.668280870406356\n",
      "Epoch [40/50], Loss: 44.673413089064304\n",
      "Epoch [41/50], Loss: 44.69903174541036\n",
      "Epoch [42/50], Loss: 44.68813349927058\n",
      "Epoch [43/50], Loss: 44.69425161392962\n",
      "Epoch [44/50], Loss: 44.69821637263063\n",
      "Epoch [45/50], Loss: 44.68558662289479\n",
      "Epoch [46/50], Loss: 44.70975429347304\n",
      "Epoch [47/50], Loss: 44.66179870230253\n",
      "Epoch [48/50], Loss: 44.67042437068751\n",
      "Epoch [49/50], Loss: 44.67937552029969\n",
      "Epoch [50/50], Loss: 44.68707543357474\n",
      "Test Loss: 41.686929964837226\n",
      "Training with lr=0.1, hidden1=4, hidden2=16\n",
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Test Loss: nan\n",
      "Training with lr=0.1, hidden1=8, hidden2=4\n",
      "Epoch [1/50], Loss: 46.04618723509741\n",
      "Epoch [2/50], Loss: 44.685676080672465\n",
      "Epoch [3/50], Loss: 44.714614815008446\n",
      "Epoch [4/50], Loss: 44.69516158182113\n",
      "Epoch [5/50], Loss: 44.681783760571086\n",
      "Epoch [6/50], Loss: 44.715371854188014\n",
      "Epoch [7/50], Loss: 44.70042975066138\n",
      "Epoch [8/50], Loss: 44.71654133405842\n",
      "Epoch [9/50], Loss: 44.68289413139468\n",
      "Epoch [10/50], Loss: 44.70021678736953\n",
      "Epoch [11/50], Loss: 44.69735199975186\n",
      "Epoch [12/50], Loss: 44.67655578988497\n",
      "Epoch [13/50], Loss: 44.70938636279497\n",
      "Epoch [14/50], Loss: 44.71434479385126\n",
      "Epoch [15/50], Loss: 44.72056758442863\n",
      "Epoch [16/50], Loss: 44.72597091862413\n",
      "Epoch [17/50], Loss: 44.68696670532226\n",
      "Epoch [18/50], Loss: 44.69054626089628\n",
      "Epoch [19/50], Loss: 44.69039233786161\n",
      "Epoch [20/50], Loss: 44.674052573032064\n",
      "Epoch [21/50], Loss: 44.635530077824825\n",
      "Epoch [22/50], Loss: 44.72817399697226\n",
      "Epoch [23/50], Loss: 44.72138288841873\n",
      "Epoch [24/50], Loss: 44.65790330855573\n",
      "Epoch [25/50], Loss: 44.66940872317455\n",
      "Epoch [26/50], Loss: 44.68119469314325\n",
      "Epoch [27/50], Loss: 44.65016587054143\n",
      "Epoch [28/50], Loss: 44.69514817409828\n",
      "Epoch [29/50], Loss: 44.73926974749956\n",
      "Epoch [30/50], Loss: 44.7012053380247\n",
      "Epoch [31/50], Loss: 44.7061396989666\n",
      "Epoch [32/50], Loss: 44.699707559679375\n",
      "Epoch [33/50], Loss: 44.65738579171603\n",
      "Epoch [34/50], Loss: 44.700749381643824\n",
      "Epoch [35/50], Loss: 44.72482002758589\n",
      "Epoch [36/50], Loss: 44.675598810539874\n",
      "Epoch [37/50], Loss: 44.64705536326424\n",
      "Epoch [38/50], Loss: 44.700835281121925\n",
      "Epoch [39/50], Loss: 44.71191643886879\n",
      "Epoch [40/50], Loss: 44.6625629862801\n",
      "Epoch [41/50], Loss: 44.643746841930955\n",
      "Epoch [42/50], Loss: 44.66500301673764\n",
      "Epoch [43/50], Loss: 44.675551129950854\n",
      "Epoch [44/50], Loss: 44.67279952002353\n",
      "Epoch [45/50], Loss: 44.68609239234299\n",
      "Epoch [46/50], Loss: 44.701191336209654\n",
      "Epoch [47/50], Loss: 44.765154478979895\n",
      "Epoch [48/50], Loss: 44.6722602093806\n",
      "Epoch [49/50], Loss: 44.69939873492132\n",
      "Epoch [50/50], Loss: 44.668784166554936\n",
      "Test Loss: 41.558502634063025\n",
      "Training with lr=0.1, hidden1=8, hidden2=8\n",
      "Epoch [1/50], Loss: 241906930559150.75\n",
      "Epoch [2/50], Loss: 44.69894510488041\n",
      "Epoch [3/50], Loss: 44.7127851361134\n",
      "Epoch [4/50], Loss: 44.72840914491747\n",
      "Epoch [5/50], Loss: 44.6750711347236\n",
      "Epoch [6/50], Loss: 44.64112452835333\n",
      "Epoch [7/50], Loss: 44.69481590145924\n",
      "Epoch [8/50], Loss: 44.7085666031134\n",
      "Epoch [9/50], Loss: 44.7502118814187\n",
      "Epoch [10/50], Loss: 44.67145283495793\n",
      "Epoch [11/50], Loss: 44.71859702125924\n",
      "Epoch [12/50], Loss: 44.69326795984487\n",
      "Epoch [13/50], Loss: 44.68810573015057\n",
      "Epoch [14/50], Loss: 44.64399377791608\n",
      "Epoch [15/50], Loss: 44.69551362834993\n",
      "Epoch [16/50], Loss: 44.70633026498263\n",
      "Epoch [17/50], Loss: 44.693118295513216\n",
      "Epoch [18/50], Loss: 44.68273440814409\n",
      "Epoch [19/50], Loss: 44.69430344378362\n",
      "Epoch [20/50], Loss: 44.71731482959184\n",
      "Epoch [21/50], Loss: 44.68461303710937\n",
      "Epoch [22/50], Loss: 44.65653520802982\n",
      "Epoch [23/50], Loss: 44.69684680876185\n",
      "Epoch [24/50], Loss: 44.66328447685867\n",
      "Epoch [25/50], Loss: 44.6868887510456\n",
      "Epoch [26/50], Loss: 44.732868363427336\n",
      "Epoch [27/50], Loss: 44.71936588600034\n",
      "Epoch [28/50], Loss: 44.66351313356493\n",
      "Epoch [29/50], Loss: 44.7195442137171\n",
      "Epoch [30/50], Loss: 44.71329974815494\n",
      "Epoch [31/50], Loss: 44.70006815175541\n",
      "Epoch [32/50], Loss: 44.68678485057393\n",
      "Epoch [33/50], Loss: 44.74467011748767\n",
      "Epoch [34/50], Loss: 44.69784150670787\n",
      "Epoch [35/50], Loss: 44.763105989675054\n",
      "Epoch [36/50], Loss: 44.715976033445266\n",
      "Epoch [37/50], Loss: 44.692248510141845\n",
      "Epoch [38/50], Loss: 44.7380744746474\n",
      "Epoch [39/50], Loss: 44.68571622567099\n",
      "Epoch [40/50], Loss: 44.71687587050141\n",
      "Epoch [41/50], Loss: 44.71090039737889\n",
      "Epoch [42/50], Loss: 44.6691888621596\n",
      "Epoch [43/50], Loss: 44.713732147216795\n",
      "Epoch [44/50], Loss: 44.72003710387183\n",
      "Epoch [45/50], Loss: 44.70103557148918\n",
      "Epoch [46/50], Loss: 44.682531175457065\n",
      "Epoch [47/50], Loss: 44.70924217036513\n",
      "Epoch [48/50], Loss: 44.69735376326764\n",
      "Epoch [49/50], Loss: 44.652861829663884\n",
      "Epoch [50/50], Loss: 44.70184377451412\n",
      "Test Loss: 41.51894803811576\n",
      "Training with lr=0.1, hidden1=8, hidden2=16\n",
      "Epoch [1/50], Loss: 9555.606912043837\n",
      "Epoch [2/50], Loss: 44.658611435186664\n",
      "Epoch [3/50], Loss: 44.67490006118524\n",
      "Epoch [4/50], Loss: 44.667622982087686\n",
      "Epoch [5/50], Loss: 44.66893495653496\n",
      "Epoch [6/50], Loss: 44.6521231260456\n",
      "Epoch [7/50], Loss: 44.653420432669215\n",
      "Epoch [8/50], Loss: 44.65290790620397\n",
      "Epoch [9/50], Loss: 44.7095166628478\n",
      "Epoch [10/50], Loss: 44.69036613839572\n",
      "Epoch [11/50], Loss: 44.73432484611136\n",
      "Epoch [12/50], Loss: 44.710353738753525\n",
      "Epoch [13/50], Loss: 44.68840650339595\n",
      "Epoch [14/50], Loss: 44.67547146531402\n",
      "Epoch [15/50], Loss: 44.67811847749304\n",
      "Epoch [16/50], Loss: 44.730370293288935\n",
      "Epoch [17/50], Loss: 44.72973291991187\n",
      "Epoch [18/50], Loss: 44.68004248572178\n",
      "Epoch [19/50], Loss: 44.711342333183914\n",
      "Epoch [20/50], Loss: 44.698481884940726\n",
      "Epoch [21/50], Loss: 44.66939476200792\n",
      "Epoch [22/50], Loss: 44.717446755581214\n",
      "Epoch [23/50], Loss: 44.677466770860015\n",
      "Epoch [24/50], Loss: 44.66276656604204\n",
      "Epoch [25/50], Loss: 44.70034561469907\n",
      "Epoch [26/50], Loss: 44.675524145657896\n",
      "Epoch [27/50], Loss: 44.679887921692895\n",
      "Epoch [28/50], Loss: 44.6853087503402\n",
      "Epoch [29/50], Loss: 44.70088978751761\n",
      "Epoch [30/50], Loss: 44.70952023990819\n",
      "Epoch [31/50], Loss: 44.68516030233415\n",
      "Epoch [32/50], Loss: 44.66678864213287\n",
      "Epoch [33/50], Loss: 44.668244765234775\n",
      "Epoch [34/50], Loss: 44.65906164763404\n",
      "Epoch [35/50], Loss: 44.69484215408075\n",
      "Epoch [36/50], Loss: 44.700514974750455\n",
      "Epoch [37/50], Loss: 44.68934435297231\n",
      "Epoch [38/50], Loss: 44.6751805414919\n",
      "Epoch [39/50], Loss: 44.68412265464908\n",
      "Epoch [40/50], Loss: 44.680055749611775\n",
      "Epoch [41/50], Loss: 44.70654565154529\n",
      "Epoch [42/50], Loss: 44.66824278909652\n",
      "Epoch [43/50], Loss: 44.673531107042656\n",
      "Epoch [44/50], Loss: 44.694595918499054\n",
      "Epoch [45/50], Loss: 44.709508107920165\n",
      "Epoch [46/50], Loss: 44.66818526846463\n",
      "Epoch [47/50], Loss: 44.67975547040095\n",
      "Epoch [48/50], Loss: 44.64521848960001\n",
      "Epoch [49/50], Loss: 44.67424996798156\n",
      "Epoch [50/50], Loss: 44.71962401593318\n",
      "Test Loss: 41.52263291919505\n",
      "Training with lr=0.1, hidden1=16, hidden2=4\n",
      "Epoch [1/50], Loss: 46.1891068067707\n",
      "Epoch [2/50], Loss: 44.69886893913394\n",
      "Epoch [3/50], Loss: 44.68781997805736\n",
      "Epoch [4/50], Loss: 44.671156573686446\n",
      "Epoch [5/50], Loss: 44.681623299395454\n",
      "Epoch [6/50], Loss: 44.70290759977747\n",
      "Epoch [7/50], Loss: 44.654407444938286\n",
      "Epoch [8/50], Loss: 44.67500115066278\n",
      "Epoch [9/50], Loss: 44.72798436274294\n",
      "Epoch [10/50], Loss: 44.67637181516554\n",
      "Epoch [11/50], Loss: 44.697238571917424\n",
      "Epoch [12/50], Loss: 44.68580599300197\n",
      "Epoch [13/50], Loss: 44.7553759215308\n",
      "Epoch [14/50], Loss: 44.67960969893659\n",
      "Epoch [15/50], Loss: 44.67116641060251\n",
      "Epoch [16/50], Loss: 44.69364280700684\n",
      "Epoch [17/50], Loss: 44.70668559465252\n",
      "Epoch [18/50], Loss: 44.6747673222276\n",
      "Epoch [19/50], Loss: 44.7019452892366\n",
      "Epoch [20/50], Loss: 44.671531964911786\n",
      "Epoch [21/50], Loss: 44.69259412171411\n",
      "Epoch [22/50], Loss: 44.72561382856525\n",
      "Epoch [23/50], Loss: 44.64416499216048\n",
      "Epoch [24/50], Loss: 44.689263978551644\n",
      "Epoch [25/50], Loss: 44.68811816856509\n",
      "Epoch [26/50], Loss: 44.68290553483806\n",
      "Epoch [27/50], Loss: 44.68382802244093\n",
      "Epoch [28/50], Loss: 44.67518891819188\n",
      "Epoch [29/50], Loss: 44.691410696311074\n",
      "Epoch [30/50], Loss: 44.66432897849161\n",
      "Epoch [31/50], Loss: 44.70153465896356\n",
      "Epoch [32/50], Loss: 44.68419521519395\n",
      "Epoch [33/50], Loss: 44.74547084433134\n",
      "Epoch [34/50], Loss: 44.651897377264305\n",
      "Epoch [35/50], Loss: 44.70968066981581\n",
      "Epoch [36/50], Loss: 44.66266248734271\n",
      "Epoch [37/50], Loss: 44.69471941463283\n",
      "Epoch [38/50], Loss: 44.709025980214605\n",
      "Epoch [39/50], Loss: 44.69662165407274\n",
      "Epoch [40/50], Loss: 44.69737763326676\n",
      "Epoch [41/50], Loss: 44.70129436430384\n",
      "Epoch [42/50], Loss: 44.672442526895495\n",
      "Epoch [43/50], Loss: 44.70235220487\n",
      "Epoch [44/50], Loss: 44.68073613526391\n",
      "Epoch [45/50], Loss: 44.67285652160645\n",
      "Epoch [46/50], Loss: 44.715371084994956\n",
      "Epoch [47/50], Loss: 44.70293592234127\n",
      "Epoch [48/50], Loss: 44.703968479594245\n",
      "Epoch [49/50], Loss: 44.68247695047347\n",
      "Epoch [50/50], Loss: 44.70511196949443\n",
      "Test Loss: 41.655200397695296\n",
      "Training with lr=0.1, hidden1=16, hidden2=8\n",
      "Epoch [1/50], Loss: 65.79368361176037\n",
      "Epoch [2/50], Loss: 44.69513095793177\n",
      "Epoch [3/50], Loss: 44.64922089185871\n",
      "Epoch [4/50], Loss: 44.73691445647693\n",
      "Epoch [5/50], Loss: 44.71877942319776\n",
      "Epoch [6/50], Loss: 44.7017014300237\n",
      "Epoch [7/50], Loss: 44.67336852276912\n",
      "Epoch [8/50], Loss: 44.67676861247078\n",
      "Epoch [9/50], Loss: 44.67865534297756\n",
      "Epoch [10/50], Loss: 44.68612823173648\n",
      "Epoch [11/50], Loss: 44.68493520080066\n",
      "Epoch [12/50], Loss: 44.68177599047051\n",
      "Epoch [13/50], Loss: 44.65936100443856\n",
      "Epoch [14/50], Loss: 44.686501909474856\n",
      "Epoch [15/50], Loss: 44.68474640768083\n",
      "Epoch [16/50], Loss: 44.659519433193516\n",
      "Epoch [17/50], Loss: 44.78120967677382\n",
      "Epoch [18/50], Loss: 44.68599108086258\n",
      "Epoch [19/50], Loss: 44.68979610693259\n",
      "Epoch [20/50], Loss: 44.68359995982686\n",
      "Epoch [21/50], Loss: 44.66884069599089\n",
      "Epoch [22/50], Loss: 44.663355624089476\n",
      "Epoch [23/50], Loss: 44.72543778028645\n",
      "Epoch [24/50], Loss: 44.70888347313052\n",
      "Epoch [25/50], Loss: 44.68497820369533\n",
      "Epoch [26/50], Loss: 44.69667902461818\n",
      "Epoch [27/50], Loss: 44.68655876409812\n",
      "Epoch [28/50], Loss: 44.64233086382757\n",
      "Epoch [29/50], Loss: 44.680863008342804\n",
      "Epoch [30/50], Loss: 44.73417085741387\n",
      "Epoch [31/50], Loss: 44.700506110269515\n",
      "Epoch [32/50], Loss: 44.71128102912277\n",
      "Epoch [33/50], Loss: 44.738993760406\n",
      "Epoch [34/50], Loss: 44.678580862576844\n",
      "Epoch [35/50], Loss: 44.675480526783424\n",
      "Epoch [36/50], Loss: 44.68899751882084\n",
      "Epoch [37/50], Loss: 44.6752535022673\n",
      "Epoch [38/50], Loss: 44.677752291569945\n",
      "Epoch [39/50], Loss: 44.669139880821355\n",
      "Epoch [40/50], Loss: 44.670102654128776\n",
      "Epoch [41/50], Loss: 44.641142623150934\n",
      "Epoch [42/50], Loss: 44.72588499725842\n",
      "Epoch [43/50], Loss: 44.66543551273033\n",
      "Epoch [44/50], Loss: 44.68538902157643\n",
      "Epoch [45/50], Loss: 44.672061038408124\n",
      "Epoch [46/50], Loss: 44.666024530129356\n",
      "Epoch [47/50], Loss: 44.67362645258669\n",
      "Epoch [48/50], Loss: 44.72095305333372\n",
      "Epoch [49/50], Loss: 44.675752671038516\n",
      "Epoch [50/50], Loss: 44.67777758113674\n",
      "Test Loss: 41.633011832492045\n",
      "Training with lr=0.1, hidden1=16, hidden2=16\n",
      "Epoch [1/50], Loss: 46.417134134886695\n",
      "Epoch [2/50], Loss: 44.69154692790547\n",
      "Epoch [3/50], Loss: 44.65742991087867\n",
      "Epoch [4/50], Loss: 44.71814759051213\n",
      "Epoch [5/50], Loss: 44.69903298675037\n",
      "Epoch [6/50], Loss: 44.69874407033451\n",
      "Epoch [7/50], Loss: 44.6844135722176\n",
      "Epoch [8/50], Loss: 44.693327087652484\n",
      "Epoch [9/50], Loss: 44.69693232677022\n",
      "Epoch [10/50], Loss: 44.73461991919846\n",
      "Epoch [11/50], Loss: 44.687719169991915\n",
      "Epoch [12/50], Loss: 44.706238021225225\n",
      "Epoch [13/50], Loss: 44.69812606436307\n",
      "Epoch [14/50], Loss: 44.712301960929494\n",
      "Epoch [15/50], Loss: 44.69647264949611\n",
      "Epoch [16/50], Loss: 44.700710668720184\n",
      "Epoch [17/50], Loss: 44.73258214856757\n",
      "Epoch [18/50], Loss: 44.7312653651003\n",
      "Epoch [19/50], Loss: 44.71389789893979\n",
      "Epoch [20/50], Loss: 44.68982187177314\n",
      "Epoch [21/50], Loss: 44.70163561711546\n",
      "Epoch [22/50], Loss: 44.688106680698084\n",
      "Epoch [23/50], Loss: 44.6932361039959\n",
      "Epoch [24/50], Loss: 44.71380405738706\n",
      "Epoch [25/50], Loss: 44.672196009901704\n",
      "Epoch [26/50], Loss: 44.70779910478436\n",
      "Epoch [27/50], Loss: 44.69355085404193\n",
      "Epoch [28/50], Loss: 44.726884229065945\n",
      "Epoch [29/50], Loss: 44.707332310911085\n",
      "Epoch [30/50], Loss: 44.68199705843065\n",
      "Epoch [31/50], Loss: 44.6906316444522\n",
      "Epoch [32/50], Loss: 44.66458951606125\n",
      "Epoch [33/50], Loss: 44.68183273565574\n",
      "Epoch [34/50], Loss: 44.69404249347624\n",
      "Epoch [35/50], Loss: 44.69854567480869\n",
      "Epoch [36/50], Loss: 44.71336900054431\n",
      "Epoch [37/50], Loss: 44.73879050583136\n",
      "Epoch [38/50], Loss: 44.706988281500145\n",
      "Epoch [39/50], Loss: 44.665593400548715\n",
      "Epoch [40/50], Loss: 44.71899918102827\n",
      "Epoch [41/50], Loss: 44.70484162627673\n",
      "Epoch [42/50], Loss: 44.71530865215864\n",
      "Epoch [43/50], Loss: 44.69807027285216\n",
      "Epoch [44/50], Loss: 44.6991813847276\n",
      "Epoch [45/50], Loss: 44.647477203118996\n",
      "Epoch [46/50], Loss: 44.65586535969719\n",
      "Epoch [47/50], Loss: 44.72454433128482\n",
      "Epoch [48/50], Loss: 44.68548093076612\n",
      "Epoch [49/50], Loss: 44.69478688161881\n",
      "Epoch [50/50], Loss: 44.66281296776943\n",
      "Test Loss: 41.55191919457821\n",
      "Training with lr=0.1, hidden1=32, hidden2=4\n",
      "Epoch [1/50], Loss: 136.7070608451718\n",
      "Epoch [2/50], Loss: 44.70160717573322\n",
      "Epoch [3/50], Loss: 44.708355343928105\n",
      "Epoch [4/50], Loss: 44.685937612564835\n",
      "Epoch [5/50], Loss: 44.6908197059006\n",
      "Epoch [6/50], Loss: 44.66777443494953\n",
      "Epoch [7/50], Loss: 44.71302185683954\n",
      "Epoch [8/50], Loss: 44.69695071861392\n",
      "Epoch [9/50], Loss: 44.679554085653336\n",
      "Epoch [10/50], Loss: 44.718312332278394\n",
      "Epoch [11/50], Loss: 44.69443359375\n",
      "Epoch [12/50], Loss: 44.65738402194664\n",
      "Epoch [13/50], Loss: 44.692764860684754\n",
      "Epoch [14/50], Loss: 44.66590356670442\n",
      "Epoch [15/50], Loss: 44.69970941387239\n",
      "Epoch [16/50], Loss: 44.71928118721384\n",
      "Epoch [17/50], Loss: 44.69212357567959\n",
      "Epoch [18/50], Loss: 44.698309263635856\n",
      "Epoch [19/50], Loss: 44.70374789003466\n",
      "Epoch [20/50], Loss: 44.70363661343934\n",
      "Epoch [21/50], Loss: 44.68416886251481\n",
      "Epoch [22/50], Loss: 44.70494230927014\n",
      "Epoch [23/50], Loss: 44.70496502235287\n",
      "Epoch [24/50], Loss: 44.740063607888146\n",
      "Epoch [25/50], Loss: 44.68961573428795\n",
      "Epoch [26/50], Loss: 44.69638312918241\n",
      "Epoch [27/50], Loss: 44.68368194767686\n",
      "Epoch [28/50], Loss: 44.69076487431761\n",
      "Epoch [29/50], Loss: 44.64414540900559\n",
      "Epoch [30/50], Loss: 44.68153837860608\n",
      "Epoch [31/50], Loss: 44.67227062788166\n",
      "Epoch [32/50], Loss: 44.72380458644179\n",
      "Epoch [33/50], Loss: 44.68889183607258\n",
      "Epoch [34/50], Loss: 44.68917110005363\n",
      "Epoch [35/50], Loss: 44.647802033971566\n",
      "Epoch [36/50], Loss: 44.659945866319\n",
      "Epoch [37/50], Loss: 44.65103379546619\n",
      "Epoch [38/50], Loss: 44.6809085158051\n",
      "Epoch [39/50], Loss: 44.67010922666456\n",
      "Epoch [40/50], Loss: 44.70969833624167\n",
      "Epoch [41/50], Loss: 44.671387706819125\n",
      "Epoch [42/50], Loss: 44.69709601793133\n",
      "Epoch [43/50], Loss: 44.68375393601715\n",
      "Epoch [44/50], Loss: 44.67551537185419\n",
      "Epoch [45/50], Loss: 44.67313486318119\n",
      "Epoch [46/50], Loss: 44.70896926004379\n",
      "Epoch [47/50], Loss: 44.71209110822834\n",
      "Epoch [48/50], Loss: 44.71540141809182\n",
      "Epoch [49/50], Loss: 44.66547036092789\n",
      "Epoch [50/50], Loss: 44.74162741176418\n",
      "Test Loss: 41.51764239973694\n",
      "Training with lr=0.1, hidden1=32, hidden2=8\n",
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Test Loss: nan\n",
      "Training with lr=0.1, hidden1=32, hidden2=16\n",
      "Epoch [1/50], Loss: 46.01199742926926\n",
      "Epoch [2/50], Loss: 44.68653727672139\n",
      "Epoch [3/50], Loss: 44.6977044590184\n",
      "Epoch [4/50], Loss: 44.67549906245998\n",
      "Epoch [5/50], Loss: 44.688690348140526\n",
      "Epoch [6/50], Loss: 44.68897590637207\n",
      "Epoch [7/50], Loss: 44.69676054344803\n",
      "Epoch [8/50], Loss: 44.6825466656294\n",
      "Epoch [9/50], Loss: 44.676000119819015\n",
      "Epoch [10/50], Loss: 44.66929008608959\n",
      "Epoch [11/50], Loss: 44.67851102860247\n",
      "Epoch [12/50], Loss: 44.64707825457464\n",
      "Epoch [13/50], Loss: 44.68467980369193\n",
      "Epoch [14/50], Loss: 44.70224472421115\n",
      "Epoch [15/50], Loss: 44.66860217110055\n",
      "Epoch [16/50], Loss: 44.70761914487745\n",
      "Epoch [17/50], Loss: 44.68206886541648\n",
      "Epoch [18/50], Loss: 44.71984774480101\n",
      "Epoch [19/50], Loss: 44.690823245439375\n",
      "Epoch [20/50], Loss: 44.716687318145254\n",
      "Epoch [21/50], Loss: 44.69657186289302\n",
      "Epoch [22/50], Loss: 44.67985647232806\n",
      "Epoch [23/50], Loss: 44.694663813856785\n",
      "Epoch [24/50], Loss: 44.71846267825267\n",
      "Epoch [25/50], Loss: 44.67546558067447\n",
      "Epoch [26/50], Loss: 44.686406260631124\n",
      "Epoch [27/50], Loss: 44.695942431590595\n",
      "Epoch [28/50], Loss: 44.668057294751776\n",
      "Epoch [29/50], Loss: 44.6949110187468\n",
      "Epoch [30/50], Loss: 44.70640869140625\n",
      "Epoch [31/50], Loss: 44.658529387927445\n",
      "Epoch [32/50], Loss: 44.70283047410308\n",
      "Epoch [33/50], Loss: 44.684929581939194\n",
      "Epoch [34/50], Loss: 44.68907582329922\n",
      "Epoch [35/50], Loss: 44.70847669194956\n",
      "Epoch [36/50], Loss: 44.68760236521236\n",
      "Epoch [37/50], Loss: 44.72470569297916\n",
      "Epoch [38/50], Loss: 44.67264431187364\n",
      "Epoch [39/50], Loss: 44.7023561071177\n",
      "Epoch [40/50], Loss: 44.689066127089205\n",
      "Epoch [41/50], Loss: 44.72293058301582\n",
      "Epoch [42/50], Loss: 44.66751737125584\n",
      "Epoch [43/50], Loss: 44.67399714735688\n",
      "Epoch [44/50], Loss: 44.7312391500004\n",
      "Epoch [45/50], Loss: 44.697809316291185\n",
      "Epoch [46/50], Loss: 44.70220463236824\n",
      "Epoch [47/50], Loss: 44.69780160247302\n",
      "Epoch [48/50], Loss: 44.677873648971804\n",
      "Epoch [49/50], Loss: 44.70021717509285\n",
      "Epoch [50/50], Loss: 44.669777535610514\n",
      "Test Loss: 41.50919977581228\n",
      "Training with lr=0.5, hidden1=4, hidden2=4\n",
      "Epoch [1/50], Loss: 84.84315452575683\n",
      "Epoch [2/50], Loss: 45.27631045794878\n",
      "Epoch [3/50], Loss: 45.29732798591989\n",
      "Epoch [4/50], Loss: 45.44899947057005\n",
      "Epoch [5/50], Loss: 45.31432168053799\n",
      "Epoch [6/50], Loss: 45.30508989427911\n",
      "Epoch [7/50], Loss: 45.3157931781206\n",
      "Epoch [8/50], Loss: 45.42192378434979\n",
      "Epoch [9/50], Loss: 45.32703327741779\n",
      "Epoch [10/50], Loss: 45.33873311027152\n",
      "Epoch [11/50], Loss: 45.28205527633917\n",
      "Epoch [12/50], Loss: 45.492016920496205\n",
      "Epoch [13/50], Loss: 45.27410679801566\n",
      "Epoch [14/50], Loss: 45.33470945514616\n",
      "Epoch [15/50], Loss: 45.35744672368784\n",
      "Epoch [16/50], Loss: 45.29218501106637\n",
      "Epoch [17/50], Loss: 45.56495198734471\n",
      "Epoch [18/50], Loss: 45.38356820403553\n",
      "Epoch [19/50], Loss: 45.386499136002335\n",
      "Epoch [20/50], Loss: 45.15782494466813\n",
      "Epoch [21/50], Loss: 45.20214294058378\n",
      "Epoch [22/50], Loss: 45.33620291537926\n",
      "Epoch [23/50], Loss: 45.47131947376689\n",
      "Epoch [24/50], Loss: 45.25998918502057\n",
      "Epoch [25/50], Loss: 45.398770019656325\n",
      "Epoch [26/50], Loss: 45.340300463066725\n",
      "Epoch [27/50], Loss: 45.284893314173964\n",
      "Epoch [28/50], Loss: 45.37680380461646\n",
      "Epoch [29/50], Loss: 45.56294448102107\n",
      "Epoch [30/50], Loss: 45.440597574828104\n",
      "Epoch [31/50], Loss: 45.16843259842669\n",
      "Epoch [32/50], Loss: 45.33845099777472\n",
      "Epoch [33/50], Loss: 45.31460980899998\n",
      "Epoch [34/50], Loss: 45.230850469870646\n",
      "Epoch [35/50], Loss: 45.35216384012191\n",
      "Epoch [36/50], Loss: 45.42419551786829\n",
      "Epoch [37/50], Loss: 45.360277401032995\n",
      "Epoch [38/50], Loss: 45.26050488831567\n",
      "Epoch [39/50], Loss: 45.28959797718486\n",
      "Epoch [40/50], Loss: 45.19808889295234\n",
      "Epoch [41/50], Loss: 45.22647601268331\n",
      "Epoch [42/50], Loss: 45.214517036813206\n",
      "Epoch [43/50], Loss: 45.25567772662053\n",
      "Epoch [44/50], Loss: 45.3345998482626\n",
      "Epoch [45/50], Loss: 45.13298230718394\n",
      "Epoch [46/50], Loss: 45.400798028414364\n",
      "Epoch [47/50], Loss: 45.30103870454382\n",
      "Epoch [48/50], Loss: 45.41254123625208\n",
      "Epoch [49/50], Loss: 45.39308876913102\n",
      "Epoch [50/50], Loss: 45.2483720935759\n",
      "Test Loss: 41.5302231329998\n",
      "Training with lr=0.5, hidden1=4, hidden2=8\n",
      "Epoch [1/50], Loss: 46.05883832525034\n",
      "Epoch [2/50], Loss: 45.35058312337907\n",
      "Epoch [3/50], Loss: 45.16590009595527\n",
      "Epoch [4/50], Loss: 45.091081956957204\n",
      "Epoch [5/50], Loss: 45.30699896890609\n",
      "Epoch [6/50], Loss: 45.368802367663775\n",
      "Epoch [7/50], Loss: 45.367719919173446\n",
      "Epoch [8/50], Loss: 45.103316535324346\n",
      "Epoch [9/50], Loss: 45.355715357671016\n",
      "Epoch [10/50], Loss: 45.46896697185078\n",
      "Epoch [11/50], Loss: 45.44240871491979\n",
      "Epoch [12/50], Loss: 45.35524522437424\n",
      "Epoch [13/50], Loss: 45.266494544607696\n",
      "Epoch [14/50], Loss: 45.33704784580919\n",
      "Epoch [15/50], Loss: 45.23814280150366\n",
      "Epoch [16/50], Loss: 45.33104596685191\n",
      "Epoch [17/50], Loss: 45.35629268083416\n",
      "Epoch [18/50], Loss: 45.35722092800453\n",
      "Epoch [19/50], Loss: 45.259150545714334\n",
      "Epoch [20/50], Loss: 45.36526600884609\n",
      "Epoch [21/50], Loss: 45.2925473760386\n",
      "Epoch [22/50], Loss: 45.4404643387091\n",
      "Epoch [23/50], Loss: 45.22302498739274\n",
      "Epoch [24/50], Loss: 45.30634633048636\n",
      "Epoch [25/50], Loss: 45.23191877271308\n",
      "Epoch [26/50], Loss: 45.25917008822081\n",
      "Epoch [27/50], Loss: 45.255325886460604\n",
      "Epoch [28/50], Loss: 45.39235195097376\n",
      "Epoch [29/50], Loss: 45.30426059160076\n",
      "Epoch [30/50], Loss: 45.355289628075774\n",
      "Epoch [31/50], Loss: 45.400973441952566\n",
      "Epoch [32/50], Loss: 45.279770253916254\n",
      "Epoch [33/50], Loss: 45.38924587750044\n",
      "Epoch [34/50], Loss: 45.37037413550205\n",
      "Epoch [35/50], Loss: 45.3133119927078\n",
      "Epoch [36/50], Loss: 45.345035709318566\n",
      "Epoch [37/50], Loss: 45.35159701050305\n",
      "Epoch [38/50], Loss: 45.18730928389753\n",
      "Epoch [39/50], Loss: 45.244410649283985\n",
      "Epoch [40/50], Loss: 45.22940818911693\n",
      "Epoch [41/50], Loss: 45.29347505725798\n",
      "Epoch [42/50], Loss: 45.22105190714852\n",
      "Epoch [43/50], Loss: 45.27743105653857\n",
      "Epoch [44/50], Loss: 45.323439629351505\n",
      "Epoch [45/50], Loss: 45.39453021815566\n",
      "Epoch [46/50], Loss: 45.20640437954762\n",
      "Epoch [47/50], Loss: 45.13974426769819\n",
      "Epoch [48/50], Loss: 45.31404435830038\n",
      "Epoch [49/50], Loss: 45.23730690127513\n",
      "Epoch [50/50], Loss: 45.37799551916904\n",
      "Test Loss: 41.60130440369817\n",
      "Training with lr=0.5, hidden1=4, hidden2=16\n",
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Test Loss: nan\n",
      "Training with lr=0.5, hidden1=8, hidden2=4\n",
      "Epoch [1/50], Loss: 141.03895245223748\n",
      "Epoch [2/50], Loss: 45.22100533032027\n",
      "Epoch [3/50], Loss: 45.359812061122206\n",
      "Epoch [4/50], Loss: 45.24606916396344\n",
      "Epoch [5/50], Loss: 45.35061085810427\n",
      "Epoch [6/50], Loss: 45.23391751774022\n",
      "Epoch [7/50], Loss: 45.15824004001305\n",
      "Epoch [8/50], Loss: 45.390313151625335\n",
      "Epoch [9/50], Loss: 45.37784762773357\n",
      "Epoch [10/50], Loss: 45.23192174942767\n",
      "Epoch [11/50], Loss: 45.21856987124584\n",
      "Epoch [12/50], Loss: 45.31025056682649\n",
      "Epoch [13/50], Loss: 45.33857367468662\n",
      "Epoch [14/50], Loss: 45.43269640187748\n",
      "Epoch [15/50], Loss: 45.110604489435914\n",
      "Epoch [16/50], Loss: 45.35078163146973\n",
      "Epoch [17/50], Loss: 45.4305462008617\n",
      "Epoch [18/50], Loss: 45.31309795692319\n",
      "Epoch [19/50], Loss: 45.37116069481021\n",
      "Epoch [20/50], Loss: 45.2265615994813\n",
      "Epoch [21/50], Loss: 45.32829748685243\n",
      "Epoch [22/50], Loss: 45.17311665581875\n",
      "Epoch [23/50], Loss: 45.247104278939666\n",
      "Epoch [24/50], Loss: 45.448472320056354\n",
      "Epoch [25/50], Loss: 45.25013437740138\n",
      "Epoch [26/50], Loss: 45.32486717349193\n",
      "Epoch [27/50], Loss: 45.388680911455\n",
      "Epoch [28/50], Loss: 45.22547075240338\n",
      "Epoch [29/50], Loss: 45.34543562091765\n",
      "Epoch [30/50], Loss: 45.405890461655915\n",
      "Epoch [31/50], Loss: 45.35801798711058\n",
      "Epoch [32/50], Loss: 45.23439040887551\n",
      "Epoch [33/50], Loss: 45.22977619483823\n",
      "Epoch [34/50], Loss: 45.20932181311436\n",
      "Epoch [35/50], Loss: 45.373076635892275\n",
      "Epoch [36/50], Loss: 45.190867195754755\n",
      "Epoch [37/50], Loss: 45.308621997520575\n",
      "Epoch [38/50], Loss: 45.187592009247325\n",
      "Epoch [39/50], Loss: 45.249189414352664\n",
      "Epoch [40/50], Loss: 45.25631343028584\n",
      "Epoch [41/50], Loss: 45.29796112560835\n",
      "Epoch [42/50], Loss: 45.46393226873679\n",
      "Epoch [43/50], Loss: 45.2593740901009\n",
      "Epoch [44/50], Loss: 45.170960460725375\n",
      "Epoch [45/50], Loss: 45.256371269851435\n",
      "Epoch [46/50], Loss: 45.19276628337923\n",
      "Epoch [47/50], Loss: 45.335674980038505\n",
      "Epoch [48/50], Loss: 45.26900639768507\n",
      "Epoch [49/50], Loss: 45.40844893533676\n",
      "Epoch [50/50], Loss: 45.280472527175654\n",
      "Test Loss: 41.555934978805425\n",
      "Training with lr=0.5, hidden1=8, hidden2=8\n",
      "Epoch [1/50], Loss: 87987233118512.44\n",
      "Epoch [2/50], Loss: 45.34913625248143\n",
      "Epoch [3/50], Loss: 45.26323354126977\n",
      "Epoch [4/50], Loss: 45.317966842651366\n",
      "Epoch [5/50], Loss: 45.51698633412846\n",
      "Epoch [6/50], Loss: 45.11151255623239\n",
      "Epoch [7/50], Loss: 45.560518311672524\n",
      "Epoch [8/50], Loss: 45.49993146052126\n",
      "Epoch [9/50], Loss: 45.35782497593614\n",
      "Epoch [10/50], Loss: 45.24350790742968\n",
      "Epoch [11/50], Loss: 45.27791135819232\n",
      "Epoch [12/50], Loss: 45.184931808221535\n",
      "Epoch [13/50], Loss: 45.31899117016401\n",
      "Epoch [14/50], Loss: 45.21481073098104\n",
      "Epoch [15/50], Loss: 45.46836878041752\n",
      "Epoch [16/50], Loss: 45.31181978319512\n",
      "Epoch [17/50], Loss: 45.39499121493981\n",
      "Epoch [18/50], Loss: 45.17778838423432\n",
      "Epoch [19/50], Loss: 45.37793732314813\n",
      "Epoch [20/50], Loss: 45.307032231815526\n",
      "Epoch [21/50], Loss: 45.297462325799664\n",
      "Epoch [22/50], Loss: 45.28193458181913\n",
      "Epoch [23/50], Loss: 45.27279988273246\n",
      "Epoch [24/50], Loss: 45.34206258429855\n",
      "Epoch [25/50], Loss: 45.435806815350645\n",
      "Epoch [26/50], Loss: 45.224796892385015\n",
      "Epoch [27/50], Loss: 45.422541971675685\n",
      "Epoch [28/50], Loss: 45.32347405230413\n",
      "Epoch [29/50], Loss: 45.25002790357246\n",
      "Epoch [30/50], Loss: 45.28804618960521\n",
      "Epoch [31/50], Loss: 45.28590129164399\n",
      "Epoch [32/50], Loss: 45.31182129031322\n",
      "Epoch [33/50], Loss: 45.19221799256372\n",
      "Epoch [34/50], Loss: 45.34085663342085\n",
      "Epoch [35/50], Loss: 45.28670833775254\n",
      "Epoch [36/50], Loss: 45.42906765546955\n",
      "Epoch [37/50], Loss: 45.37170962974673\n",
      "Epoch [38/50], Loss: 45.308486012943455\n",
      "Epoch [39/50], Loss: 45.14018126941118\n",
      "Epoch [40/50], Loss: 45.194580722246016\n",
      "Epoch [41/50], Loss: 45.43030050308978\n",
      "Epoch [42/50], Loss: 45.316560335628324\n",
      "Epoch [43/50], Loss: 45.351683469678534\n",
      "Epoch [44/50], Loss: 45.17241437317895\n",
      "Epoch [45/50], Loss: 45.39344342966549\n",
      "Epoch [46/50], Loss: 45.286728715114904\n",
      "Epoch [47/50], Loss: 45.26341604639272\n",
      "Epoch [48/50], Loss: 45.28900616755251\n",
      "Epoch [49/50], Loss: 45.30545020806985\n",
      "Epoch [50/50], Loss: 45.359116263467754\n",
      "Test Loss: 41.51214882071692\n",
      "Training with lr=0.5, hidden1=8, hidden2=16\n",
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Test Loss: nan\n",
      "Training with lr=0.5, hidden1=16, hidden2=4\n",
      "Epoch [1/50], Loss: 12489897030.64246\n",
      "Epoch [2/50], Loss: 45.49578984057317\n",
      "Epoch [3/50], Loss: 45.24913967945537\n",
      "Epoch [4/50], Loss: 45.32804557925365\n",
      "Epoch [5/50], Loss: 45.32224069188853\n",
      "Epoch [6/50], Loss: 45.242596257319214\n",
      "Epoch [7/50], Loss: 45.39656178208648\n",
      "Epoch [8/50], Loss: 45.2988278342075\n",
      "Epoch [9/50], Loss: 45.40106580140161\n",
      "Epoch [10/50], Loss: 45.40309759984251\n",
      "Epoch [11/50], Loss: 45.32072052877457\n",
      "Epoch [12/50], Loss: 45.38728068617524\n",
      "Epoch [13/50], Loss: 45.26981204298676\n",
      "Epoch [14/50], Loss: 45.30506037102371\n",
      "Epoch [15/50], Loss: 45.25274479662786\n",
      "Epoch [16/50], Loss: 45.15032617225022\n",
      "Epoch [17/50], Loss: 45.400797553140606\n",
      "Epoch [18/50], Loss: 45.34902695202437\n",
      "Epoch [19/50], Loss: 45.29187254358511\n",
      "Epoch [20/50], Loss: 45.496405460795415\n",
      "Epoch [21/50], Loss: 45.3663944994817\n",
      "Epoch [22/50], Loss: 45.294788629500594\n",
      "Epoch [23/50], Loss: 45.363056348581786\n",
      "Epoch [24/50], Loss: 45.177107304432354\n",
      "Epoch [25/50], Loss: 45.28137894302118\n",
      "Epoch [26/50], Loss: 45.251322446103956\n",
      "Epoch [27/50], Loss: 45.39680441246658\n",
      "Epoch [28/50], Loss: 45.34288725931136\n",
      "Epoch [29/50], Loss: 45.38320496981261\n",
      "Epoch [30/50], Loss: 45.17837725780049\n",
      "Epoch [31/50], Loss: 45.27093539316146\n",
      "Epoch [32/50], Loss: 45.34381343653945\n",
      "Epoch [33/50], Loss: 45.237027158893525\n",
      "Epoch [34/50], Loss: 45.216584602731174\n",
      "Epoch [35/50], Loss: 45.17328868928503\n",
      "Epoch [36/50], Loss: 45.30242786094791\n",
      "Epoch [37/50], Loss: 45.40759832663614\n",
      "Epoch [38/50], Loss: 45.478143348068485\n",
      "Epoch [39/50], Loss: 45.336908130958435\n",
      "Epoch [40/50], Loss: 45.167380498667235\n",
      "Epoch [41/50], Loss: 45.41919678547343\n",
      "Epoch [42/50], Loss: 45.40046555878686\n",
      "Epoch [43/50], Loss: 45.33058097088923\n",
      "Epoch [44/50], Loss: 45.35992458531114\n",
      "Epoch [45/50], Loss: 45.30209544760282\n",
      "Epoch [46/50], Loss: 45.409929757040054\n",
      "Epoch [47/50], Loss: 45.39298389622422\n",
      "Epoch [48/50], Loss: 45.2130772012179\n",
      "Epoch [49/50], Loss: 45.38879047143655\n",
      "Epoch [50/50], Loss: 45.515426660756596\n",
      "Test Loss: 42.80779030486828\n",
      "Training with lr=0.5, hidden1=16, hidden2=8\n",
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Test Loss: nan\n",
      "Training with lr=0.5, hidden1=16, hidden2=16\n",
      "Epoch [1/50], Loss: 1.627215463427712e+18\n",
      "Epoch [2/50], Loss: 45.27036769429191\n",
      "Epoch [3/50], Loss: 45.197231364641034\n",
      "Epoch [4/50], Loss: 45.28821353286993\n",
      "Epoch [5/50], Loss: 45.232577908625366\n",
      "Epoch [6/50], Loss: 45.51509290601386\n",
      "Epoch [7/50], Loss: 45.33515104074947\n",
      "Epoch [8/50], Loss: 45.3772394555514\n",
      "Epoch [9/50], Loss: 45.288720324782076\n",
      "Epoch [10/50], Loss: 45.321265117457656\n",
      "Epoch [11/50], Loss: 45.26588589402496\n",
      "Epoch [12/50], Loss: 45.349011249229555\n",
      "Epoch [13/50], Loss: 45.22150685982626\n",
      "Epoch [14/50], Loss: 45.2322503574559\n",
      "Epoch [15/50], Loss: 45.224305027821025\n",
      "Epoch [16/50], Loss: 45.44025665658419\n",
      "Epoch [17/50], Loss: 45.23662668447025\n",
      "Epoch [18/50], Loss: 45.20303543591108\n",
      "Epoch [19/50], Loss: 45.37934551551694\n",
      "Epoch [20/50], Loss: 45.30798294192455\n",
      "Epoch [21/50], Loss: 45.1684907975744\n",
      "Epoch [22/50], Loss: 45.18831792737617\n",
      "Epoch [23/50], Loss: 45.25083331749087\n",
      "Epoch [24/50], Loss: 45.47265287305488\n",
      "Epoch [25/50], Loss: 45.33453672753006\n",
      "Epoch [26/50], Loss: 45.28796502097708\n",
      "Epoch [27/50], Loss: 45.48167668014276\n",
      "Epoch [28/50], Loss: 45.21993110531666\n",
      "Epoch [29/50], Loss: 45.362855883113674\n",
      "Epoch [30/50], Loss: 45.31258111547251\n",
      "Epoch [31/50], Loss: 45.284265999715835\n",
      "Epoch [32/50], Loss: 45.40008945777768\n",
      "Epoch [33/50], Loss: 45.37676521051125\n",
      "Epoch [34/50], Loss: 45.55975464992836\n",
      "Epoch [35/50], Loss: 45.1755340951388\n",
      "Epoch [36/50], Loss: 45.24400173249792\n",
      "Epoch [37/50], Loss: 45.18463869251189\n",
      "Epoch [38/50], Loss: 45.317831151993545\n",
      "Epoch [39/50], Loss: 45.14919678109591\n",
      "Epoch [40/50], Loss: 45.252881925614155\n",
      "Epoch [41/50], Loss: 45.36505368967525\n",
      "Epoch [42/50], Loss: 45.28963553006532\n",
      "Epoch [43/50], Loss: 45.15765246719611\n",
      "Epoch [44/50], Loss: 45.33283757069072\n",
      "Epoch [45/50], Loss: 45.32292310996134\n",
      "Epoch [46/50], Loss: 45.247179709887895\n",
      "Epoch [47/50], Loss: 45.275432242721806\n",
      "Epoch [48/50], Loss: 45.31897593013576\n",
      "Epoch [49/50], Loss: 45.22736306425001\n",
      "Epoch [50/50], Loss: 45.34716942896608\n",
      "Test Loss: 41.51797591638929\n",
      "Training with lr=0.5, hidden1=32, hidden2=4\n",
      "Epoch [1/50], Loss: 45.86347365144823\n",
      "Epoch [2/50], Loss: 45.07547396675485\n",
      "Epoch [3/50], Loss: 45.36122672596916\n",
      "Epoch [4/50], Loss: 45.12924939139945\n",
      "Epoch [5/50], Loss: 45.415816413379105\n",
      "Epoch [6/50], Loss: 45.40898790828517\n",
      "Epoch [7/50], Loss: 45.51196005148966\n",
      "Epoch [8/50], Loss: 45.32943133369821\n",
      "Epoch [9/50], Loss: 45.34653307805296\n",
      "Epoch [10/50], Loss: 45.34203232624492\n",
      "Epoch [11/50], Loss: 45.17957980046507\n",
      "Epoch [12/50], Loss: 45.274707469002145\n",
      "Epoch [13/50], Loss: 45.29123712133189\n",
      "Epoch [14/50], Loss: 45.56834332825708\n",
      "Epoch [15/50], Loss: 45.23477835733382\n",
      "Epoch [16/50], Loss: 45.14502804240242\n",
      "Epoch [17/50], Loss: 45.3447778013886\n",
      "Epoch [18/50], Loss: 45.26999781249\n",
      "Epoch [19/50], Loss: 45.16338777385774\n",
      "Epoch [20/50], Loss: 45.36131055237817\n",
      "Epoch [21/50], Loss: 45.194461522336866\n",
      "Epoch [22/50], Loss: 45.382121570774764\n",
      "Epoch [23/50], Loss: 45.284931801967936\n",
      "Epoch [24/50], Loss: 45.47305799390449\n",
      "Epoch [25/50], Loss: 45.279706204523805\n",
      "Epoch [26/50], Loss: 45.57103243968526\n",
      "Epoch [27/50], Loss: 45.329626521126166\n",
      "Epoch [28/50], Loss: 45.40671338566014\n",
      "Epoch [29/50], Loss: 45.30677230084529\n",
      "Epoch [30/50], Loss: 45.32956824380843\n",
      "Epoch [31/50], Loss: 45.36994873422091\n",
      "Epoch [32/50], Loss: 45.377357214005265\n",
      "Epoch [33/50], Loss: 45.33024920479196\n",
      "Epoch [34/50], Loss: 45.22783977008257\n",
      "Epoch [35/50], Loss: 45.46335936999712\n",
      "Epoch [36/50], Loss: 45.38374408972068\n",
      "Epoch [37/50], Loss: 45.52590015598985\n",
      "Epoch [38/50], Loss: 45.38678447535781\n",
      "Epoch [39/50], Loss: 45.43766961957588\n",
      "Epoch [40/50], Loss: 45.32862541636483\n",
      "Epoch [41/50], Loss: 45.295021394823415\n",
      "Epoch [42/50], Loss: 45.33481140136719\n",
      "Epoch [43/50], Loss: 45.23090526393202\n",
      "Epoch [44/50], Loss: 45.39938570866819\n",
      "Epoch [45/50], Loss: 45.33558429030121\n",
      "Epoch [46/50], Loss: 45.24001195938861\n",
      "Epoch [47/50], Loss: 45.338880917283355\n",
      "Epoch [48/50], Loss: 45.12980632469302\n",
      "Epoch [49/50], Loss: 45.34394326131852\n",
      "Epoch [50/50], Loss: 45.36537998387071\n",
      "Test Loss: 47.603179247324704\n",
      "Training with lr=0.5, hidden1=32, hidden2=8\n",
      "Epoch [1/50], Loss: 46.126994304969664\n",
      "Epoch [2/50], Loss: 45.16933747588611\n",
      "Epoch [3/50], Loss: 45.17173166118685\n",
      "Epoch [4/50], Loss: 45.29880126577909\n",
      "Epoch [5/50], Loss: 45.4263436739562\n",
      "Epoch [6/50], Loss: 45.27173269303118\n",
      "Epoch [7/50], Loss: 45.38149181428503\n",
      "Epoch [8/50], Loss: 45.340925810767004\n",
      "Epoch [9/50], Loss: 45.24177917730613\n",
      "Epoch [10/50], Loss: 45.20989414902984\n",
      "Epoch [11/50], Loss: 45.20088336506828\n",
      "Epoch [12/50], Loss: 45.257870902389776\n",
      "Epoch [13/50], Loss: 45.230989349865524\n",
      "Epoch [14/50], Loss: 45.2971640508683\n",
      "Epoch [15/50], Loss: 45.42170685940101\n",
      "Epoch [16/50], Loss: 45.267519722610224\n",
      "Epoch [17/50], Loss: 45.45509015380359\n",
      "Epoch [18/50], Loss: 45.33635784837066\n",
      "Epoch [19/50], Loss: 45.338150268304545\n",
      "Epoch [20/50], Loss: 45.33245862741939\n",
      "Epoch [21/50], Loss: 45.3041009684078\n",
      "Epoch [22/50], Loss: 45.093965699242766\n",
      "Epoch [23/50], Loss: 45.39382015603488\n",
      "Epoch [24/50], Loss: 45.280464416253764\n",
      "Epoch [25/50], Loss: 45.414916917144275\n",
      "Epoch [26/50], Loss: 45.22482770075563\n",
      "Epoch [27/50], Loss: 45.4100730583316\n",
      "Epoch [28/50], Loss: 45.302069635860256\n",
      "Epoch [29/50], Loss: 45.378723982513925\n",
      "Epoch [30/50], Loss: 45.3456289822938\n",
      "Epoch [31/50], Loss: 45.581349460414195\n",
      "Epoch [32/50], Loss: 45.304767846279454\n",
      "Epoch [33/50], Loss: 45.440598797407304\n",
      "Epoch [34/50], Loss: 45.335840525392626\n",
      "Epoch [35/50], Loss: 45.21732637999488\n",
      "Epoch [36/50], Loss: 45.4407060716973\n",
      "Epoch [37/50], Loss: 45.39751229520704\n",
      "Epoch [38/50], Loss: 45.31651255498167\n",
      "Epoch [39/50], Loss: 45.2625675170148\n",
      "Epoch [40/50], Loss: 45.463436576968334\n",
      "Epoch [41/50], Loss: 45.189615296535806\n",
      "Epoch [42/50], Loss: 45.390041429488384\n",
      "Epoch [43/50], Loss: 45.237381691229146\n",
      "Epoch [44/50], Loss: 45.20823500899018\n",
      "Epoch [45/50], Loss: 45.39948555367892\n",
      "Epoch [46/50], Loss: 45.19886785726078\n",
      "Epoch [47/50], Loss: 45.31518685387783\n",
      "Epoch [48/50], Loss: 45.21208512353115\n",
      "Epoch [49/50], Loss: 45.299338966119485\n",
      "Epoch [50/50], Loss: 45.238605367941936\n",
      "Test Loss: 41.740082238466684\n",
      "Training with lr=0.5, hidden1=32, hidden2=16\n",
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Test Loss: nan\n",
      "Best Loss: 41.504081456715824\n",
      "Best Params: Learning Rate=0.1, Hidden1=4, Hidden2=4\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for lr, l1, l2 in hyperparameter_combinations:\n",
    "    model = NeuralNetwork(input_size=15, l1=l1, l2=l2, output_size=1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(f\"Training with lr={lr}, hidden1={l1}, hidden2={l2}\")\n",
    "    train_model(model, optimizer, loss_module, train_loader, epochs=50)\n",
    "    test_loss = evaluate_model(model, test_loader, criterion)\n",
    "    \n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        best_params = (lr, l1, l2)\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best Loss: {best_loss}\")\n",
    "print(f\"Best Params: Learning Rate={best_params[0]}, Hidden1={best_params[1]}, Hidden2={best_params[2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
